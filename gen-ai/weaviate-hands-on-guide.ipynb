{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": "# Weaviate Hands-On Guide\n\n> Practical, step-by-step tutorial for mastering Weaviate vector database with 2025 features.\n\n## Prerequisites\n\n```bash\n# Install dependencies with uv\nuv pip install weaviate-client>=4.17 numpy grpcio sentence-transformers\n\n# Or with pip\npip install weaviate-client>=4.17 numpy grpcio sentence-transformers\n\n# Start local Weaviate instance (v1.27+)\ncd gen-ai/weaviate-setup\ndocker-compose up -d\n\n# Or connect to remote server (see Section 1)\n```\n\n## Embedding Model\n\nThis notebook uses **sentence-transformers/all-MiniLM-L6-v2**:\n- **Size**: 22M parameters (~80MB model)\n- **Dimensions**: 384\n- **Speed**: ~1000 sentences/sec on CPU\n- **Quality**: High MTEB performance for semantic similarity\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n1. Connect to Weaviate with production-ready configuration\n2. Create collections with advanced vectorizer and quantization settings\n3. Perform CRUD operations with error handling\n4. Execute vector similarity searches with dynamic ef\n5. Use hybrid search with alpha parameter tuning (2025)\n6. Apply filters and aggregations\n7. Implement batch operations with monitoring\n8. Configure RQ quantization for 4x compression (2025)\n9. Use AI Agents for intelligent queries (2025 Preview)\n10. Build a production-ready RAG system with real embeddings\n\n---"
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {},
      "source": [
        "## Section 1: Setup and Connection\n",
        "\n",
        "Production-ready connection with error handling and gRPC support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-2",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Connect to Weaviate with production-ready configuration.\"\"\"\n",
        "\n",
        "from typing import List, Dict, Any, Optional\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import weaviate\n",
        "from weaviate.classes.config import Configure, Property, DataType, VectorDistances\n",
        "from weaviate.classes.query import Filter, MetadataQuery\n",
        "import numpy as np\n",
        "\n",
        "# Connection parameters\n",
        "WEAVIATE_HOST = \"localhost\"\n",
        "WEAVIATE_PORT = 8080\n",
        "WEAVIATE_GRPC_PORT = 50051  # Required for v4 client\n",
        "\n",
        "def connect_with_retry(max_retries: int = 3, retry_delay: int = 2) -> weaviate.WeaviateClient:\n",
        "    \"\"\"Connect to Weaviate with retry logic.\n",
        "    \n",
        "    Args:\n",
        "        max_retries: Maximum number of connection attempts\n",
        "        retry_delay: Seconds to wait between retries\n",
        "        \n",
        "    Returns:\n",
        "        Connected Weaviate client\n",
        "        \n",
        "    Raises:\n",
        "        RuntimeError: If connection fails after all retries\n",
        "    \"\"\"\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            client = weaviate.connect_to_local(\n",
        "                host=WEAVIATE_HOST,\n",
        "                port=WEAVIATE_PORT,\n",
        "                grpc_port=WEAVIATE_GRPC_PORT\n",
        "            )\n",
        "            \n",
        "            if client.is_ready():\n",
        "                return client\n",
        "                \n",
        "        except Exception as e:\n",
        "            if attempt < max_retries:\n",
        "                print(f\"Connection attempt {attempt} failed: {e}\")\n",
        "                print(f\"Retrying in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                raise RuntimeError(f\"Failed to connect after {max_retries} attempts\") from e\n",
        "    \n",
        "    raise RuntimeError(\"Connection failed\")\n",
        "\n",
        "# Connect to Weaviate\n",
        "try:\n",
        "    client = connect_with_retry()\n",
        "    print(\"SUCCESS: Connected to Weaviate\\n\")\n",
        "    \n",
        "    # Get metadata\n",
        "    meta = client.get_meta()\n",
        "    print(f\"Weaviate version: {meta['version']}\")\n",
        "    print(f\"Available modules: {list(meta['modules'].keys())}\")\n",
        "    \n",
        "except RuntimeError as e:\n",
        "    print(f\"ERROR: {e}\")\n",
        "    print(\"\\nMake sure Weaviate is running:\")\n",
        "    print(\"  cd gen-ai/weaviate-setup && docker-compose up -d\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-3",
      "metadata": {},
      "source": [
        "## Section 2: Create Collection with Advanced Configuration\n",
        "\n",
        "Configure HNSW indexing and RQ quantization (2025 default in v1.33+)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-4",
      "metadata": {},
      "outputs": [],
      "source": "\"\"\"Create collection with advanced HNSW and quantization settings.\"\"\"\n\nCOLLECTION_NAME = \"Article\"\nVECTOR_DIMENSIONS = 384\n\n# Delete collection if it exists\nif client.collections.exists(COLLECTION_NAME):\n    client.collections.delete(COLLECTION_NAME)\n    print(f\"Deleted existing '{COLLECTION_NAME}' collection\\n\")\n\n# Create collection with optimized configuration\narticles = client.collections.create(\n    name=COLLECTION_NAME,\n    \n    # Manual vectorization (use text2vec-openai/cohere in production)\n    vectorizer_config=Configure.Vectorizer.none(),\n    \n    # Vector index configuration\n    vector_index_config=Configure.VectorIndex.hnsw(\n        distance_metric=VectorDistances.COSINE,\n        \n        # HNSW parameters (tuned for <100K objects)\n        max_connections=32,        # Higher for better recall\n        ef_construction=128,       # Higher for better index quality\n        \n        # Dynamic ef (runtime query tuning)\n        ef=-1,                     # Default: auto-tuned\n        \n        # RQ Quantization (2025 default in v1.33+)\n        # Enables 4x compression with minimal recall loss\n        quantizer=Configure.VectorIndex.Quantizer.rq(\n            training_limit=50000,  # Use 50K vectors for training\n            segments=3             # Balance compression vs accuracy\n        )\n    ),\n    \n    # Enable BM25 for hybrid search\n    inverted_index_config=Configure.inverted_index(\n        bm25_b=0.75,              # Document length normalization\n        bm25_k1=1.2               # Term frequency saturation\n    ),\n    \n    # Schema properties\n    properties=[\n        Property(\n            name=\"title\",\n            data_type=DataType.TEXT,\n            description=\"Article title\",\n            index_searchable=True   # Enable BM25 search\n        ),\n        Property(\n            name=\"content\",\n            data_type=DataType.TEXT,\n            description=\"Article content\",\n            index_searchable=True\n        ),\n        Property(\n            name=\"author\",\n            data_type=DataType.TEXT,\n            description=\"Article author\",\n            index_filterable=True   # Enable filtering\n        ),\n        Property(\n            name=\"category\",\n            data_type=DataType.TEXT,\n            description=\"Article category\",\n            index_filterable=True\n        ),\n        Property(\n            name=\"views\",\n            data_type=DataType.INT,\n            description=\"View count\",\n            index_range_filters=True  # Enable numeric range filters\n        )\n    ]\n)\n\nprint(f\"SUCCESS: Created '{COLLECTION_NAME}' collection\")\nprint(\"\\nConfiguration:\")\nprint(\"  - HNSW Index: maxConnections=32, efConstruction=128\")\nprint(\"  - RQ Quantization: Enabled (4x compression)\")\nprint(\"  - BM25: Enabled for hybrid search\")\nprint(\"  - Distance Metric: Cosine\")\n\n# Verify collection\nif client.collections.exists(COLLECTION_NAME):\n    print(f\"\\nVERIFIED: Collection '{COLLECTION_NAME}' is ready\")"
    },
    {
      "cell_type": "markdown",
      "id": "cell-5",
      "metadata": {},
      "source": [
        "## Section 3: Insert Single Object\n",
        "\n",
        "Insert a single article with vector embedding and error handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-6",
      "metadata": {},
      "outputs": [],
      "source": "\"\"\"Insert a single article with real embeddings.\"\"\"\n\nfrom sentence_transformers import SentenceTransformer\n\n# Initialize embedding model (cached after first run)\nprint(\"Loading embedding model: all-MiniLM-L6-v2 (384 dimensions)...\")\nembedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\nprint(\"Model loaded successfully\\n\")\n\ndef generate_embedding(text: str) -> List[float]:\n    \"\"\"Generate real embedding using sentence-transformers.\n    \n    Uses all-MiniLM-L6-v2 from MTEB leaderboard:\n    - Size: 22M parameters (~80MB)\n    - Dimensions: 384\n    - Speed: ~1000 sentences/sec on CPU\n    - Quality: High performance on semantic similarity tasks\n    \n    Args:\n        text: Input text to embed\n        \n    Returns:\n        384-dimensional embedding vector\n    \"\"\"\n    embedding = embedding_model.encode(text, normalize_embeddings=True)\n    return embedding.tolist()\n\n# Get collection\narticles = client.collections.get(COLLECTION_NAME)\n\ntry:\n    article_text = \"Introduction to Vector Databases. Vector databases are specialized databases designed to store and search high-dimensional vectors efficiently. They are essential for semantic search, recommendation systems, and RAG applications.\"\n    \n    # Generate real embedding\n    print(\"Generating embedding for article...\")\n    vector = generate_embedding(article_text)\n    \n    # Insert article\n    uuid = articles.data.insert(\n        properties={\n            \"title\": \"Introduction to Vector Databases\",\n            \"content\": \"Vector databases are specialized databases designed to store and search high-dimensional vectors efficiently. They are essential for semantic search, recommendation systems, and RAG applications.\",\n            \"author\": \"John Doe\",\n            \"category\": \"Machine Learning\",\n            \"views\": 1250\n        },\n        vector=vector\n    )\n    \n    print(f\"SUCCESS: Inserted article with real embeddings\")\n    print(f\"UUID: {uuid}\")\n    print(f\"Vector dimensions: {len(vector)}\")\n    print(f\"Vector sample: {vector[:5]}...\")\n    \nexcept Exception as e:\n    print(f\"ERROR: Insert failed: {e}\")"
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {},
      "source": [
        "## Section 4: Batch Insert with Monitoring\n",
        "\n",
        "Efficiently insert multiple objects with error tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-8",
      "metadata": {},
      "outputs": [],
      "source": "\"\"\"Batch insert with comprehensive error handling.\"\"\"\n\n# Sample dataset\nsample_articles = [\n    {\n        \"title\": \"Understanding HNSW Algorithm\",\n        \"content\": \"HNSW (Hierarchical Navigable Small World) is a graph-based algorithm used for approximate nearest neighbor search in high-dimensional spaces.\",\n        \"author\": \"Jane Smith\",\n        \"category\": \"Algorithms\",\n        \"views\": 890\n    },\n    {\n        \"title\": \"RAG Systems Explained\",\n        \"content\": \"Retrieval-Augmented Generation (RAG) combines vector search with large language models to provide contextually relevant and accurate responses.\",\n        \"author\": \"Alice Johnson\",\n        \"category\": \"AI\",\n        \"views\": 2100\n    },\n    {\n        \"title\": \"Transformer Architecture Deep Dive\",\n        \"content\": \"Transformers revolutionized NLP with attention mechanisms. They enable models like GPT and BERT to understand context and relationships in text.\",\n        \"author\": \"Bob Williams\",\n        \"category\": \"Deep Learning\",\n        \"views\": 3400\n    },\n    {\n        \"title\": \"LLM Fine-tuning with LoRA\",\n        \"content\": \"Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning technique that reduces the number of trainable parameters by 10,000x.\",\n        \"author\": \"John Doe\",\n        \"category\": \"Machine Learning\",\n        \"views\": 1800\n    },\n    {\n        \"title\": \"Embeddings and Vector Space\",\n        \"content\": \"Embeddings map discrete objects (words, images) into continuous vector spaces where semantic similarity is captured by distance metrics.\",\n        \"author\": \"Jane Smith\",\n        \"category\": \"Machine Learning\",\n        \"views\": 1500\n    },\n    {\n        \"title\": \"Quantization Techniques for Vector DBs\",\n        \"content\": \"Quantization reduces memory footprint of vector databases. RQ (Rotational Quantization) offers 4x compression with <1% recall loss, making it ideal for production.\",\n        \"author\": \"Alice Johnson\",\n        \"category\": \"Optimization\",\n        \"views\": 950\n    },\n    {\n        \"title\": \"Hybrid Search Strategies\",\n        \"content\": \"Hybrid search combines vector similarity with keyword search (BM25). Tuning the alpha parameter (0=BM25, 1=vector) optimizes for your use case. Start with alpha=0.7 for semantic-heavy tasks.\",\n        \"author\": \"Bob Williams\",\n        \"category\": \"Search\",\n        \"views\": 1200\n    },\n    {\n        \"title\": \"AI Agents in Vector Databases\",\n        \"content\": \"AI Agents (2025 Preview) enable intelligent query planning, automatic query transformation, and personalized search results without manual tuning.\",\n        \"author\": \"Jane Smith\",\n        \"category\": \"AI\",\n        \"views\": 2500\n    }\n]\n\n# Batch insert with error tracking\narticles = client.collections.get(COLLECTION_NAME)\nerrors = []\n\ntry:\n    with articles.batch.dynamic() as batch:\n        for i, article in enumerate(sample_articles):\n            vector = generate_embedding(article[\"title\"] + \" \" + article[\"content\"])\n            batch.add_object(\n                properties=article,\n                vector=vector\n            )\n    \n    # Check for batch errors\n    if batch.failed_objects:\n        errors = batch.failed_objects\n        print(f\"WARNING: {len(errors)} objects failed to insert\")\n        for error in errors[:5]:  # Show first 5\n            print(f\"  - {error}\")\n    \n    print(f\"\\nSUCCESS: Batch inserted {len(sample_articles)} articles\")\n    \n    # Verify count\n    result = articles.aggregate.over_all(total_count=True)\n    print(f\"Total articles in collection: {result.total_count}\")\n    \nexcept Exception as e:\n    print(f\"ERROR: Batch insert failed: {e}\")"
    },
    {
      "cell_type": "markdown",
      "id": "cell-9",
      "metadata": {},
      "source": [
        "## Section 5: Retrieve Objects\n",
        "\n",
        "Fetch and display stored articles with pagination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-10",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Retrieve articles with pagination.\"\"\"\n",
        "\n",
        "articles = client.collections.get(COLLECTION_NAME)\n",
        "\n",
        "# Fetch objects with offset/limit for pagination\n",
        "response = articles.query.fetch_objects(\n",
        "    limit=10,\n",
        "    offset=0  # Use for pagination: page 2 = offset=10, page 3 = offset=20\n",
        ")\n",
        "\n",
        "print(f\"Retrieved {len(response.objects)} articles:\\n\")\n",
        "\n",
        "for i, obj in enumerate(response.objects, 1):\n",
        "    print(f\"{i}. {obj.properties['title']}\")\n",
        "    print(f\"   Author: {obj.properties['author']}\")\n",
        "    print(f\"   Category: {obj.properties['category']}\")\n",
        "    print(f\"   Views: {obj.properties['views']:,}\")\n",
        "    print(f\"   UUID: {obj.uuid}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-11",
      "metadata": {},
      "source": [
        "## Section 6: Vector Similarity Search with Dynamic ef\n",
        "\n",
        "Perform semantic search with runtime query tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {},
      "outputs": [],
      "source": "\"\"\"Vector similarity search with dynamic ef tuning.\"\"\"\n\n# Generate query vector\nquery_vector = generate_embedding(\"vector database semantic search algorithms\")\n\n# Search with dynamic ef for higher recall\nresponse = articles.query.near_vector(\n    near_vector=query_vector,\n    limit=5,\n    \n    # Return distance and additional metadata\n    return_metadata=MetadataQuery(\n        distance=True,\n        certainty=True  # Similarity score (0-1)\n    ),\n    \n    # Dynamic ef: Higher = better recall but slower\n    # Default: -1 (auto-tuned)\n    # Manual: 64 (fast), 128 (balanced), 256+ (high recall)\n    # autocut=1  # Uncomment to enable autocut (stop at natural relevance gap)\n)\n\nprint(\"Top 5 semantically similar articles:\\n\")\n\nfor i, obj in enumerate(response.objects, 1):\n    distance = obj.metadata.distance if obj.metadata.distance else 0.0\n    certainty = obj.metadata.certainty if obj.metadata.certainty else 0.0\n    \n    print(f\"{i}. {obj.properties['title']}\")\n    print(f\"   Category: {obj.properties['category']}\")\n    print(f\"   Distance: {distance:.4f} | Certainty: {certainty:.2%}\")\n    print(f\"   Content: {obj.properties['content'][:120]}...\")\n    print()"
    },
    {
      "cell_type": "markdown",
      "id": "cell-13",
      "metadata": {},
      "source": [
        "## Section 7: Hybrid Search with Alpha Tuning (2025)\n",
        "\n",
        "Combine semantic (vector) and keyword (BM25) search with configurable weighting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-14",
      "metadata": {},
      "outputs": [],
      "source": "\"\"\"Hybrid search with alpha parameter tuning.\"\"\"\n\nfrom weaviate.classes.query import HybridFusion\n\n# Search query\nsearch_query = \"machine learning algorithms\"\n\n# Generate query vector (in production, embed the query)\nquery_vector = generate_embedding(search_query)\n\nprint(f\"Search query: '{search_query}'\\n\")\nprint(\"Testing different alpha values:\\n\")\nprint(\"  alpha=0.0: Pure keyword (BM25)\")\nprint(\"  alpha=0.5: Balanced hybrid\")\nprint(\"  alpha=0.7: Semantic-heavy (recommended for most tasks)\")\nprint(\"  alpha=1.0: Pure vector search\\n\")\nprint(\"=\" * 80)\n\n# Test different alpha values\nfor alpha in [0.0, 0.5, 0.7, 1.0]:\n    response = articles.query.hybrid(\n        query=search_query,\n        vector=query_vector,\n        \n        # Alpha parameter: 0 (BM25) to 1 (vector)\n        alpha=alpha,\n        \n        # Fusion algorithm\n        fusion_type=HybridFusion.RELATIVE_SCORE,  # or RANKED\n        \n        limit=3,\n        return_metadata=MetadataQuery(\n            score=True,  # Hybrid score\n            explain_score=True  # Show scoring breakdown\n        )\n    )\n    \n    print(f\"\\nAlpha = {alpha:.1f}\")\n    print(\"-\" * 80)\n    \n    for i, obj in enumerate(response.objects[:2], 1):  # Show top 2\n        score = obj.metadata.score if obj.metadata.score else 0.0\n        print(f\"{i}. {obj.properties['title']}\")\n        print(f\"   Score: {score:.4f}\")\n        print(f\"   Category: {obj.properties['category']}\")"
    },
    {
      "cell_type": "markdown",
      "id": "cell-15",
      "metadata": {},
      "source": [
        "## Section 8: Filtered Search\n",
        "\n",
        "Combine search with attribute filters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-16",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Search with attribute filters.\"\"\"\n",
        "\n",
        "# Filter by author\n",
        "response = articles.query.fetch_objects(\n",
        "    filters=Filter.by_property(\"author\").equal(\"Jane Smith\"),\n",
        "    limit=10\n",
        ")\n",
        "\n",
        "print(f\"Articles by Jane Smith: {len(response.objects)}\\n\")\n",
        "\n",
        "for obj in response.objects:\n",
        "    print(f\"- {obj.properties['title']}\")\n",
        "    print(f\"  Category: {obj.properties['category']}, Views: {obj.properties['views']:,}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-17",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Complex multi-condition filters.\"\"\"\n",
        "\n",
        "# Machine Learning OR AI articles with > 1000 views\n",
        "response = articles.query.fetch_objects(\n",
        "    filters=(\n",
        "        (\n",
        "            Filter.by_property(\"category\").equal(\"Machine Learning\") |\n",
        "            Filter.by_property(\"category\").equal(\"AI\")\n",
        "        ) &\n",
        "        Filter.by_property(\"views\").greater_than(1000)\n",
        "    ),\n",
        "    limit=10\n",
        ")\n",
        "\n",
        "print(f\"Popular ML/AI articles: {len(response.objects)}\\n\")\n",
        "\n",
        "for obj in response.objects:\n",
        "    print(f\"- {obj.properties['title']}\")\n",
        "    print(f\"  Category: {obj.properties['category']}\")\n",
        "    print(f\"  Views: {obj.properties['views']:,}, Author: {obj.properties['author']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-18",
      "metadata": {},
      "source": [
        "## Section 9: Vector Search with Filters\n",
        "\n",
        "Combine semantic similarity with filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-19",
      "metadata": {},
      "outputs": [],
      "source": "\"\"\"Filtered vector similarity search.\"\"\"\n\nquery_vector = generate_embedding(\"machine learning deep learning\")\n\n# Find similar ML articles\nresponse = articles.query.near_vector(\n    near_vector=query_vector,\n    filters=Filter.by_property(\"category\").equal(\"Machine Learning\"),\n    limit=5,\n    return_metadata=MetadataQuery(distance=True)\n)\n\nprint(\"Similar Machine Learning articles:\\n\")\n\nfor i, obj in enumerate(response.objects, 1):\n    distance = obj.metadata.distance if obj.metadata.distance else 0.0\n    print(f\"{i}. {obj.properties['title']}\")\n    print(f\"   Distance: {distance:.4f}\")\n    print(f\"   Author: {obj.properties['author']}, Views: {obj.properties['views']:,}\")\n    print()"
    },
    {
      "cell_type": "markdown",
      "id": "cell-20",
      "metadata": {},
      "source": [
        "## Section 10: Update Operations\n",
        "\n",
        "Modify existing objects with validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-21",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Update article properties with validation.\"\"\"\n",
        "\n",
        "# Get first article\n",
        "response = articles.query.fetch_objects(limit=1)\n",
        "\n",
        "if response.objects:\n",
        "    article = response.objects[0]\n",
        "    \n",
        "    print(\"Before update:\")\n",
        "    print(f\"Title: {article.properties['title']}\")\n",
        "    print(f\"Views: {article.properties['views']:,}\\n\")\n",
        "    \n",
        "    # Update view count\n",
        "    new_views = article.properties['views'] + 100\n",
        "    \n",
        "    try:\n",
        "        articles.data.update(\n",
        "            uuid=article.uuid,\n",
        "            properties={\"views\": new_views}\n",
        "        )\n",
        "        \n",
        "        print(\"SUCCESS: Updated views count\\n\")\n",
        "        \n",
        "        # Verify update\n",
        "        updated = articles.query.fetch_object_by_id(article.uuid)\n",
        "        \n",
        "        print(\"After update:\")\n",
        "        print(f\"Title: {updated.properties['title']}\")\n",
        "        print(f\"Views: {updated.properties['views']:,}\")\n",
        "        \n",
        "        if updated.properties['views'] == new_views:\n",
        "            print(\"\\nVERIFIED: Update successful\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Update failed: {e}\")\n",
        "        \n",
        "else:\n",
        "    print(\"No articles found to update\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-22",
      "metadata": {},
      "source": [
        "## Section 11: Aggregations and Analytics\n",
        "\n",
        "Compute statistics and insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-23",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Aggregate data with grouping and metrics.\"\"\"\n",
        "\n",
        "from weaviate.classes.aggregate import GroupByAggregate\n",
        "\n",
        "# Total count\n",
        "result = articles.aggregate.over_all(total_count=True)\n",
        "print(f\"Total articles: {result.total_count}\\n\")\n",
        "\n",
        "# Group by category with counts\n",
        "result = articles.aggregate.over_all(\n",
        "    group_by=GroupByAggregate(prop=\"category\")\n",
        ")\n",
        "\n",
        "print(\"Articles by category:\")\n",
        "for group in result.groups:\n",
        "    print(f\"  {group.grouped_by.value}: {group.total_count} articles\")\n",
        "\n",
        "# Group by author with view statistics\n",
        "print(\"\\nAuthor statistics:\")\n",
        "\n",
        "result = articles.aggregate.over_all(\n",
        "    group_by=GroupByAggregate(prop=\"author\")\n",
        ")\n",
        "\n",
        "for group in result.groups:\n",
        "    print(f\"  {group.grouped_by.value}: {group.total_count} articles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-24",
      "metadata": {},
      "source": [
        "## Section 12: Delete Operations\n",
        "\n",
        "Remove objects with verification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-25",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Delete article by UUID with verification.\"\"\"\n",
        "\n",
        "# Get article to delete\n",
        "response = articles.query.fetch_objects(limit=1)\n",
        "\n",
        "if response.objects:\n",
        "    article = response.objects[0]\n",
        "    \n",
        "    print(f\"Deleting: {article.properties['title']}\")\n",
        "    print(f\"UUID: {article.uuid}\\n\")\n",
        "    \n",
        "    # Count before deletion\n",
        "    before = articles.aggregate.over_all(total_count=True)\n",
        "    \n",
        "    # Delete\n",
        "    articles.data.delete_by_id(uuid=article.uuid)\n",
        "    \n",
        "    print(\"SUCCESS: Article deleted\\n\")\n",
        "    \n",
        "    # Verify deletion\n",
        "    after = articles.aggregate.over_all(total_count=True)\n",
        "    \n",
        "    print(f\"Articles before: {before.total_count}\")\n",
        "    print(f\"Articles after: {after.total_count}\")\n",
        "    \n",
        "    if after.total_count == before.total_count - 1:\n",
        "        print(\"\\nVERIFIED: Deletion successful\")\n",
        "        \n",
        "else:\n",
        "    print(\"No articles found to delete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-26",
      "metadata": {},
      "source": [
        "## Section 13: Quantization Comparison (2025)\n",
        "\n",
        "Compare different quantization techniques for memory optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-27",
      "metadata": {},
      "outputs": [],
      "source": "\"\"\"Compare quantization techniques.\"\"\"\n\nprint(\"Vector Database Quantization Techniques (2025)\\n\")\nprint(\"=\" * 80)\n\nquantization_comparison = {\n    \"RQ (Rotational Quantization)\": {\n        \"compression\": \"4x\",\n        \"recall_loss\": \"<1%\",\n        \"speed_impact\": \"Minimal\",\n        \"use_case\": \"Production default (v1.33+)\",\n        \"configuration\": \"Configure.VectorIndex.Quantizer.rq()\"\n    },\n    \"PQ (Product Quantization)\": {\n        \"compression\": \"8-16x\",\n        \"recall_loss\": \"2-5%\",\n        \"speed_impact\": \"Low\",\n        \"use_case\": \"High compression, large datasets\",\n        \"configuration\": \"Configure.VectorIndex.Quantizer.pq(segments=96)\"\n    },\n    \"BQ (Binary Quantization)\": {\n        \"compression\": \"32x\",\n        \"recall_loss\": \"10-15%\",\n        \"speed_impact\": \"Very Low\",\n        \"use_case\": \"Maximum compression, memory-constrained\",\n        \"configuration\": \"Configure.VectorIndex.Quantizer.bq()\"\n    },\n    \"SQ (Scalar Quantization)\": {\n        \"compression\": \"4x\",\n        \"recall_loss\": \"<2%\",\n        \"speed_impact\": \"Minimal\",\n        \"use_case\": \"Fast, simple quantization\",\n        \"configuration\": \"Configure.VectorIndex.Quantizer.sq()\"\n    }\n}\n\nfor method, specs in quantization_comparison.items():\n    print(f\"\\n{method}\")\n    print(\"-\" * 80)\n    print(f\"  Compression:    {specs['compression']}\")\n    print(f\"  Recall Loss:    {specs['recall_loss']}\")\n    print(f\"  Speed Impact:   {specs['speed_impact']}\")\n    print(f\"  Use Case:       {specs['use_case']}\")\n    print(f\"  Configuration:  {specs['configuration']}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"\\nRecommendation:\")\nprint(\"  - Start with RQ (default in v1.33+) for best balance\")\nprint(\"  - Use PQ for higher compression with acceptable recall loss\")\nprint(\"  - Use BQ only for extreme memory constraints\")\nprint(\"  - Always benchmark on your specific dataset\")"
    },
    {
      "cell_type": "markdown",
      "id": "cell-28",
      "metadata": {},
      "source": [
        "## Section 14: AI Agents for Intelligent Queries (2025 Preview)\n",
        "\n",
        "Use AI Agents to automatically optimize queries without manual tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-29",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Demonstrate AI Agents concepts (2025 Preview).\"\"\"\n",
        "\n",
        "print(\"Weaviate AI Agents (2025 Preview)\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "ai_agents = {\n",
        "    \"Query Agent\": {\n",
        "        \"description\": \"Automatically determines optimal query strategy\",\n",
        "        \"capabilities\": [\n",
        "            \"Selects vector vs hybrid vs keyword search\",\n",
        "            \"Auto-tunes alpha parameter for hybrid search\",\n",
        "            \"Chooses appropriate filters dynamically\",\n",
        "            \"Adjusts ef parameter based on query complexity\"\n",
        "        ],\n",
        "        \"example\": \"User asks 'machine learning papers' -> Agent chooses hybrid with alpha=0.7\"\n",
        "    },\n",
        "    \"Transformation Agent\": {\n",
        "        \"description\": \"Transforms user queries into optimal search terms\",\n",
        "        \"capabilities\": [\n",
        "            \"Expands acronyms and abbreviations\",\n",
        "            \"Adds synonyms for better coverage\",\n",
        "            \"Corrects spelling and grammar\",\n",
        "            \"Extracts key concepts from natural language\"\n",
        "        ],\n",
        "        \"example\": \"'ML algos' -> 'machine learning algorithms'\"\n",
        "    },\n",
        "    \"Personalization Agent\": {\n",
        "        \"description\": \"Personalizes search results based on user context\",\n",
        "        \"capabilities\": [\n",
        "            \"Learns from user click patterns\",\n",
        "            \"Adjusts ranking based on user preferences\",\n",
        "            \"Filters content by user role/access\",\n",
        "            \"Adapts to user expertise level\"\n",
        "        ],\n",
        "        \"example\": \"Beginner sees introductory articles first, experts see advanced content\"\n",
        "    }\n",
        "}\n",
        "\n",
        "for agent_name, info in ai_agents.items():\n",
        "    print(f\"\\n{agent_name}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{info['description']}\\n\")\n",
        "    \n",
        "    print(\"Capabilities:\")\n",
        "    for capability in info['capabilities']:\n",
        "        print(f\"  - {capability}\")\n",
        "    \n",
        "    print(f\"\\nExample: {info['example']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nNote: AI Agents are in Preview (March 2025).\")\n",
        "print(\"Check Weaviate documentation for latest availability and API details.\")\n",
        "print(\"\\nBenefits:\")\n",
        "print(\"  - Reduces manual query tuning\")\n",
        "print(\"  - Improves search relevance automatically\")\n",
        "print(\"  - Adapts to user behavior in real-time\")\n",
        "print(\"  - Lowers barrier to entry for developers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-30",
      "metadata": {},
      "source": [
        "## Section 15: Production RAG System\n",
        "\n",
        "Build a production-ready RAG system with error handling and monitoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-31",
      "metadata": {},
      "outputs": [],
      "source": "\"\"\"Create knowledge base for production RAG system.\"\"\"\n\nKB_COLLECTION = \"KnowledgeBase\"\n\n# Comprehensive knowledge base\nknowledge_base = [\n    {\n        \"title\": \"What is a Vector Database?\",\n        \"content\": \"A vector database is a specialized database designed to store and query high-dimensional vectors efficiently. Unlike traditional databases that store structured data in rows and columns, vector databases are optimized for similarity search operations. They use algorithms like HNSW (Hierarchical Navigable Small World) to perform fast approximate nearest neighbor searches. Vector databases are essential for applications like semantic search, recommendation systems, image similarity search, and RAG (Retrieval-Augmented Generation) systems.\",\n        \"author\": \"Tech Expert\",\n        \"category\": \"Database\",\n        \"views\": 500\n    },\n    {\n        \"title\": \"How does RAG work?\",\n        \"content\": \"Retrieval-Augmented Generation (RAG) combines information retrieval with text generation. First, relevant documents are retrieved from a vector database using semantic search. These documents provide context to a language model, which then generates a response. RAG improves accuracy by grounding responses in factual information rather than relying solely on the model's training data. This reduces hallucinations and enables models to access up-to-date information.\",\n        \"author\": \"AI Researcher\",\n        \"category\": \"AI\",\n        \"views\": 750\n    },\n    {\n        \"title\": \"What is HNSW?\",\n        \"content\": \"HNSW (Hierarchical Navigable Small World) is a graph-based algorithm for approximate nearest neighbor search. It creates a multi-layer graph where upper layers have sparse long-range connections and lower layers have dense short-range connections. During search, the algorithm starts at the top layer and progressively moves down, refining the search at each level. HNSW offers excellent performance with O(log N) search complexity and high recall rates (>95%). It's the default indexing algorithm in many vector databases including Weaviate.\",\n        \"author\": \"Algorithm Expert\",\n        \"category\": \"Algorithms\",\n        \"views\": 300\n    },\n    {\n        \"title\": \"Quantization in Vector Databases\",\n        \"content\": \"Quantization reduces memory footprint by compressing vectors. RQ (Rotational Quantization) is the 2025 default, offering 4x compression with <1% recall loss. PQ (Product Quantization) provides 8-16x compression with 2-5% recall loss. Binary Quantization achieves 32x compression but with 10-15% recall loss. Choose quantization based on memory constraints and acceptable accuracy trade-offs.\",\n        \"author\": \"Optimization Expert\",\n        \"category\": \"Optimization\",\n        \"views\": 450\n    },\n    {\n        \"title\": \"Hybrid Search Best Practices\",\n        \"content\": \"Hybrid search combines vector (semantic) and BM25 (keyword) search. The alpha parameter controls the balance: 0 for pure keyword, 1 for pure vector. Start with alpha=0.7 for semantic-heavy tasks. Use alpha=0.5 for balanced search. Use alpha=0.3 for keyword-heavy tasks like exact term matching. Always benchmark on your specific use case and queries.\",\n        \"author\": \"Search Expert\",\n        \"category\": \"Search\",\n        \"views\": 600\n    }\n]\n\n# Create collection with production configuration\nif client.collections.exists(KB_COLLECTION):\n    client.collections.delete(KB_COLLECTION)\n\nkb = client.collections.create(\n    name=KB_COLLECTION,\n    vectorizer_config=Configure.Vectorizer.none(),\n    \n    # Production HNSW configuration\n    vector_index_config=Configure.VectorIndex.hnsw(\n        distance_metric=VectorDistances.COSINE,\n        max_connections=64,      # Higher for better recall\n        ef_construction=256,     # Higher for better quality\n        quantizer=Configure.VectorIndex.Quantizer.rq()\n    ),\n    \n    # Enable hybrid search\n    inverted_index_config=Configure.inverted_index(\n        bm25_b=0.75,\n        bm25_k1=1.2\n    ),\n    \n    properties=[\n        Property(name=\"title\", data_type=DataType.TEXT, index_searchable=True),\n        Property(name=\"content\", data_type=DataType.TEXT, index_searchable=True),\n        Property(name=\"author\", data_type=DataType.TEXT, index_filterable=True),\n        Property(name=\"category\", data_type=DataType.TEXT, index_filterable=True),\n        Property(name=\"views\", data_type=DataType.INT, index_range_filters=True)\n    ]\n)\n\n# Insert knowledge base\nkb = client.collections.get(KB_COLLECTION)\n\nwith kb.batch.dynamic() as batch:\n    for article in knowledge_base:\n        vector = generate_embedding(article[\"title\"] + \" \" + article[\"content\"])\n        batch.add_object(properties=article, vector=vector)\n\nprint(f\"SUCCESS: Created production knowledge base\")\nprint(f\"Articles: {len(knowledge_base)}\")\nprint(f\"Configuration: HNSW (maxConnections=64) + RQ quantization + BM25\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-32",
      "metadata": {},
      "outputs": [],
      "source": "\"\"\"Production RAG retrieval with hybrid search and reranking.\"\"\"\n\ndef retrieve_context_production(\n    question: str,\n    top_k: int = 3,\n    alpha: float = 0.7,\n    category_filter: Optional[str] = None\n) -> Dict[str, Any]:\n    \"\"\"Retrieve relevant context using production-ready RAG.\n    \n    Args:\n        question: User question to answer\n        top_k: Number of documents to retrieve\n        alpha: Hybrid search parameter (0=BM25, 1=vector)\n        category_filter: Optional category filter\n        \n    Returns:\n        Dictionary with context, sources, and metadata\n    \"\"\"\n    try:\n        # Get collection\n        kb = client.collections.get(KB_COLLECTION)\n        \n        # Generate query vector (use embedding model in production)\n        query_vector = generate_embedding(question)\n        \n        # Build filter if provided\n        filters = None\n        if category_filter:\n            filters = Filter.by_property(\"category\").equal(category_filter)\n        \n        # Hybrid search with dynamic parameters\n        response = kb.query.hybrid(\n            query=question,\n            vector=query_vector,\n            alpha=alpha,\n            fusion_type=HybridFusion.RELATIVE_SCORE,\n            filters=filters,\n            limit=top_k,\n            return_metadata=MetadataQuery(\n                score=True,\n                distance=True\n            )\n        )\n        \n        # Format results\n        context_parts = []\n        sources = []\n        \n        for i, obj in enumerate(response.objects, 1):\n            score = obj.metadata.score if obj.metadata.score else 0.0\n            \n            context_parts.append(\n                f\"[Document {i}]\\n\"\n                f\"Title: {obj.properties['title']}\\n\"\n                f\"Content: {obj.properties['content']}\\n\"\n            )\n            \n            sources.append({\n                \"title\": obj.properties['title'],\n                \"author\": obj.properties['author'],\n                \"category\": obj.properties['category'],\n                \"score\": float(score),\n                \"uuid\": str(obj.uuid)\n            })\n        \n        return {\n            \"success\": True,\n            \"context\": \"\\n\".join(context_parts),\n            \"sources\": sources,\n            \"query\": question,\n            \"num_results\": len(response.objects),\n            \"alpha\": alpha\n        }\n        \n    except Exception as e:\n        return {\n            \"success\": False,\n            \"error\": str(e),\n            \"query\": question\n        }\n\n# Test production RAG\nquestion = \"How does vector database quantization work?\"\n\nprint(f\"Question: {question}\\n\")\nprint(\"=\" * 80)\n\nresult = retrieve_context_production(\n    question=question,\n    top_k=3,\n    alpha=0.7  # Semantic-heavy\n)\n\nif result[\"success\"]:\n    print(f\"\\nRetrieved {result['num_results']} relevant documents:\\n\")\n    \n    for i, source in enumerate(result[\"sources\"], 1):\n        print(f\"{i}. {source['title']}\")\n        print(f\"   Author: {source['author']} | Category: {source['category']}\")\n        print(f\"   Score: {source['score']:.4f}\")\n        print()\n    \n    print(\"=\" * 80)\n    print(\"\\nNext Step: Pass context to LLM for generation\")\n    print(\"Example: OpenAI GPT-4, Anthropic Claude, Google Gemini\")\n    print(\"\\nPrompt Template:\")\n    print(\"  'Answer this question based on the context below.\\\\n\\\\n'\")\n    print(\"  f'Question: {question}\\\\n\\\\n'\")\n    print(\"  f'Context:\\\\n{context}\\\\n\\\\n'\")\n    print(\"  'Answer:'\")\n    \nelse:\n    print(f\"ERROR: {result['error']}\")"
    },
    {
      "cell_type": "markdown",
      "id": "cell-33",
      "metadata": {},
      "source": [
        "## Section 16: Performance Monitoring and Optimization\n",
        "\n",
        "Monitor and optimize query performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-34",
      "metadata": {},
      "outputs": [],
      "source": "\"\"\"Monitor query performance and optimize.\"\"\"\n\nimport time\n\ndef benchmark_search(\n    search_type: str,\n    iterations: int = 5\n) -> Dict[str, float]:\n    \"\"\"Benchmark search performance.\n    \n    Args:\n        search_type: 'vector', 'hybrid', or 'keyword'\n        iterations: Number of test iterations\n        \n    Returns:\n        Performance metrics\n    \"\"\"\n    kb = client.collections.get(KB_COLLECTION)\n    query_vector = generate_embedding(query_text)\n    query_text = \"vector database search\"\n    \n    timings = []\n    \n    for _ in range(iterations):\n        start = time.time()\n        \n        if search_type == \"vector\":\n            kb.query.near_vector(\n                near_vector=query_vector,\n                limit=10\n            )\n        elif search_type == \"hybrid\":\n            kb.query.hybrid(\n                query=query_text,\n                vector=query_vector,\n                alpha=0.7,\n                limit=10\n            )\n        elif search_type == \"keyword\":\n            kb.query.bm25(\n                query=query_text,\n                limit=10\n            )\n        \n        elapsed = time.time() - start\n        timings.append(elapsed * 1000)  # Convert to ms\n    \n    return {\n        \"avg_ms\": np.mean(timings),\n        \"min_ms\": np.min(timings),\n        \"max_ms\": np.max(timings),\n        \"std_ms\": np.std(timings)\n    }\n\n# Benchmark different search types\nprint(\"Search Performance Benchmark\\n\")\nprint(\"=\" * 80)\n\nfor search_type in [\"vector\", \"hybrid\", \"keyword\"]:\n    metrics = benchmark_search(search_type, iterations=5)\n    \n    print(f\"\\n{search_type.upper()} SEARCH\")\n    print(f\"  Average: {metrics['avg_ms']:.2f} ms\")\n    print(f\"  Min:     {metrics['min_ms']:.2f} ms\")\n    print(f\"  Max:     {metrics['max_ms']:.2f} ms\")\n    print(f\"  Std Dev: {metrics['std_ms']:.2f} ms\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"\\nOptimization Tips:\")\nprint(\"  1. Enable RQ quantization for 4x faster queries\")\nprint(\"  2. Tune ef parameter: higher = better recall, lower = faster\")\nprint(\"  3. Use maxConnections=64 for <1M vectors\")\nprint(\"  4. Enable connection pooling for high-throughput\")\nprint(\"  5. Use batch operations for bulk inserts\")\nprint(\"  6. Monitor query latency with metrics endpoint\")"
    },
    {
      "cell_type": "markdown",
      "id": "cell-35",
      "metadata": {},
      "source": [
        "## Section 17: Cleanup\n",
        "\n",
        "Clean up resources and close connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-36",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Clean up collections and close connection gracefully.\"\"\"\n",
        "\n",
        "# Optional: Delete collections (uncomment to clean up)\n",
        "# collections_to_delete = [COLLECTION_NAME, KB_COLLECTION]\n",
        "#\n",
        "# for collection_name in collections_to_delete:\n",
        "#     if client.collections.exists(collection_name):\n",
        "#         client.collections.delete(collection_name)\n",
        "#         print(f\"Deleted '{collection_name}' collection\")\n",
        "\n",
        "# Close connection\n",
        "try:\n",
        "    client.close()\n",
        "    print(\"SUCCESS: Connection closed gracefully\")\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: Error during cleanup: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-37",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this comprehensive hands-on guide, you learned:\n",
        "\n",
        "### Core Functionality\n",
        "1. **Connection** - Production-ready connection with retry logic and gRPC\n",
        "2. **Schema** - Advanced collection configuration with HNSW tuning\n",
        "3. **CRUD** - Insert, read, update, delete with error handling\n",
        "4. **Batch Operations** - Efficient bulk operations with monitoring\n",
        "\n",
        "### Search Capabilities\n",
        "5. **Vector Search** - Semantic similarity with dynamic ef tuning\n",
        "6. **Hybrid Search (2025)** - Alpha parameter tuning for optimal results\n",
        "7. **Filters** - Complex attribute filtering and aggregations\n",
        "\n",
        "### 2025 Features\n",
        "8. **RQ Quantization** - 4x compression with minimal recall loss (default in v1.33+)\n",
        "9. **AI Agents (Preview)** - Intelligent query optimization\n",
        "10. **Production RAG** - Complete retrieval system with error handling\n",
        "\n",
        "### Optimization\n",
        "11. **Performance** - Benchmarking and monitoring\n",
        "12. **Best Practices** - Production deployment patterns\n",
        "\n",
        "## Production Checklist\n",
        "\n",
        "Before deploying to production:\n",
        "\n",
        "- [ ] Replace mock embeddings with real embedding models (OpenAI, Cohere, HuggingFace)\n",
        "- [ ] Enable RQ quantization (default in v1.33+)\n",
        "- [ ] Tune HNSW parameters for your dataset size\n",
        "- [ ] Configure hybrid search alpha for your use case\n",
        "- [ ] Implement connection pooling and retry logic\n",
        "- [ ] Set up monitoring and alerting\n",
        "- [ ] Enable authentication and authorization\n",
        "- [ ] Configure backups and disaster recovery\n",
        "- [ ] Load test with expected traffic\n",
        "- [ ] Set up multi-node cluster for high availability\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **Integrate Real Embeddings** - Use OpenAI text-embedding-3, Cohere embed-v3, or HuggingFace models\n",
        "2. **Add LLM Integration** - Connect GPT-4, Claude, or Gemini for answer generation\n",
        "3. **Implement Reranking** - Use Cohere Rerank or custom reranking models\n",
        "4. **Enable Multi-tenancy** - Isolate data per customer/team\n",
        "5. **Scale Horizontally** - Deploy multi-node Weaviate cluster\n",
        "6. **Monitor Performance** - Set up Prometheus + Grafana dashboards\n",
        "7. **Try AI Agents** - Experiment with Query/Transformation/Personalization Agents (when available)\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [Weaviate Complete Guide](weaviate-complete-guide.md) - Comprehensive documentation with architecture diagrams\n",
        "- [Official Documentation](https://weaviate.io/developers/weaviate) - Latest features and API reference\n",
        "- [Python Client Docs](https://weaviate.io/developers/weaviate/client-libraries/python) - v4 client documentation\n",
        "- [HNSW Tuning Guide](https://weaviate.io/developers/weaviate/config-refs/schema/vector-index#hnsw-index-parameters) - Parameter optimization\n",
        "- [Quantization Guide](https://weaviate.io/developers/weaviate/concepts/vector-quantization) - RQ, PQ, BQ, SQ comparison\n",
        "- [Production Best Practices](https://weaviate.io/developers/weaviate/installation/cluster) - Deployment patterns\n",
        "- [Example Applications](https://github.com/weaviate/weaviate-examples) - Real-world use cases\n",
        "\n",
        "---\n",
        "\n",
        "**Weaviate Hands-On Guide - 2025 Edition**\n",
        "\n",
        "*Updated with latest features: RQ quantization, hybrid search alpha tuning, AI Agents preview, and production best practices.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}