# Feature Engineering & Feature Selection - Complete Guide
## Industry-Standard Techniques with Examples (2024-2025)

---

## Table of Contents
1. Feature Engineering Fundamentals
2. Feature Transformation Techniques
3. Feature Creation & Extraction
4. Feature Selection Methods
5. Feature Engineering for NLP/LLMs
6. Real-World Examples
7. Best Practices & Interview Topics

---

## Part 1: Feature Engineering Fundamentals

### What is Feature Engineering?

**Definition:** The process of using domain knowledge to transform raw data into features that better represent the underlying problem to predictive models.

**Why it Matters:**
> "Even the most advanced algorithms can fail if trained on poorly designed features."
> â€” Industry Best Practice, 2024

**Impact:**
- Can improve model accuracy by 10-50%
- Reduces training time
- Improves model interpretability
- Often more impactful than algorithm selection

### The Feature Engineering Pipeline

```
Raw Data â†’ Cleaning â†’ Transformation â†’ Creation â†’ Selection â†’ Model Training
```

---

## Part 2: Feature Transformation Techniques

### 2.1 Handling Missing Values

**Industry Standard Approaches:**

```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer

# Example dataset
df = pd.DataFrame({
    'age': [25, 30, np.nan, 45, np.nan],
    'salary': [50000, 60000, 55000, np.nan, 70000],
    'department': ['IT', 'HR', np.nan, 'IT', 'Finance']
})

# Method 1: Simple Imputation
# For numerical features
imputer_num = SimpleImputer(strategy='median')  # mean, median, most_frequent
df['age'] = imputer_num.fit_transform(df[['age']])

# For categorical features
imputer_cat = SimpleImputer(strategy='most_frequent')
df['department'] = imputer_cat.fit_transform(df[['department']].values.reshape(-1,1))

# Method 2: KNN Imputation (more sophisticated)
knn_imputer = KNNImputer(n_neighbors=2)
df[['age', 'salary']] = knn_imputer.fit_transform(df[['age', 'salary']])

# Method 3: Missing Indicator (capture missingness as signal)
from sklearn.impute import MissingIndicator

indicator = MissingIndicator()
missing_mask = indicator.fit_transform(df)
df['age_was_missing'] = missing_mask[:, 0].astype(int)

print(df)
```

**When to Use Each:**
- **Mean/Median:** Quick, works for normally distributed data
- **Most Frequent:** Categorical features
- **KNN:** When features are correlated
- **Missing Indicator:** When missingness itself is informative

### 2.2 Encoding Categorical Variables

#### One-Hot Encoding (OHE)

**Best for:** Low cardinality (< 10-15 categories)

```python
from sklearn.preprocessing import OneHotEncoder
import pandas as pd

data = pd.DataFrame({
    'city': ['NYC', 'LA', 'SF', 'NYC', 'LA'],
    'price': [100, 150, 200, 120, 180]
})

# Method 1: Pandas get_dummies
encoded = pd.get_dummies(data, columns=['city'], prefix='city')
print(encoded)

# Method 2: Scikit-learn (better for production)
encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop first to avoid multicollinearity
encoded_array = encoder.fit_transform(data[['city']])
encoded_df = pd.DataFrame(
    encoded_array,
    columns=encoder.get_feature_names_out(['city'])
)
```

#### Label Encoding

**Best for:** Ordinal categories (has ordering)

```python
from sklearn.preprocessing import LabelEncoder

data = pd.DataFrame({
    'size': ['Small', 'Medium', 'Large', 'Small', 'Medium'],
    'rating': ['Poor', 'Good', 'Excellent', 'Good', 'Poor']
})

# For ordinal data with meaningful order
size_mapping = {'Small': 1, 'Medium': 2, 'Large': 3}
data['size_encoded'] = data['size'].map(size_mapping)

# Using LabelEncoder
encoder = LabelEncoder()
data['rating_encoded'] = encoder.fit_transform(data['rating'])

print(data)
```

#### Target Encoding

**Best for:** High cardinality categorical features

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold

def target_encode(df, column, target, n_folds=5):
    """
    Target encoding with cross-validation to prevent overfitting

    Industry standard: Use CV to avoid data leakage!
    """
    encoded = pd.Series(index=df.index, dtype=float)
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)

    for train_idx, val_idx in kf.split(df):
        # Calculate mean target for each category in training fold
        target_means = df.iloc[train_idx].groupby(column)[target].mean()

        # Apply to validation fold
        encoded.iloc[val_idx] = df.iloc[val_idx][column].map(target_means)

    # Handle unseen categories with global mean
    global_mean = df[target].mean()
    encoded = encoded.fillna(global_mean)

    return encoded

# Example
data = pd.DataFrame({
    'user_id': [1, 2, 3, 1, 2, 4, 5, 3, 4, 5],
    'purchased': [1, 0, 1, 1, 0, 1, 0, 1, 1, 0]
})

data['user_id_encoded'] = target_encode(data, 'user_id', 'purchased')
print(data)
```

#### Frequency Encoding

**Best for:** High cardinality without target leakage

```python
def frequency_encode(df, column):
    """
    Encode based on frequency of occurrence
    """
    freq = df[column].value_counts(normalize=True)
    return df[column].map(freq)

data = pd.DataFrame({
    'category': ['A', 'B', 'A', 'C', 'A', 'B', 'D']
})

data['category_freq'] = frequency_encode(data, 'category')
print(data)
# A appears 3/7 times = 0.428
# B appears 2/7 times = 0.286
```

### 2.3 Numerical Feature Transformations

#### Scaling and Normalization

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

data = pd.DataFrame({
    'age': [25, 30, 35, 40, 45],
    'salary': [50000, 60000, 55000, 80000, 70000],
    'experience': [2, 5, 4, 10, 8]
})

# StandardScaler (Z-score normalization): mean=0, std=1
# Best for: Normally distributed data, when outliers are not extreme
scaler_standard = StandardScaler()
data['salary_standard'] = scaler_standard.fit_transform(data[['salary']])

# MinMaxScaler: scales to [0, 1]
# Best for: When you need bounded values, neural networks
scaler_minmax = MinMaxScaler()
data['age_minmax'] = scaler_minmax.fit_transform(data[['age']])

# RobustScaler: uses median and IQR (outlier-resistant)
# Best for: Data with outliers
scaler_robust = RobustScaler()
data['experience_robust'] = scaler_robust.fit_transform(data[['experience']])

print(data)
```

#### Log Transformation

**Best for:** Skewed distributions, reducing impact of outliers

```python
import numpy as np
import matplotlib.pyplot as plt

data = pd.DataFrame({
    'income': [30000, 45000, 50000, 500000, 60000, 75000, 1000000]
})

# Log transform to handle skewness
data['income_log'] = np.log1p(data['income'])  # log1p = log(1 + x), handles 0 values

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(12, 4))
data['income'].hist(bins=20, ax=axes[0])
axes[0].set_title('Original Distribution')
data['income_log'].hist(bins=20, ax=axes[1])
axes[1].set_title('Log-Transformed Distribution')
```

#### Box-Cox and Yeo-Johnson Transformations

```python
from sklearn.preprocessing import PowerTransformer

# Box-Cox: only for positive values
transformer_boxcox = PowerTransformer(method='box-cox')
data_positive = pd.DataFrame({'value': [1, 5, 10, 15, 100]})
data_positive['transformed'] = transformer_boxcox.fit_transform(data_positive[['value']])

# Yeo-Johnson: works with negative values too
transformer_yeo = PowerTransformer(method='yeo-johnson')
data_any = pd.DataFrame({'value': [-5, 0, 5, 10, 100]})
data_any['transformed'] = transformer_yeo.fit_transform(data_any[['value']])

print(data_any)
```

### 2.4 Binning/Discretization

```python
from sklearn.preprocessing import KBinsDiscretizer
import pandas as pd

data = pd.DataFrame({
    'age': [18, 25, 35, 45, 55, 65, 75]
})

# Method 1: Manual binning
data['age_group'] = pd.cut(
    data['age'],
    bins=[0, 30, 50, 100],
    labels=['Young', 'Middle', 'Senior']
)

# Method 2: Equal-width binning
binner = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
data['age_binned'] = binner.fit_transform(data[['age']])

# Method 3: Quantile-based binning (equal frequency)
data['age_quantile'] = pd.qcut(data['age'], q=3, labels=['Low', 'Medium', 'High'])

print(data)
```

---

## Part 3: Feature Creation & Extraction

### 3.1 Polynomial Features

```python
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd

data = pd.DataFrame({
    'x1': [1, 2, 3, 4],
    'x2': [2, 4, 6, 8]
})

# Create polynomial features (interactions + powers)
poly = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly.fit_transform(data)

feature_names = poly.get_feature_names_out(['x1', 'x2'])
poly_df = pd.DataFrame(poly_features, columns=feature_names)
print(poly_df)
# Creates: x1, x2, x1^2, x1*x2, x2^2
```

### 3.2 Interaction Features

```python
# Manual interaction creation
data['x1_x2_interaction'] = data['x1'] * data['x2']
data['x1_div_x2'] = data['x1'] / (data['x2'] + 1e-10)  # avoid division by zero

# Domain-specific features
ecommerce = pd.DataFrame({
    'price': [100, 150, 200],
    'quantity': [2, 1, 3],
    'discount': [10, 20, 15]
})

ecommerce['total_cost'] = ecommerce['price'] * ecommerce['quantity']
ecommerce['final_price'] = ecommerce['price'] * (1 - ecommerce['discount']/100)
ecommerce['avg_item_price'] = ecommerce['total_cost'] / ecommerce['quantity']

print(ecommerce)
```

### 3.3 Date/Time Features

```python
import pandas as pd

data = pd.DataFrame({
    'transaction_date': pd.date_range('2024-01-01', periods=10, freq='D')
})

# Extract temporal features
data['year'] = data['transaction_date'].dt.year
data['month'] = data['transaction_date'].dt.month
data['day'] = data['transaction_date'].dt.day
data['day_of_week'] = data['transaction_date'].dt.dayofweek  # Monday=0
data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)
data['quarter'] = data['transaction_date'].dt.quarter
data['day_of_year'] = data['transaction_date'].dt.dayofyear

# Cyclical encoding (important for ML models!)
data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)
data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)

print(data.head())
```

### 3.4 Text Features (Traditional NLP)

```python
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import pandas as pd

documents = [
    "Machine learning is amazing",
    "Deep learning uses neural networks",
    "Machine learning and deep learning overlap"
]

# TF-IDF Features
tfidf = TfidfVectorizer(max_features=10, ngram_range=(1, 2))
tfidf_matrix = tfidf.fit_transform(documents)
tfidf_df = pd.DataFrame(
    tfidf_matrix.toarray(),
    columns=tfidf.get_feature_names_out()
)
print("TF-IDF Features:")
print(tfidf_df)

# Additional text features
df = pd.DataFrame({'text': documents})
df['text_length'] = df['text'].str.len()
df['word_count'] = df['text'].str.split().str.len()
df['avg_word_length'] = df['text_length'] / df['word_count']
df['num_uppercase'] = df['text'].str.count(r'[A-Z]')
df['num_special_chars'] = df['text'].str.count(r'[^a-zA-Z0-9\s]')

print("\nText statistics:")
print(df)
```

---

## Part 4: Feature Selection Methods

### 4.1 Filter Methods

#### Variance Threshold

```python
from sklearn.feature_selection import VarianceThreshold
import pandas as pd
import numpy as np

# Remove low-variance features
data = pd.DataFrame({
    'f1': [1, 1, 1, 1, 1],  # Zero variance
    'f2': [1, 2, 1, 2, 1],  # Low variance
    'f3': [1, 5, 10, 15, 20]  # High variance
})

selector = VarianceThreshold(threshold=0.5)
selected_features = selector.fit_transform(data)

print(f"Original shape: {data.shape}")
print(f"After variance threshold: {selected_features.shape}")
print(f"Selected features: {data.columns[selector.get_support()]}")
```

#### Correlation-Based Selection

```python
import seaborn as sns
import matplotlib.pyplot as plt

# Remove highly correlated features
data = pd.DataFrame({
    'f1': [1, 2, 3, 4, 5],
    'f2': [2, 4, 6, 8, 10],  # Highly correlated with f1
    'f3': [5, 4, 3, 2, 1]    # Negative correlation
})

# Calculate correlation matrix
corr_matrix = data.corr().abs()

# Select upper triangle
upper_tri = corr_matrix.where(
    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
)

# Find features with correlation > 0.95
to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]

print(f"Features to drop due to high correlation: {to_drop}")

# Visualize
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
```

#### Univariate Statistical Tests

```python
from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif
from sklearn.datasets import make_classification

# Generate sample data
X, y = make_classification(n_samples=100, n_features=20, n_informative=5, random_state=42)

# For classification: chi-squared test (for non-negative features)
selector_chi2 = SelectKBest(chi2, k=5)
X_selected_chi2 = selector_chi2.fit_transform(X, y)

# F-test (ANOVA)
selector_f = SelectKBest(f_classif, k=5)
X_selected_f = selector_f.fit_transform(X, y)

# Mutual Information (can capture non-linear relationships!)
selector_mi = SelectKBest(mutual_info_classif, k=5)
X_selected_mi = selector_mi.fit_transform(X, y)

# Get feature scores
scores = pd.DataFrame({
    'feature': range(X.shape[1]),
    'chi2_score': selector_chi2.scores_,
    'f_score': selector_f.scores_,
    'mi_score': selector_mi.scores_
})

print(scores.sort_values('mi_score', ascending=False).head(10))
```

### 4.2 Wrapper Methods

#### Recursive Feature Elimination (RFE)

```python
from sklearn.feature_selection import RFE, RFECV
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold

# Generate data
X, y = make_classification(n_samples=200, n_features=20, n_informative=10, random_state=42)

# RFE with fixed number of features
estimator = RandomForestClassifier(n_estimators=100, random_state=42)
rfe = RFE(estimator=estimator, n_features_to_select=10)
rfe.fit(X, y)

print(f"Selected features (RFE): {rfe.support_}")
print(f"Feature ranking: {rfe.ranking_}")

# RFECV - automatically finds optimal number of features using CV
rfecv = RFECV(
    estimator=estimator,
    step=1,
    cv=StratifiedKFold(5),
    scoring='accuracy',
    n_jobs=-1
)
rfecv.fit(X, y)

print(f"Optimal number of features: {rfecv.n_features_}")
print(f"Selected features (RFECV): {rfecv.support_}")

# Plot number of features vs. CV scores
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1),
         rfecv.cv_results_['mean_test_score'])
plt.xlabel('Number of Features')
plt.ylabel('CV Score')
plt.title('Recursive Feature Elimination with CV')
plt.grid(True)
```

#### Sequential Feature Selection

```python
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LogisticRegression

# Forward selection
sfs_forward = SequentialFeatureSelector(
    LogisticRegression(max_iter=1000),
    n_features_to_select=5,
    direction='forward',
    cv=5,
    n_jobs=-1
)
sfs_forward.fit(X, y)

print(f"Forward selection features: {sfs_forward.support_}")

# Backward elimination
sfs_backward = SequentialFeatureSelector(
    LogisticRegression(max_iter=1000),
    n_features_to_select=5,
    direction='backward',
    cv=5,
    n_jobs=-1
)
sfs_backward.fit(X, y)

print(f"Backward elimination features: {sfs_backward.support_}")
```

### 4.3 Embedded Methods

#### L1 Regularization (Lasso)

```python
from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler

# Standardize features (important for L1!)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Lasso with CV to find optimal alpha
lasso = LassoCV(cv=5, random_state=42, n_jobs=-1)
lasso.fit(X_scaled, y)

# Features with non-zero coefficients
selected_features = np.abs(lasso.coef_) > 1e-5

print(f"Lasso selected {selected_features.sum()} features")
print(f"Feature coefficients: {lasso.coef_}")

# Visualize coefficients
plt.figure(figsize=(12, 6))
plt.bar(range(len(lasso.coef_)), lasso.coef_)
plt.xlabel('Feature Index')
plt.ylabel('Coefficient')
plt.title(f'Lasso Coefficients (alpha={lasso.alpha_:.4f})')
plt.grid(True)
```

#### Tree-Based Feature Importance

```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.feature_selection import SelectFromModel
import matplotlib.pyplot as plt

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)

# Get feature importances
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

# Plot
plt.figure(figsize=(12, 6))
plt.bar(range(X.shape[1]), importances[indices])
plt.xlabel('Feature Index')
plt.ylabel('Importance')
plt.title('Random Forest Feature Importances')
plt.grid(True)

# Select features based on importance threshold
selector = SelectFromModel(rf, threshold='median', prefit=True)
X_selected = selector.transform(X)

print(f"Original features: {X.shape[1]}")
print(f"Selected features: {X_selected.shape[1]}")
print(f"Selected feature mask: {selector.get_support()}")
```

---

## Part 5: Feature Engineering for NLP/LLMs (2024 Best Practices)

### 5.1 Traditional NLP Features

```python
import re
from sklearn.feature_extraction.text import TfidfVectorizer

def extract_text_features(text):
    """Extract traditional NLP features"""
    features = {}

    # Basic statistics
    features['char_count'] = len(text)
    features['word_count'] = len(text.split())
    features['avg_word_length'] = features['char_count'] / (features['word_count'] + 1)

    # Linguistic features
    features['uppercase_ratio'] = sum(1 for c in text if c.isupper()) / (features['char_count'] + 1)
    features['digit_count'] = sum(1 for c in text if c.isdigit())
    features['special_char_count'] = len(re.findall(r'[^a-zA-Z0-9\s]', text))

    # Sentence features
    features['sentence_count'] = len(re.split(r'[.!?]+', text))
    features['avg_sentence_length'] = features['word_count'] / (features['sentence_count'] + 1)

    # Question/exclamation
    features['has_question'] = int('?' in text)
    features['has_exclamation'] = int('!' in text)

    return features

# Example
text = "Machine Learning is amazing! Can you believe it?"
features = extract_text_features(text)
print(features)
```

### 5.2 LLM Embeddings (2024 Industry Standard)

```python
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Load pre-trained model
model = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dim embeddings

texts = [
    "Machine learning is a subset of AI",
    "Deep learning uses neural networks",
    "Python is a programming language"
]

# Generate embeddings
embeddings = model.encode(texts)

print(f"Embedding shape: {embeddings.shape}")  # (3, 384)

# Use embeddings as features for downstream tasks
from sklearn.linear_model import LogisticRegression

# Example: Sentiment classification
X_train_embeddings = embeddings  # Your training embeddings
y_train = [1, 1, 0]  # Your labels

clf = LogisticRegression()
clf.fit(X_train_embeddings, y_train)
```

### 5.3 OpenAI Embeddings

```python
# Example with OpenAI API (requires API key)
import openai

def get_openai_embedding(text, model="text-embedding-ada-002"):
    """
    Get OpenAI embeddings (1536 dimensions)
    Industry standard for semantic search and RAG
    """
    response = openai.Embedding.create(
        input=text,
        model=model
    )
    return response['data'][0]['embedding']

# Example usage
# text = "Feature engineering is crucial for ML success"
# embedding = get_openai_embedding(text)
# print(len(embedding))  # 1536
```

### 5.4 Domain-Specific Feature Engineering

```python
# Example: E-commerce product description features
def extract_product_features(description, price, category):
    """Domain-specific features for e-commerce"""
    features = {}

    # Text features
    features.update(extract_text_features(description))

    # Domain-specific
    keywords = ['new', 'sale', 'limited', 'exclusive', 'premium']
    for keyword in keywords:
        features[f'has_{keyword}'] = int(keyword.lower() in description.lower())

    # Price features
    features['price'] = price
    features['price_log'] = np.log1p(price)
    features['is_premium'] = int(price > 100)

    # Category
    features['category'] = category

    return features

# Example
product_features = extract_product_features(
    description="Brand new premium laptop on sale!",
    price=999,
    category="Electronics"
)
print(product_features)
```

---

## Part 6: Real-World Examples

### Example 1: Credit Risk Prediction

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Sample credit data
data = pd.DataFrame({
    'age': [25, 35, 45, 30, 55],
    'income': [30000, 50000, 80000, 45000, 90000],
    'loan_amount': [5000, 15000, 25000, 10000, 30000],
    'employment_length': [2, 5, 10, 3, 15],
    'num_credit_lines': [1, 3, 5, 2, 6],
    'late_payments': [0, 1, 0, 2, 0],
    'default': [0, 0, 0, 1, 0]  # Target
})

# Feature engineering
data['debt_to_income'] = data['loan_amount'] / data['income']
data['income_per_year'] = data['income'] / data['age']
data['credit_utilization'] = data['loan_amount'] / (data['num_credit_lines'] * 5000)
data['risk_score'] = (
    data['late_payments'] * 0.3 +
    data['debt_to_income'] * 0.5 +
    (1 - data['employment_length'] / data['age']) * 0.2
)

# Age binning
data['age_group'] = pd.cut(data['age'], bins=[0, 30, 45, 100], labels=['Young', 'Middle', 'Senior'])

print(data)
```

### Example 2: Time Series Features for Sales Forecasting

```python
# Sales forecasting features
sales_data = pd.DataFrame({
    'date': pd.date_range('2024-01-01', periods=30, freq='D'),
    'sales': np.random.randint(100, 500, 30)
})

# Lag features
sales_data['sales_lag_1'] = sales_data['sales'].shift(1)
sales_data['sales_lag_7'] = sales_data['sales'].shift(7)

# Rolling statistics
sales_data['sales_rolling_mean_7'] = sales_data['sales'].rolling(window=7).mean()
sales_data['sales_rolling_std_7'] = sales_data['sales'].rolling(window=7).std()

# Trend
sales_data['sales_diff'] = sales_data['sales'].diff()

# Seasonal features
sales_data['day_of_week'] = sales_data['date'].dt.dayofweek
sales_data['is_weekend'] = sales_data['day_of_week'].isin([5, 6]).astype(int)

print(sales_data.head(10))
```

---

## Part 7: Best Practices & Interview Topics

### Best Practices (Industry Standard 2024)

1. **Understand Your Data First**
   - EDA before feature engineering
   - Understand distributions, correlations, missing patterns

2. **Domain Knowledge is Key**
   - Consult domain experts
   - Research industry-specific features

3. **Avoid Data Leakage**
   - Fit transformers on training data only
   - Use pipelines to prevent leakage

```python
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('selector', SelectKBest(k=10)),
    ('classifier', LogisticRegression())
])

# Fit only on training data
pipeline.fit(X_train, y_train)
# Transform test data using fitted parameters
y_pred = pipeline.predict(X_test)
```

4. **Feature Engineering is Iterative**
   - Start simple
   - Measure impact
   - Iterate based on model performance

5. **Document Everything**
   - Why features were created
   - Domain logic behind features
   - Expected impact

6. **Automate When Possible**
   - Use libraries: FeatureTools, tsfresh
   - Create reusable functions

### Interview Topics

**Questions You Might Face:**

1. **"How do you handle imbalanced categorical features?"**
   - Use frequency/target encoding instead of OHE
   - Group rare categories into "Other"

2. **"Explain the difference between filter, wrapper, and embedded methods"**
   - Filter: Statistical tests (fast, independent of model)
   - Wrapper: Use model for selection (slow, model-specific)
   - Embedded: Selection during training (efficient, model-specific)

3. **"How do you prevent data leakage in feature engineering?"**
   - Use pipelines
   - Fit only on training data
   - Be careful with target encoding

4. **"What's your approach to feature engineering for a new problem?"**
   1. Understand business problem
   2. EDA to understand data
   3. Create baseline features
   4. Domain-specific features
   5. Automated feature generation
   6. Feature selection
   7. Iterate based on performance

### Your Experience to Highlight

**From your resume:**
- "Built RAG evaluation framework comparing chunking strategies, embeddings, re-rankers"
- "Processed petabyte-scale unstructured data using advanced NLP pipelines and vector embeddings"

**Connect to:**
- Embedding-based features (LLM embeddings)
- Feature selection for RAG (which embeddings performed best)
- Scale considerations (feature engineering at petabyte scale)

---

## Summary

### Key Takeaways

1. **Feature Engineering > Algorithm Selection** (often!)
2. **Domain Knowledge is Critical**
3. **Prevent Data Leakage** with pipelines
4. **Iterate and Measure** impact
5. **2024 Trend:** LLM embeddings replacing manual NLP features

### Tools & Libraries

- **Scikit-learn:** StandardScaler, OneHotEncoder, SelectKBest, RFE
- **Pandas:** Data manipulation, feature creation
- **Category Encoders:** Advanced encoding methods
- **FeatureTools:** Automated feature engineering
- **Sentence Transformers:** Modern embeddings
- **OpenAI API:** State-of-the-art embeddings

### For Your Interview

Be ready to discuss:
- Your RAG evaluation framework (feature selection for embeddings!)
- Feature engineering for NLP at scale
- Trade-offs between different encoding methods
- How you measured feature importance in production systems
- Modern approaches (LLM embeddings vs traditional features)

You're now equipped with industry-standard feature engineering knowledge! ðŸš€
