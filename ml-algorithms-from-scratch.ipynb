{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Algorithms From Scratch - Cisco Coding Round Prep\n",
    "\n",
    "## Top 4 ML Algorithms Asked in Interviews\n",
    "\n",
    "Based on 2025 interview trends, these are the **ONLY 4 algorithms commonly asked**:\n",
    "1. **Linear Regression**\n",
    "2. **Logistic Regression** \n",
    "3. **K-Nearest Neighbors (k-NN)**\n",
    "4. **K-Means Clustering**\n",
    "\n",
    "**Key Rule:** Interviewers expect you to implement these **WITHOUT sklearn** - only NumPy allowed!\n",
    "\n",
    "### Interview Examples from FAANG:\n",
    "- **Uber**: \"Write AUC from scratch using vanilla Python\"\n",
    "- **Google**: \"Write K-Means using NumPy only\"\n",
    "- **Startup**: \"Code Gradient Descent from scratch using NumPy and SciPy only\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports - ONLY these allowed in most interviews\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For testing only (won't be available in interview)\n",
    "from sklearn.datasets import make_regression, make_classification, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Algorithm 1: Linear Regression\n",
    "\n",
    "**Time:** 20-25 minutes  \n",
    "**Difficulty:** Easy-Medium  \n",
    "**Asked by:** Google, Meta, Amazon\n",
    "\n",
    "Implement Linear Regression using:\n",
    "1. Normal Equation (closed-form)\n",
    "2. Gradient Descent (iterative)\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "**Hypothesis:** $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + ... + \\theta_n x_n = \\theta^T x$\n",
    "\n",
    "**Cost Function (MSE):** $J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "**Normal Equation:** $\\theta = (X^T X)^{-1} X^T y$\n",
    "\n",
    "**Gradient:** $\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Normal Equation (Closed-Form Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionNormal:\n",
    "    \"\"\"\n",
    "    Linear Regression using Normal Equation\n",
    "    \n",
    "    Time Complexity: O(n^3) due to matrix inversion\n",
    "    Space Complexity: O(n^2)\n",
    "    \n",
    "    Pros: No hyperparameters, exact solution\n",
    "    Cons: Slow for large n, doesn't work if X^T X is singular\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit using Normal Equation: Î¸ = (X^T X)^(-1) X^T y\n",
    "        \n",
    "        Args:\n",
    "            X: (m, n) feature matrix\n",
    "            y: (m,) target vector\n",
    "        \"\"\"\n",
    "        # Add intercept term (column of ones)\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        # Normal equation: Î¸ = (X^T X)^(-1) X^T y\n",
    "        self.theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using Î¸^T x\n",
    "        \"\"\"\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b @ self.theta\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R^2 score\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Test\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegressionNormal()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Theta (weights): {model.theta}\")\n",
    "print(f\"R^2 Score: {model.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train, y_train, alpha=0.5, label='Training data')\n",
    "plt.plot(X_train, model.predict(X_train), 'r-', label='Fitted line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Linear Regression - Normal Equation')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "y_pred = model.predict(X_test)\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Predictions vs True Values')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Gradient Descent (Iterative Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    \"\"\"\n",
    "    Linear Regression using Gradient Descent\n",
    "    \n",
    "    Time Complexity: O(m * n * iterations)\n",
    "    Space Complexity: O(n)\n",
    "    \n",
    "    Pros: Works for large datasets, can use mini-batch/stochastic variants\n",
    "    Cons: Requires hyperparameter tuning (learning rate, iterations)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, tolerance=1e-6):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.theta = None\n",
    "        self.cost_history = []\n",
    "    \n",
    "    def fit(self, X, y, verbose=False):\n",
    "        \"\"\"\n",
    "        Fit using Gradient Descent\n",
    "        \n",
    "        Update rule: Î¸ = Î¸ - Î± * (1/m) * X^T * (X*Î¸ - y)\n",
    "        \"\"\"\n",
    "        # Add intercept\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        m, n = X_b.shape\n",
    "        \n",
    "        # Initialize theta randomly\n",
    "        self.theta = np.random.randn(n)\n",
    "        \n",
    "        # Gradient descent\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # Predictions\n",
    "            predictions = X_b @ self.theta\n",
    "            \n",
    "            # Errors\n",
    "            errors = predictions - y\n",
    "            \n",
    "            # Compute cost (MSE)\n",
    "            cost = (1/(2*m)) * np.sum(errors ** 2)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Compute gradient: (1/m) * X^T * errors\n",
    "            gradient = (1/m) * X_b.T @ errors\n",
    "            \n",
    "            # Update parameters\n",
    "            self.theta = self.theta - self.learning_rate * gradient\n",
    "            \n",
    "            # Early stopping\n",
    "            if iteration > 0 and abs(self.cost_history[-1] - self.cost_history[-2]) < self.tolerance:\n",
    "                if verbose:\n",
    "                    print(f\"Converged at iteration {iteration}\")\n",
    "                break\n",
    "            \n",
    "            if verbose and iteration % 100 == 0:\n",
    "                print(f\"Iteration {iteration}: Cost = {cost:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b @ self.theta\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Test\n",
    "model_gd = LinearRegressionGD(learning_rate=0.01, n_iterations=1000)\n",
    "model_gd.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "print(f\"\\nTheta (weights): {model_gd.theta}\")\n",
    "print(f\"R^2 Score: {model_gd.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Plot cost history\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_gd.cost_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.title('Cost Function Convergence')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test, y_test, alpha=0.5, label='Test data')\n",
    "plt.plot(X_test, model_gd.predict(X_test), 'r-', label='Predictions')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Test Set Predictions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interview Discussion Points\n",
    "\n",
    "**Q: When to use Normal Equation vs Gradient Descent?**\n",
    "- **Normal Equation:** n < 10,000 features, need exact solution, no hyperparameter tuning\n",
    "- **Gradient Descent:** Large datasets, n > 10,000, can use mini-batch/SGD, more scalable\n",
    "\n",
    "**Q: How to improve Gradient Descent?**\n",
    "1. **Feature Scaling:** Normalize features to [0, 1] or standardize to mean=0, std=1\n",
    "2. **Learning Rate Scheduling:** Decrease learning rate over time\n",
    "3. **Mini-batch/SGD:** Use subsets of data for faster updates\n",
    "4. **Momentum:** Add momentum to escape local minima\n",
    "\n",
    "**Q: What if X^T X is singular (non-invertible)?**\n",
    "- Use **Regularization** (Ridge/Lasso)\n",
    "- Remove redundant/correlated features\n",
    "- Use SVD decomposition instead of inverse\n",
    "- Switch to Gradient Descent\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2: Logistic Regression\n",
    "\n",
    "**Time:** 25-30 minutes  \n",
    "**Difficulty:** Medium  \n",
    "**Asked by:** Facebook, Google, Uber\n",
    "\n",
    "Binary classification using logistic regression.\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "**Hypothesis:** $h_\\theta(x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}}$\n",
    "\n",
    "**Cost Function (Cross-Entropy):**  \n",
    "$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)}))]$\n",
    "\n",
    "**Gradient:**  \n",
    "$\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression for Binary Classification\n",
    "    \n",
    "    Uses Gradient Descent with Cross-Entropy loss\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, tolerance=1e-6):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.theta = None\n",
    "        self.cost_history = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Sigmoid function with numerical stability\n",
    "        \n",
    "        Ïƒ(z) = 1 / (1 + e^(-z))\n",
    "        \"\"\"\n",
    "        # Clip to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def compute_cost(self, X, y, theta):\n",
    "        \"\"\"\n",
    "        Cross-Entropy Cost Function\n",
    "        \n",
    "        J(Î¸) = -1/m * Î£[y*log(h) + (1-y)*log(1-h)]\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        h = self.sigmoid(X @ theta)\n",
    "        \n",
    "        # Add small epsilon to prevent log(0)\n",
    "        epsilon = 1e-10\n",
    "        cost = -1/m * np.sum(\n",
    "            y * np.log(h + epsilon) + (1 - y) * np.log(1 - h + epsilon)\n",
    "        )\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def fit(self, X, y, verbose=False):\n",
    "        \"\"\"\n",
    "        Fit using Gradient Descent\n",
    "        \"\"\"\n",
    "        # Add intercept\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        m, n = X_b.shape\n",
    "        \n",
    "        # Initialize theta\n",
    "        self.theta = np.zeros(n)\n",
    "        \n",
    "        # Gradient descent\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # Predictions\n",
    "            h = self.sigmoid(X_b @ self.theta)\n",
    "            \n",
    "            # Errors\n",
    "            errors = h - y\n",
    "            \n",
    "            # Compute cost\n",
    "            cost = self.compute_cost(X_b, y, self.theta)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Compute gradient\n",
    "            gradient = (1/m) * X_b.T @ errors\n",
    "            \n",
    "            # Update parameters\n",
    "            self.theta = self.theta - self.learning_rate * gradient\n",
    "            \n",
    "            # Early stopping\n",
    "            if iteration > 0 and abs(self.cost_history[-1] - self.cost_history[-2]) < self.tolerance:\n",
    "                if verbose:\n",
    "                    print(f\"Converged at iteration {iteration}\")\n",
    "                break\n",
    "            \n",
    "            if verbose and iteration % 100 == 0:\n",
    "                print(f\"Iteration {iteration}: Cost = {cost:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return probabilities\n",
    "        \"\"\"\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return self.sigmoid(X_b @ self.theta)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Return binary predictions\n",
    "        \"\"\"\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate accuracy\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "# Test\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
    "                          n_informative=2, random_state=42, n_clusters_per_class=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_lr = LogisticRegression(learning_rate=0.1, n_iterations=1000)\n",
    "model_lr.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {model_lr.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Visualize decision boundary\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Cost history\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(model_lr.cost_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost (Cross-Entropy)')\n",
    "plt.title('Cost Function Convergence')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 2: Decision boundary\n",
    "plt.subplot(1, 3, 2)\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "Z = model_lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k', alpha=0.7)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary')\n",
    "\n",
    "# Plot 3: Probability distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "probs = model_lr.predict_proba(X_test)\n",
    "plt.hist(probs[y_test == 0], bins=20, alpha=0.5, label='Class 0')\n",
    "plt.hist(probs[y_test == 1], bins=20, alpha=0.5, label='Class 1')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Probability Distribution')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interview Discussion Points\n",
    "\n",
    "**Q: Why can't we use MSE for logistic regression?**\n",
    "- MSE creates non-convex cost function with logistic hypothesis\n",
    "- Gradient descent might get stuck in local minima\n",
    "- Cross-entropy is convex and better suited for classification\n",
    "\n",
    "**Q: How to handle class imbalance?**\n",
    "1. **Weighted loss:** Give more weight to minority class\n",
    "2. **Threshold tuning:** Adjust decision threshold based on precision/recall needs\n",
    "3. **SMOTE:** Oversample minority class\n",
    "4. **Undersampling:** Downsample majority class\n",
    "\n",
    "**Q: Logistic Regression vs Neural Network?**\n",
    "- LR: Linear decision boundary, interpretable, fast, good for linearly separable data\n",
    "- NN: Non-linear, flexible, slower, better for complex patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 3: K-Nearest Neighbors (k-NN)\n",
    "\n",
    "**Time:** 20-25 minutes  \n",
    "**Difficulty:** Easy-Medium  \n",
    "**Asked by:** Amazon, Microsoft\n",
    "\n",
    "Instance-based learning algorithm for classification and regression.\n",
    "\n",
    "### Algorithm\n",
    "1. Store all training data\n",
    "2. For new point, find k nearest neighbors\n",
    "3. **Classification:** Majority vote\n",
    "4. **Regression:** Average of k neighbors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    \"\"\"\n",
    "    K-Nearest Neighbors Classifier\n",
    "    \n",
    "    Time Complexity:\n",
    "    - Training: O(1) - just store data\n",
    "    - Prediction: O(n * d) for each query where n=samples, d=features\n",
    "    \n",
    "    Space Complexity: O(n * d)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=3, distance_metric='euclidean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            k: Number of neighbors\n",
    "            distance_metric: 'euclidean' or 'manhattan'\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    \n",
    "    def compute_distance(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Compute distance between two points\n",
    "        \"\"\"\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2, axis=1))\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return np.sum(np.abs(x1 - x2), axis=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distance metric: {self.distance_metric}\")\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Store training data (lazy learning)\n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        return self\n",
    "    \n",
    "    def predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Predict label for single instance\n",
    "        \"\"\"\n",
    "        # Compute distances to all training points\n",
    "        distances = self.compute_distance(self.X_train, x)\n",
    "        \n",
    "        # Get indices of k nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        \n",
    "        # Get labels of k nearest neighbors\n",
    "        k_nearest_labels = self.y_train[k_indices]\n",
    "        \n",
    "        # Majority vote\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict labels for multiple instances\n",
    "        \"\"\"\n",
    "        return np.array([self.predict_single(x) for x in X])\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate accuracy\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "# Test\n",
    "X, y = make_classification(n_samples=500, n_features=2, n_redundant=0,\n",
    "                          n_informative=2, random_state=42, n_clusters_per_class=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Try different k values\n",
    "k_values = [1, 3, 5, 10, 20]\n",
    "scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNNClassifier(k=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    score = knn.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "    print(f\"k={k}: Accuracy = {score:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot 1: k vs Accuracy\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(k_values, scores, 'bo-')\n",
    "plt.xlabel('k (number of neighbors)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('k vs Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 2: Decision boundary for k=3\n",
    "plt.subplot(1, 3, 2)\n",
    "knn_best = KNNClassifier(k=3)\n",
    "knn_best.fit(X_train, y_train)\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 50),\n",
    "                     np.linspace(y_min, y_max, 50))\n",
    "Z = knn_best.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k', alpha=0.7)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary (k=3)')\n",
    "\n",
    "# Plot 3: Training data\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', alpha=0.7)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Training Data')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized k-NN using Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifierVectorized:\n",
    "    \"\"\"\n",
    "    Vectorized k-NN for better performance\n",
    "    \n",
    "    Uses matrix operations instead of loops\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Vectorized prediction using broadcasting\n",
    "        \n",
    "        Distance matrix: (n_test, n_train)\n",
    "        \"\"\"\n",
    "        # Compute pairwise distances using broadcasting\n",
    "        # ||x - y||^2 = ||x||^2 + ||y||^2 - 2*x*y\n",
    "        X_train_sq = np.sum(self.X_train ** 2, axis=1)\n",
    "        X_test_sq = np.sum(X ** 2, axis=1)[:, np.newaxis]\n",
    "        cross_term = -2 * X @ self.X_train.T\n",
    "        \n",
    "        distances = np.sqrt(X_test_sq + X_train_sq + cross_term)\n",
    "        \n",
    "        # Get k nearest neighbors for each test point\n",
    "        k_indices = np.argpartition(distances, self.k, axis=1)[:, :self.k]\n",
    "        \n",
    "        # Get labels of k nearest neighbors\n",
    "        k_nearest_labels = self.y_train[k_indices]\n",
    "        \n",
    "        # Majority vote using bincount (faster than Counter for large datasets)\n",
    "        predictions = np.array([\n",
    "            np.bincount(labels).argmax() \n",
    "            for labels in k_nearest_labels\n",
    "        ])\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "# Compare performance\n",
    "import time\n",
    "\n",
    "X_large, y_large = make_classification(n_samples=2000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_large, y_large, test_size=0.2)\n",
    "\n",
    "# Original k-NN\n",
    "start = time.time()\n",
    "knn_original = KNNClassifier(k=5)\n",
    "knn_original.fit(X_train, y_train)\n",
    "score_original = knn_original.score(X_test, y_test)\n",
    "time_original = time.time() - start\n",
    "\n",
    "# Vectorized k-NN\n",
    "start = time.time()\n",
    "knn_vectorized = KNNClassifierVectorized(k=5)\n",
    "knn_vectorized.fit(X_train, y_train)\n",
    "score_vectorized = knn_vectorized.score(X_test, y_test)\n",
    "time_vectorized = time.time() - start\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(f\"Original k-NN: {time_original:.4f}s, Accuracy: {score_original:.4f}\")\n",
    "print(f\"Vectorized k-NN: {time_vectorized:.4f}s, Accuracy: {score_vectorized:.4f}\")\n",
    "print(f\"Speedup: {time_original / time_vectorized:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interview Discussion Points\n",
    "\n",
    "**Q: How to choose k?**\n",
    "- Small k: More flexible, sensitive to noise, higher variance\n",
    "- Large k: Smoother boundary, less sensitive to noise, higher bias\n",
    "- Use cross-validation to find optimal k\n",
    "- Rule of thumb: k = sqrt(n)\n",
    "\n",
    "**Q: Pros and Cons of k-NN?**\n",
    "\n",
    "**Pros:**\n",
    "- Simple to understand and implement\n",
    "- No training phase (lazy learning)\n",
    "- Naturally handles multi-class\n",
    "- Can model complex decision boundaries\n",
    "\n",
    "**Cons:**\n",
    "- Slow prediction (O(n) for each query)\n",
    "- Memory intensive (stores all training data)\n",
    "- Sensitive to feature scaling\n",
    "- Curse of dimensionality (doesn't work well in high dimensions)\n",
    "\n",
    "**Q: How to make k-NN faster?**\n",
    "1. **KD-Trees/Ball Trees:** O(log n) search instead of O(n)\n",
    "2. **Approximate NN:** LSH (Locality Sensitive Hashing)\n",
    "3. **Dimensionality Reduction:** PCA before k-NN\n",
    "4. **Feature Selection:** Remove irrelevant features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 4: K-Means Clustering\n",
    "\n",
    "**Time:** 30-35 minutes  \n",
    "**Difficulty:** Medium  \n",
    "**Asked by:** Google, Amazon, Facebook\n",
    "\n",
    "Unsupervised learning algorithm for clustering.\n",
    "\n",
    "### Algorithm\n",
    "1. Initialize k centroids randomly\n",
    "2. **Assignment Step:** Assign each point to nearest centroid\n",
    "3. **Update Step:** Recompute centroids as mean of assigned points\n",
    "4. Repeat 2-3 until convergence\n",
    "\n",
    "**Objective:** Minimize within-cluster sum of squares (WCSS)\n",
    "\n",
    "$J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    \"\"\"\n",
    "    K-Means Clustering\n",
    "    \n",
    "    Time Complexity: O(n * k * d * iterations)\n",
    "    Space Complexity: O(n + k)\n",
    "    \n",
    "    where n=samples, k=clusters, d=features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters=3, max_iters=100, random_state=42, tolerance=1e-4):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "        self.random_state = random_state\n",
    "        self.tolerance = tolerance\n",
    "        self.centroids = None\n",
    "        self.labels = None\n",
    "        self.inertia_ = None\n",
    "        self.n_iter_ = 0\n",
    "    \n",
    "    def initialize_centroids(self, X):\n",
    "        \"\"\"\n",
    "        Initialize centroids randomly from data points\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        random_indices = np.random.choice(X.shape[0], self.n_clusters, replace=False)\n",
    "        return X[random_indices]\n",
    "    \n",
    "    def compute_distances(self, X, centroids):\n",
    "        \"\"\"\n",
    "        Compute distances from each point to each centroid\n",
    "        \n",
    "        Returns: (n_samples, n_clusters) matrix\n",
    "        \"\"\"\n",
    "        distances = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for k in range(self.n_clusters):\n",
    "            distances[:, k] = np.linalg.norm(X - centroids[k], axis=1)\n",
    "        return distances\n",
    "    \n",
    "    def assign_clusters(self, X, centroids):\n",
    "        \"\"\"\n",
    "        Assign each point to nearest centroid\n",
    "        \"\"\"\n",
    "        distances = self.compute_distances(X, centroids)\n",
    "        return np.argmin(distances, axis=1)\n",
    "    \n",
    "    def update_centroids(self, X, labels):\n",
    "        \"\"\"\n",
    "        Update centroids as mean of assigned points\n",
    "        \"\"\"\n",
    "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        for k in range(self.n_clusters):\n",
    "            cluster_points = X[labels == k]\n",
    "            if len(cluster_points) > 0:\n",
    "                centroids[k] = cluster_points.mean(axis=0)\n",
    "            else:\n",
    "                # Handle empty cluster - reinitialize randomly\n",
    "                centroids[k] = X[np.random.choice(X.shape[0])]\n",
    "        return centroids\n",
    "    \n",
    "    def compute_inertia(self, X, labels, centroids):\n",
    "        \"\"\"\n",
    "        Compute within-cluster sum of squares\n",
    "        \"\"\"\n",
    "        inertia = 0\n",
    "        for k in range(self.n_clusters):\n",
    "            cluster_points = X[labels == k]\n",
    "            if len(cluster_points) > 0:\n",
    "                inertia += np.sum((cluster_points - centroids[k]) ** 2)\n",
    "        return inertia\n",
    "    \n",
    "    def fit(self, X, verbose=False):\n",
    "        \"\"\"\n",
    "        Fit K-Means clustering\n",
    "        \"\"\"\n",
    "        # Initialize centroids\n",
    "        self.centroids = self.initialize_centroids(X)\n",
    "        \n",
    "        for iteration in range(self.max_iters):\n",
    "            # Assignment step\n",
    "            old_centroids = self.centroids.copy()\n",
    "            self.labels = self.assign_clusters(X, self.centroids)\n",
    "            \n",
    "            # Update step\n",
    "            self.centroids = self.update_centroids(X, self.labels)\n",
    "            \n",
    "            # Check convergence\n",
    "            centroid_shift = np.linalg.norm(self.centroids - old_centroids)\n",
    "            \n",
    "            if verbose:\n",
    "                inertia = self.compute_inertia(X, self.labels, self.centroids)\n",
    "                print(f\"Iteration {iteration}: Inertia = {inertia:.4f}, Shift = {centroid_shift:.6f}\")\n",
    "            \n",
    "            if centroid_shift < self.tolerance:\n",
    "                if verbose:\n",
    "                    print(f\"Converged at iteration {iteration}\")\n",
    "                break\n",
    "        \n",
    "        self.n_iter_ = iteration + 1\n",
    "        self.inertia_ = self.compute_inertia(X, self.labels, self.centroids)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict cluster labels for new data\n",
    "        \"\"\"\n",
    "        return self.assign_clusters(X, self.centroids)\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"\n",
    "        Fit and return cluster labels\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.labels\n",
    "\n",
    "# Test\n",
    "X, y_true = make_blobs(n_samples=500, centers=4, n_features=2, random_state=42)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, max_iters=100, random_state=42)\n",
    "kmeans.fit(X, verbose=True)\n",
    "\n",
    "print(f\"\\nFinal Inertia: {kmeans.inertia_:.4f}\")\n",
    "print(f\"Number of iterations: {kmeans.n_iter_}\")\n",
    "print(f\"Centroids:\\n{kmeans.centroids}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: True labels\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', alpha=0.6, edgecolors='k')\n",
    "plt.title('True Labels')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Plot 2: Predicted clusters\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels, cmap='viridis', alpha=0.6, edgecolors='k')\n",
    "plt.scatter(kmeans.centroids[:, 0], kmeans.centroids[:, 1], \n",
    "           c='red', marker='X', s=200, edgecolors='black', linewidth=2, label='Centroids')\n",
    "plt.title('K-Means Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: Elbow method\n",
    "plt.subplot(1, 3, 3)\n",
    "inertias = []\n",
    "K_range = range(1, 10)\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(X)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "plt.plot(K_range, inertias, 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia (WCSS)')\n",
    "plt.title('Elbow Method')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means++: Better Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansPlusPlus(KMeans):\n",
    "    \"\"\"\n",
    "    K-Means with K-Means++ initialization\n",
    "    \n",
    "    Better centroid initialization for faster convergence\n",
    "    \"\"\"\n",
    "    \n",
    "    def initialize_centroids(self, X):\n",
    "        \"\"\"\n",
    "        K-Means++ initialization\n",
    "        \n",
    "        1. Choose first centroid randomly\n",
    "        2. For each subsequent centroid:\n",
    "           - Compute D(x) = distance to nearest centroid\n",
    "           - Choose new centroid with probability proportional to D(x)^2\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        # Choose first centroid randomly\n",
    "        centroids = [X[np.random.choice(X.shape[0])]]\n",
    "        \n",
    "        # Choose remaining k-1 centroids\n",
    "        for _ in range(1, self.n_clusters):\n",
    "            # Compute distances to nearest centroid\n",
    "            distances = np.array([\n",
    "                min(np.linalg.norm(x - c) ** 2 for c in centroids)\n",
    "                for x in X\n",
    "            ])\n",
    "            \n",
    "            # Choose next centroid with probability proportional to distance^2\n",
    "            probabilities = distances / distances.sum()\n",
    "            next_centroid_idx = np.random.choice(X.shape[0], p=probabilities)\n",
    "            centroids.append(X[next_centroid_idx])\n",
    "        \n",
    "        return np.array(centroids)\n",
    "\n",
    "# Compare K-Means vs K-Means++\n",
    "X, y_true = make_blobs(n_samples=1000, centers=5, n_features=2, random_state=42)\n",
    "\n",
    "# Regular K-Means\n",
    "kmeans_regular = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans_regular.fit(X)\n",
    "\n",
    "# K-Means++\n",
    "kmeans_pp = KMeansPlusPlus(n_clusters=5, random_state=42)\n",
    "kmeans_pp.fit(X)\n",
    "\n",
    "print(\"Comparison:\")\n",
    "print(f\"Regular K-Means: {kmeans_regular.n_iter_} iterations, Inertia: {kmeans_regular.inertia_:.2f}\")\n",
    "print(f\"K-Means++: {kmeans_pp.n_iter_} iterations, Inertia: {kmeans_pp.inertia_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interview Discussion Points\n",
    "\n",
    "**Q: Limitations of K-Means?**\n",
    "1. **Must specify k:** Need to know number of clusters beforehand\n",
    "2. **Sensitive to initialization:** Different random starts give different results\n",
    "3. **Assumes spherical clusters:** Doesn't work well for elongated or irregular shapes\n",
    "4. **Sensitive to outliers:** Outliers can skew centroids\n",
    "5. **Euclidean distance:** Assumes all features equally important\n",
    "\n",
    "**Q: How to choose k?**\n",
    "1. **Elbow Method:** Plot inertia vs k, look for \"elbow\"\n",
    "2. **Silhouette Score:** Measure how similar point is to its cluster vs other clusters\n",
    "3. **Domain Knowledge:** Use business understanding\n",
    "4. **Cross-Validation:** For downstream task\n",
    "\n",
    "**Q: K-Means vs Hierarchical Clustering?**\n",
    "- **K-Means:** Faster O(nkd), requires k, produces flat clusters\n",
    "- **Hierarchical:** Slower O(nÂ³), no need for k, produces dendrogram, deterministic\n",
    "\n",
    "**Q: How to handle categorical features?**\n",
    "- K-Modes (uses mode instead of mean)\n",
    "- One-hot encoding + K-Means\n",
    "- Use appropriate distance metric (Hamming distance)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Commonly Asked Supporting Functions\n",
    "\n",
    "These are frequently asked as part of main algorithms.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_scratch(X, y, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split data into train and test sets\n",
    "    \n",
    "    Args:\n",
    "        X: Features (n_samples, n_features)\n",
    "        y: Labels (n_samples,)\n",
    "        test_size: Fraction for test set\n",
    "        random_state: Seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    n_test = int(n_samples * test_size)\n",
    "    \n",
    "    # Random shuffle indices\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    \n",
    "    test_indices = indices[:n_test]\n",
    "    train_indices = indices[n_test:]\n",
    "    \n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "# Test\n",
    "X = np.arange(100).reshape(20, 5)\n",
    "y = np.arange(20)\n",
    "X_train, X_test, y_train, y_test = train_test_split_scratch(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    \"\"\"\n",
    "    Standardize features: (x - mean) / std\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.std = np.std(X, axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return (X - self.mean) / (self.std + 1e-8)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "class MinMaxScaler:\n",
    "    \"\"\"\n",
    "    Scale features to [0, 1]: (x - min) / (max - min)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.min = np.min(X, axis=0)\n",
    "        self.max = np.max(X, axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return (X - self.min) / (self.max - self.min + 1e-8)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "# Test\n",
    "X = np.random.randn(100, 3) * 10 + 50\n",
    "\n",
    "scaler_std = StandardScaler()\n",
    "X_std = scaler_std.fit_transform(X)\n",
    "print(f\"Standardized - Mean: {X_std.mean(axis=0)}, Std: {X_std.std(axis=0)}\")\n",
    "\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_minmax = scaler_minmax.fit_transform(X)\n",
    "print(f\"MinMax - Min: {X_minmax.min(axis=0)}, Max: {X_minmax.max(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, y, model_class, k=5, **model_params):\n",
    "    \"\"\"\n",
    "    K-Fold Cross-Validation\n",
    "    \n",
    "    Args:\n",
    "        X: Features\n",
    "        y: Labels\n",
    "        model_class: Model class to instantiate\n",
    "        k: Number of folds\n",
    "        **model_params: Parameters for model\n",
    "    \n",
    "    Returns:\n",
    "        List of scores for each fold\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    fold_size = n_samples // k\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for fold in range(k):\n",
    "        # Create train/val split\n",
    "        val_start = fold * fold_size\n",
    "        val_end = (fold + 1) * fold_size if fold < k - 1 else n_samples\n",
    "        \n",
    "        val_indices = indices[val_start:val_end]\n",
    "        train_indices = np.concatenate([indices[:val_start], indices[val_end:]])\n",
    "        \n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y_train, y_val = y[train_indices], y[val_indices]\n",
    "        \n",
    "        # Train and evaluate\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        score = model.score(X_val, y_val)\n",
    "        scores.append(score)\n",
    "        \n",
    "        print(f\"Fold {fold + 1}: Score = {score:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMean Score: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
    "    return scores\n",
    "\n",
    "# Test with Logistic Regression\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "scores = k_fold_cross_validation(X, y, LogisticRegression, k=5, \n",
    "                                 learning_rate=0.1, n_iterations=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Checklist\n",
    "\n",
    "### Before the Interview\n",
    "- [ ] Review all 4 core algorithms (Linear Regression, Logistic Regression, k-NN, K-Means)\n",
    "- [ ] Practice implementing without sklearn\n",
    "- [ ] Understand time/space complexity\n",
    "- [ ] Know when to use each algorithm\n",
    "- [ ] Practice explaining trade-offs\n",
    "\n",
    "### During the Interview\n",
    "- [ ] Clarify requirements (classification/regression, dataset size, constraints)\n",
    "- [ ] Ask about allowed libraries (NumPy? SciPy? sklearn?)\n",
    "- [ ] Discuss approach before coding\n",
    "- [ ] Write clean, modular code\n",
    "- [ ] Test with simple example\n",
    "- [ ] Discuss complexity and optimizations\n",
    "\n",
    "### Common Follow-up Questions\n",
    "1. How would you optimize for large datasets?\n",
    "2. What if data doesn't fit in memory?\n",
    "3. How to handle missing values?\n",
    "4. What about categorical features?\n",
    "5. How to evaluate model performance?\n",
    "6. What if features have different scales?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've now implemented the **TOP 4** ML algorithms asked in interviews:\n",
    "\n",
    "1. âœ… **Linear Regression** - Normal Equation & Gradient Descent\n",
    "2. âœ… **Logistic Regression** - Binary classification with cross-entropy\n",
    "3. âœ… **K-Nearest Neighbors** - Instance-based learning with optimizations\n",
    "4. âœ… **K-Means Clustering** - Unsupervised learning with K-Means++\n",
    "\n",
    "**Plus supporting functions:**\n",
    "- Train-test split\n",
    "- Feature scaling (StandardScaler, MinMaxScaler)\n",
    "- K-Fold cross-validation\n",
    "\n",
    "**You're ready for ML coding interviews at Google, Facebook, Amazon, Microsoft, and Cisco!**\n",
    "\n",
    "Good luck! ðŸš€\n",
    "</cell>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
