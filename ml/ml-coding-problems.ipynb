{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ML Coding Practice Problems\n",
    "\n",
    "## For Sindhuja's Coding Round (Oct 23)\n",
    "\n",
    "### General Guidelines\n",
    "- Write clean, readable code\n",
    "- Think aloud - explain your approach\n",
    "- Consider edge cases\n",
    "- Discuss time/space complexity\n",
    "- Ask clarifying questions\n",
    "- Test your solution\n",
    "\n",
    "**7 Problems covering:**\n",
    "1. Attention Mechanism\n",
    "2. Cosine Similarity\n",
    "3. Data Collator\n",
    "4. Gradient Descent\n",
    "5. Confusion Matrix\n",
    "6. K-Means Clustering\n",
    "7. Softmax Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for ML coding\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 1: Attention Mechanism Implementation (Medium)\n",
    "\n",
    "**Time:** 30-40 minutes  \n",
    "**Topics:** Transformers, Neural Networks\n",
    "\n",
    "Implement scaled dot-product attention from scratch.\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
    "```\n",
    "\n",
    "### Your Turn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Your implementation\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix (batch_size, seq_len, d_k)\n",
    "        K: Key matrix (batch_size, seq_len, d_k)\n",
    "        V: Value matrix (batch_size, seq_len, d_v)\n",
    "        mask: Optional mask (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        output: (batch_size, seq_len, d_v)\n",
    "        attention_weights: (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    # 1. Compute scores: Q @ K^T / sqrt(d_k)\n",
    "    # 2. Apply mask if provided\n",
    "    # 3. Apply softmax\n",
    "    # 4. Multiply by V\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "batch_size, seq_len, d_k, d_v = 2, 4, 8, 8\n",
    "Q = np.random.randn(batch_size, seq_len, d_k)\n",
    "K = np.random.randn(batch_size, seq_len, d_k)\n",
    "V = np.random.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(f\"Output shape: {output.shape}\")  # (2, 4, 8)\n",
    "print(f\"Attention weights shape: {weights.shape}\")  # (2, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def scaled_dot_product_attention_solution(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \"\"\"\n",
    "    # Get dimension of keys\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Compute attention scores: Q @ K^T / sqrt(d_k)\n",
    "    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = scores + (mask * -1e9)\n",
    "    \n",
    "    # Apply softmax\n",
    "    attention_weights = np.exp(scores) / np.exp(scores).sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    # Apply attention to values\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test\n",
    "batch_size, seq_len, d_k, d_v = 2, 4, 8, 8\n",
    "Q = np.random.randn(batch_size, seq_len, d_k)\n",
    "K = np.random.randn(batch_size, seq_len, d_k)\n",
    "V = np.random.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "output, weights = scaled_dot_product_attention_solution(Q, K, V)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights sum: {weights.sum(axis=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "**Key Discussion Points:**\n",
    "- Why divide by sqrt(d_k)? Prevents softmax saturation\n",
    "- Time complexity: O(n²d) where n is sequence length\n",
    "- Space complexity: O(n²) for attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2: Cosine Similarity (Easy-Medium)\n",
    "\n",
    "**Time:** 20-25 minutes  \n",
    "**Topics:** NLP, Embeddings\n",
    "\n",
    "Find top-k most similar sentences using cosine similarity.\n",
    "\n",
    "### Your Turn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_sentences(query_embedding, sentence_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Your implementation\n",
    "    \n",
    "    Args:\n",
    "        query_embedding: (d,)\n",
    "        sentence_embeddings: (n, d)\n",
    "        k: top-k similar\n",
    "    \n",
    "    Returns:\n",
    "        indices: top-k indices\n",
    "        similarities: cosine similarity scores\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    # 1. Normalize embeddings\n",
    "    # 2. Compute dot product\n",
    "    # 3. Get top-k\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "np.random.seed(42)\n",
    "query_embedding = np.random.randn(384)\n",
    "sentence_embeddings = np.random.randn(100, 384)\n",
    "\n",
    "indices, similarities = find_similar_sentences(query_embedding, sentence_embeddings, k=5)\n",
    "print(f\"Top 5 indices: {indices}\")\n",
    "print(f\"Similarities: {similarities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def find_similar_sentences_solution(query_embedding, sentence_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Find top-k similar using cosine similarity\n",
    "    \"\"\"\n",
    "    # Normalize query\n",
    "    query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "    \n",
    "    # Normalize sentences\n",
    "    sentence_norms = sentence_embeddings / np.linalg.norm(\n",
    "        sentence_embeddings, axis=1, keepdims=True\n",
    "    )\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    similarities = np.dot(sentence_norms, query_norm)\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "    top_k_similarities = similarities[top_k_indices]\n",
    "    \n",
    "    return top_k_indices, top_k_similarities\n",
    "\n",
    "# Test\n",
    "np.random.seed(42)\n",
    "query_embedding = np.random.randn(384)\n",
    "sentence_embeddings = np.random.randn(100, 384)\n",
    "\n",
    "indices, similarities = find_similar_sentences_solution(query_embedding, sentence_embeddings, k=5)\n",
    "print(f\"Top 5 indices: {indices}\")\n",
    "print(f\"Similarities: {similarities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: Data Collator for Fine-tuning (Medium)\n",
    "\n",
    "**Time:** 25-30 minutes  \n",
    "**Topics:** Data Processing\n",
    "\n",
    "Implement data collator for batching variable-length sequences.\n",
    "\n",
    "### Your Turn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Your implementation\n",
    "    \n",
    "    Args:\n",
    "        batch: List of {'input_ids': [...], 'labels': [...]}\n",
    "        tokenizer: Has pad_token_id\n",
    "        max_length: Max sequence length\n",
    "    \n",
    "    Returns:\n",
    "        Dict with padded tensors\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    # 1. Find max length in batch\n",
    "    # 2. Create padded tensors\n",
    "    # 3. Create attention mask\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "class MockTokenizer:\n",
    "    pad_token_id = 0\n",
    "\n",
    "tokenizer = MockTokenizer()\n",
    "batch = [\n",
    "    {'input_ids': [1,2,3,4,5], 'labels': [1,2,3,4,5]},\n",
    "    {'input_ids': [1,2,3], 'labels': [1,2,3]},\n",
    "]\n",
    "\n",
    "result = collate_batch(batch, tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def collate_batch_solution(batch, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Collate batch with padding\n",
    "    \"\"\"\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Find max length in batch\n",
    "    batch_max_length = min(max(len(ids) for ids in input_ids), max_length)\n",
    "    \n",
    "    batch_size = len(batch)\n",
    "    padded_input_ids = torch.full(\n",
    "        (batch_size, batch_max_length),\n",
    "        tokenizer.pad_token_id,\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    padded_labels = torch.full(\n",
    "        (batch_size, batch_max_length),\n",
    "        -100,\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    attention_mask = torch.zeros((batch_size, batch_max_length), dtype=torch.long)\n",
    "    \n",
    "    for i, (ids, lbls) in enumerate(zip(input_ids, labels)):\n",
    "        length = min(len(ids), batch_max_length)\n",
    "        padded_input_ids[i, :length] = torch.tensor(ids[:length])\n",
    "        padded_labels[i, :length] = torch.tensor(lbls[:length])\n",
    "        attention_mask[i, :length] = 1\n",
    "    \n",
    "    return {\n",
    "        'input_ids': padded_input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': padded_labels\n",
    "    }\n",
    "\n",
    "# Test\n",
    "tokenizer = MockTokenizer()\n",
    "batch = [\n",
    "    {'input_ids': [1,2,3,4,5], 'labels': [1,2,3,4,5]},\n",
    "    {'input_ids': [1,2,3], 'labels': [1,2,3]},\n",
    "]\n",
    "\n",
    "result = collate_batch_solution(batch, tokenizer)\n",
    "print(\"Input IDs:\", result['input_ids'])\n",
    "print(\"Attention mask:\", result['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4: Gradient Descent (Easy-Medium)\n",
    "\n",
    "**Time:** 20-25 minutes  \n",
    "**Topics:** Optimization\n",
    "\n",
    "Minimize f(x) = (x-3)² + 5 using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(learning_rate=0.1, num_iterations=100, initial_x=0):\n",
    "    \"\"\"\n",
    "    Your implementation\n",
    "    \n",
    "    f(x) = (x-3)^2 + 5\n",
    "    f'(x) = 2(x-3)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "x_hist, f_hist = gradient_descent(learning_rate=0.1, num_iterations=50)\n",
    "print(f\"Final x: {x_hist[-1]}\")\n",
    "print(f\"Final f(x): {f_hist[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def gradient_descent_solution(learning_rate=0.1, num_iterations=100, initial_x=0):\n",
    "    \"\"\"\n",
    "    Minimize f(x) = (x-3)^2 + 5\n",
    "    \"\"\"\n",
    "    x = initial_x\n",
    "    x_history = [x]\n",
    "    f_history = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Function value\n",
    "        f_x = (x - 3)**2 + 5\n",
    "        f_history.append(f_x)\n",
    "        \n",
    "        # Gradient\n",
    "        gradient = 2 * (x - 3)\n",
    "        \n",
    "        # Update\n",
    "        x = x - learning_rate * gradient\n",
    "        x_history.append(x)\n",
    "        \n",
    "        # Early stopping\n",
    "        if abs(gradient) < 1e-6:\n",
    "            print(f\"Converged at iteration {i}\")\n",
    "            break\n",
    "    \n",
    "    return x_history, f_history\n",
    "\n",
    "# Test\n",
    "x_hist, f_hist = gradient_descent_solution(learning_rate=0.1, num_iterations=50)\n",
    "print(f\"Final x: {x_hist[-1]:.6f}\")  # Should be ~3\n",
    "print(f\"Final f(x): {f_hist[-1]:.6f}\")  # Should be ~5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 5: Confusion Matrix and Metrics (Easy)\n",
    "\n",
    "**Time:** 15-20 minutes  \n",
    "**Topics:** Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Your implementation\n",
    "    \n",
    "    Returns:\n",
    "        Dict with precision, recall, f1_score\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    # Calculate TP, FP, FN, TN\n",
    "    # precision = TP / (TP + FP)\n",
    "    # recall = TP / (TP + FN)\n",
    "    # f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n",
    "print(compute_metrics(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def compute_metrics_solution(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics from scratch\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': {'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn}\n",
    "    }\n",
    "\n",
    "# Test\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n",
    "metrics = compute_metrics_solution(y_true, y_pred)\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 6: K-Means Clustering (Medium)\n",
    "\n",
    "**Time:** 30-35 minutes  \n",
    "**Topics:** Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters=3, max_iters=100, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "        self.random_state = random_state\n",
    "        self.centroids = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Your implementation\"\"\"\n",
    "        # TODO: Implement\n",
    "        # 1. Initialize centroids\n",
    "        # 2. Assign points to clusters\n",
    "        # 3. Update centroids\n",
    "        # 4. Check convergence\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict cluster labels\"\"\"\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, random_state=42)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "labels = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 7: Softmax with Numerical Stability (Easy-Medium)\n",
    "\n",
    "**Time:** 15-20 minutes  \n",
    "**Topics:** Numerical Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_stable(x):\n",
    "    \"\"\"\n",
    "    Your implementation\n",
    "    \n",
    "    Hint: Subtract max to prevent overflow\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "x_large = np.array([[1000, 1001, 1002]])\n",
    "print(softmax_stable(x_large))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def softmax_stable_solution(x):\n",
    "    \"\"\"\n",
    "    Numerically stable softmax\n",
    "    \"\"\"\n",
    "    x_shifted = x - np.max(x, axis=-1, keepdims=True)\n",
    "    exp_x = np.exp(x_shifted)\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Test\n",
    "x_large = np.array([[1000, 1001, 1002]])\n",
    "result = softmax_stable_solution(x_large)\n",
    "print(\"Stable softmax:\", result)\n",
    "print(\"Sum:\", result.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "## Interview Tips for Coding Round\n",
    "\n",
    "### 1. Clarify Requirements\n",
    "- Ask about input/output formats\n",
    "- Confirm edge cases\n",
    "- Understand performance requirements\n",
    "\n",
    "### 2. Think Aloud\n",
    "- Explain your approach before coding\n",
    "- Discuss trade-offs\n",
    "- Mention alternative solutions\n",
    "\n",
    "### 3. Write Clean Code\n",
    "- Use meaningful variable names\n",
    "- Add comments for complex logic\n",
    "- Follow Python conventions (PEP 8)\n",
    "\n",
    "### 4. Test Your Solution\n",
    "- Walk through a simple example\n",
    "- Test edge cases\n",
    "- Verify output shapes and types\n",
    "\n",
    "### 5. Discuss Complexity\n",
    "- Time complexity\n",
    "- Space complexity\n",
    "- How to optimize for large scale\n",
    "\n",
    "### 6. Be Ready to Extend\n",
    "- How would you handle batching?\n",
    "- How to optimize for production?\n",
    "- What if dataset doesn't fit in memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "## Topics to Review Before Interview\n",
    "\n",
    "- [ ] NumPy operations (broadcasting, indexing, matrix operations)\n",
    "- [ ] PyTorch basics (tensors, autograd, nn.Module)\n",
    "- [ ] Attention mechanism implementation\n",
    "- [ ] Data loading and batching\n",
    "- [ ] Common ML metrics implementation\n",
    "- [ ] Numerical stability tricks\n",
    "- [ ] Vectorization techniques\n",
    "- [ ] Basic optimization algorithms\n",
    "\n",
    "## Good Luck!\n",
    "\n",
    "You've practiced:\n",
    "- Transformer components (attention)\n",
    "- Similarity search (embeddings)\n",
    "- Data processing (collation, padding)\n",
    "- Optimization (gradient descent)\n",
    "- Evaluation (metrics)\n",
    "- Clustering (K-means)\n",
    "- Numerical stability (softmax)\n",
    "\n",
    "These cover the most common ML coding interview topics!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
