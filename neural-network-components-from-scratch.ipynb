{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Components From Scratch\n",
    "\n",
    "## Deep Learning Coding Interview Prep\n",
    "\n",
    "Based on 2025 interview trends, companies are asking candidates to **implement neural network components from scratch** without relying on high-level libraries like Keras or PyTorch `nn.Module`.\n",
    "\n",
    "### What You'll Implement:\n",
    "1. **Linear (Dense) Layers** - Forward and backward pass\n",
    "2. **Activation Functions** - ReLU, Sigmoid, Tanh, Softmax\n",
    "3. **Loss Functions** - MSE, Cross-Entropy\n",
    "4. **Optimizers** - SGD, Momentum, Adam\n",
    "5. **Batch Normalization** - Normalization layer\n",
    "6. **Dropout** - Regularization technique\n",
    "7. **Complete Neural Network** - End-to-end implementation\n",
    "\n",
    "### Interview Companies Asking These:\n",
    "- Google, Meta, Amazon\n",
    "- Cisco (for ML Engineer roles)\n",
    "- Startups with ML focus\n",
    "\n",
    "**Key Rule:** Only NumPy allowed - no PyTorch `nn.Module` or TensorFlow `keras.layers`!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# For testing/comparison only\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear (Dense/Fully-Connected) Layer\n",
    "\n",
    "**Time:** 20-25 minutes  \n",
    "**Difficulty:** Medium  \n",
    "**Commonly Asked:** Yes\n",
    "\n",
    "The fundamental building block of neural networks.\n",
    "\n",
    "### Forward Pass\n",
    "$y = Wx + b$\n",
    "\n",
    "where:\n",
    "- $W$: weight matrix $(n_{out}, n_{in})$\n",
    "- $x$: input $(n_{in},)$ or $(batch, n_{in})$\n",
    "- $b$: bias vector $(n_{out},)$\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "Given $\\frac{\\partial L}{\\partial y}$ (gradient from next layer), compute:\n",
    "\n",
    "1. $\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\cdot x^T$\n",
    "2. $\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y}$\n",
    "3. $\\frac{\\partial L}{\\partial x} = W^T \\cdot \\frac{\\partial L}{\\partial y}$ (pass to previous layer)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"\n",
    "    Fully connected (dense) layer\n",
    "    \n",
    "    y = Wx + b\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features: Number of input features\n",
    "            out_features: Number of output features\n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Xavier/Glorot initialization\n",
    "        limit = np.sqrt(6 / (in_features + out_features))\n",
    "        self.W = np.random.uniform(-limit, limit, (out_features, in_features))\n",
    "        self.b = np.zeros((out_features, 1))\n",
    "        \n",
    "        # For backward pass\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: Input (batch_size, in_features) or (in_features, 1)\n",
    "        \n",
    "        Returns:\n",
    "            y: Output (batch_size, out_features) or (out_features, 1)\n",
    "        \"\"\"\n",
    "        self.x = x  # Cache for backward pass\n",
    "        \n",
    "        # Handle both (batch, features) and (features, 1) shapes\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(-1, 1)\n",
    "        \n",
    "        # y = Wx + b\n",
    "        return self.W @ x.T + self.b\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "        \n",
    "        Args:\n",
    "            grad_output: Gradient from next layer (out_features, batch_size)\n",
    "        \n",
    "        Returns:\n",
    "            grad_input: Gradient w.r.t. input (in_features, batch_size)\n",
    "        \"\"\"\n",
    "        batch_size = grad_output.shape[1] if grad_output.ndim > 1 else 1\n",
    "        \n",
    "        # Gradient w.r.t. weights: dL/dW = dL/dy * x^T\n",
    "        if self.x.ndim == 1:\n",
    "            x_reshaped = self.x.reshape(1, -1)\n",
    "        else:\n",
    "            x_reshaped = self.x\n",
    "        \n",
    "        self.dW = (grad_output @ x_reshaped) / batch_size\n",
    "        \n",
    "        # Gradient w.r.t. bias: dL/db = dL/dy (sum over batch)\n",
    "        self.db = np.sum(grad_output, axis=1, keepdims=True) / batch_size\n",
    "        \n",
    "        # Gradient w.r.t. input: dL/dx = W^T * dL/dy\n",
    "        grad_input = self.W.T @ grad_output\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "    def update_params(self, learning_rate: float):\n",
    "        \"\"\"\n",
    "        Update parameters using gradients\n",
    "        \"\"\"\n",
    "        self.W -= learning_rate * self.dW\n",
    "        self.b -= learning_rate * self.db\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Linear(in_features={self.in_features}, out_features={self.out_features})\"\n",
    "\n",
    "# Test\n",
    "layer = Linear(in_features=5, out_features=3)\n",
    "x = np.random.randn(2, 5)  # Batch of 2, 5 features each\n",
    "\n",
    "# Forward pass\n",
    "y = layer.forward(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Weights shape: {layer.W.shape}\")\n",
    "print(f\"Bias shape: {layer.b.shape}\")\n",
    "\n",
    "# Backward pass\n",
    "grad_output = np.random.randn(*y.shape)\n",
    "grad_input = layer.backward(grad_output)\n",
    "print(f\"\\nGradient w.r.t. input shape: {grad_input.shape}\")\n",
    "print(f\"Gradient w.r.t. weights shape: {layer.dW.shape}\")\n",
    "print(f\"Gradient w.r.t. bias shape: {layer.db.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Activation Functions\n",
    "\n",
    "**Time:** 15-20 minutes per activation  \n",
    "**Difficulty:** Easy-Medium\n",
    "\n",
    "Common activation functions and their derivatives.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 ReLU (Rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU activation: f(x) = max(0, x)\n",
    "    \n",
    "    Derivative: f'(x) = 1 if x > 0, else 0\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gradient: dL/dx = dL/dy * 1 if x > 0, else 0\n",
    "        \"\"\"\n",
    "        return grad_output * (self.x > 0).astype(float)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU()\"\n",
    "\n",
    "# Test\n",
    "relu = ReLU()\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "y = relu.forward(x)\n",
    "print(f\"ReLU({x}) = {y}\")\n",
    "\n",
    "grad_output = np.ones_like(y)\n",
    "grad_input = relu.backward(grad_output)\n",
    "print(f\"Gradient: {grad_input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    Sigmoid activation: f(x) = 1 / (1 + e^(-x))\n",
    "    \n",
    "    Derivative: f'(x) = f(x) * (1 - f(x))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # Numerical stability: clip x to prevent overflow\n",
    "        x_clipped = np.clip(x, -500, 500)\n",
    "        self.y = 1 / (1 + np.exp(-x_clipped))\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gradient: dL/dx = dL/dy * y * (1 - y)\n",
    "        \"\"\"\n",
    "        return grad_output * self.y * (1 - self.y)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Sigmoid()\"\n",
    "\n",
    "# Test\n",
    "sigmoid = Sigmoid()\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "y = sigmoid.forward(x)\n",
    "print(f\"Sigmoid({x}) = {y}\")\n",
    "\n",
    "grad_output = np.ones_like(y)\n",
    "grad_input = sigmoid.backward(grad_output)\n",
    "print(f\"Gradient: {grad_input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    Tanh activation: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "    \n",
    "    Derivative: f'(x) = 1 - f(x)^2\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.y = np.tanh(x)\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gradient: dL/dx = dL/dy * (1 - y^2)\n",
    "        \"\"\"\n",
    "        return grad_output * (1 - self.y ** 2)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Tanh()\"\n",
    "\n",
    "# Test\n",
    "tanh = Tanh()\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "y = tanh.forward(x)\n",
    "print(f\"Tanh({x}) = {y}\")\n",
    "\n",
    "grad_output = np.ones_like(y)\n",
    "grad_input = tanh.backward(grad_output)\n",
    "print(f\"Gradient: {grad_input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Softmax activation: f(x_i) = e^(x_i) / Σ(e^(x_j))\n",
    "    \n",
    "    Used for multi-class classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Numerically stable softmax\n",
    "        \"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        x_shifted = x - np.max(x, axis=0, keepdims=True)\n",
    "        exp_x = np.exp(x_shifted)\n",
    "        self.y = exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gradient of softmax\n",
    "        \n",
    "        When combined with cross-entropy, this simplifies to (y - t)\n",
    "        where t is the true label (one-hot)\n",
    "        \"\"\"\n",
    "        # General softmax gradient (Jacobian)\n",
    "        # For efficiency, we typically combine softmax + cross-entropy\n",
    "        # and use simplified gradient (y - t)\n",
    "        return grad_output\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Softmax()\"\n",
    "\n",
    "# Test\n",
    "softmax = Softmax()\n",
    "x = np.array([[2.0, 1.0, 0.1]]).T  # 3 classes\n",
    "y = softmax.forward(x)\n",
    "print(f\"Softmax input: {x.T}\")\n",
    "print(f\"Softmax output: {y.T}\")\n",
    "print(f\"Sum of probabilities: {y.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activations\n",
    "x_range = np.linspace(-5, 5, 100)\n",
    "\n",
    "relu_func = ReLU()\n",
    "sigmoid_func = Sigmoid()\n",
    "tanh_func = Tanh()\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Plot activations\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x_range, relu_func.forward(x_range), label='ReLU', linewidth=2)\n",
    "plt.plot(x_range, sigmoid_func.forward(x_range), label='Sigmoid', linewidth=2)\n",
    "plt.plot(x_range, tanh_func.forward(x_range), label='Tanh', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Activation Functions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot derivatives\n",
    "plt.subplot(1, 3, 2)\n",
    "grad = np.ones_like(x_range)\n",
    "plt.plot(x_range, relu_func.backward(grad), label=\"ReLU'\", linewidth=2)\n",
    "plt.plot(x_range, sigmoid_func.backward(grad), label=\"Sigmoid'\", linewidth=2)\n",
    "plt.plot(x_range, tanh_func.backward(grad), label=\"Tanh'\", linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.title('Activation Function Derivatives')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Comparison table\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.axis('off')\n",
    "table_data = [\n",
    "    ['Function', 'Range', 'Use Case'],\n",
    "    ['ReLU', '[0, ∞)', 'Hidden layers (most common)'],\n",
    "    ['Sigmoid', '(0, 1)', 'Binary classification output'],\n",
    "    ['Tanh', '(-1, 1)', 'Hidden layers (RNNs)'],\n",
    "    ['Softmax', '(0, 1) sum=1', 'Multi-class output']\n",
    "]\n",
    "table = plt.table(cellText=table_data, cellLoc='left', loc='center',\n",
    "                 colWidths=[0.3, 0.3, 0.4])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 2)\n",
    "plt.title('Activation Function Comparison', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Loss Functions\n",
    "\n",
    "**Time:** 15-20 minutes per loss  \n",
    "**Difficulty:** Medium\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    \"\"\"\n",
    "    Mean Squared Error Loss\n",
    "    \n",
    "    L = (1/N) * Σ(y_pred - y_true)^2\n",
    "    \n",
    "    Gradient: dL/dy_pred = (2/N) * (y_pred - y_true)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "        self.y_true = None\n",
    "    \n",
    "    def forward(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute MSE loss\n",
    "        \n",
    "        Args:\n",
    "            y_pred: Predictions\n",
    "            y_true: Ground truth\n",
    "        \n",
    "        Returns:\n",
    "            loss: Scalar loss value\n",
    "        \"\"\"\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        \n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute gradient w.r.t. predictions\n",
    "        \n",
    "        Returns:\n",
    "            grad: Gradient of loss w.r.t. y_pred\n",
    "        \"\"\"\n",
    "        N = self.y_pred.size\n",
    "        return (2 / N) * (self.y_pred - self.y_true)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"MSELoss()\"\n",
    "\n",
    "# Test\n",
    "mse_loss = MSELoss()\n",
    "y_pred = np.array([[2.5], [0.0], [2.1]])\n",
    "y_true = np.array([[3.0], [0.5], [2.0]])\n",
    "\n",
    "loss = mse_loss.forward(y_pred, y_true)\n",
    "grad = mse_loss.backward()\n",
    "\n",
    "print(f\"Predictions: {y_pred.T}\")\n",
    "print(f\"True values: {y_true.T}\")\n",
    "print(f\"MSE Loss: {loss:.4f}\")\n",
    "print(f\"Gradient: {grad.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCELoss:\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy Loss\n",
    "    \n",
    "    L = -(1/N) * Σ[y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred)]\n",
    "    \n",
    "    Gradient: dL/dy_pred = (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "        self.y_true = None\n",
    "        self.epsilon = 1e-10  # For numerical stability\n",
    "    \n",
    "    def forward(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred_clipped = np.clip(y_pred, self.epsilon, 1 - self.epsilon)\n",
    "        self.y_pred = y_pred_clipped\n",
    "        self.y_true = y_true\n",
    "        \n",
    "        return -np.mean(\n",
    "            y_true * np.log(y_pred_clipped) + \n",
    "            (1 - y_true) * np.log(1 - y_pred_clipped)\n",
    "        )\n",
    "    \n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Simplified gradient when used with sigmoid:\n",
    "        dL/dy_pred = y_pred - y_true\n",
    "        \"\"\"\n",
    "        N = self.y_pred.size\n",
    "        return -(self.y_true / self.y_pred - \n",
    "                (1 - self.y_true) / (1 - self.y_pred)) / N\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"BCELoss()\"\n",
    "\n",
    "# Test\n",
    "bce_loss = BCELoss()\n",
    "y_pred = np.array([[0.9], [0.2], [0.8]])\n",
    "y_true = np.array([[1.0], [0.0], [1.0]])\n",
    "\n",
    "loss = bce_loss.forward(y_pred, y_true)\n",
    "grad = bce_loss.backward()\n",
    "\n",
    "print(f\"Predictions: {y_pred.T}\")\n",
    "print(f\"True labels: {y_true.T}\")\n",
    "print(f\"BCE Loss: {loss:.4f}\")\n",
    "print(f\"Gradient: {grad.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Cross-Entropy Loss (Multi-Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    \"\"\"\n",
    "    Cross-Entropy Loss for multi-class classification\n",
    "    \n",
    "    Combines Softmax + Negative Log-Likelihood\n",
    "    \n",
    "    L = -(1/N) * Σ Σ y_true[i,c] * log(y_pred[i,c])\n",
    "    \n",
    "    When combined with softmax, gradient simplifies to:\n",
    "    dL/dz = y_pred - y_true (where z is logit before softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "        self.y_true = None\n",
    "        self.epsilon = 1e-10\n",
    "    \n",
    "    def forward(self, logits: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: Raw scores before softmax (num_classes, batch_size)\n",
    "            y_true: One-hot encoded labels (num_classes, batch_size)\n",
    "        \"\"\"\n",
    "        # Apply softmax\n",
    "        softmax = Softmax()\n",
    "        self.y_pred = softmax.forward(logits)\n",
    "        self.y_true = y_true\n",
    "        \n",
    "        # Clip for numerical stability\n",
    "        y_pred_clipped = np.clip(self.y_pred, self.epsilon, 1 - self.epsilon)\n",
    "        \n",
    "        # Cross-entropy\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred_clipped), axis=0))\n",
    "    \n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gradient w.r.t. logits (before softmax)\n",
    "        \n",
    "        Simplified: dL/dz = (y_pred - y_true) / batch_size\n",
    "        \"\"\"\n",
    "        batch_size = self.y_pred.shape[1]\n",
    "        return (self.y_pred - self.y_true) / batch_size\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"CrossEntropyLoss()\"\n",
    "\n",
    "# Test\n",
    "ce_loss = CrossEntropyLoss()\n",
    "\n",
    "# Logits for 3 classes, batch of 2\n",
    "logits = np.array([[2.0, 1.0],   # class 0\n",
    "                   [1.0, 3.0],   # class 1\n",
    "                   [0.1, 0.5]])  # class 2\n",
    "\n",
    "# One-hot encoded labels (true class: [0, 1])\n",
    "y_true = np.array([[1.0, 0.0],\n",
    "                   [0.0, 1.0],\n",
    "                   [0.0, 0.0]])\n",
    "\n",
    "loss = ce_loss.forward(logits, y_true)\n",
    "grad = ce_loss.backward()\n",
    "\n",
    "print(f\"Logits:\\n{logits}\")\n",
    "print(f\"\\nPredicted probabilities (after softmax):\\n{ce_loss.y_pred}\")\n",
    "print(f\"\\nTrue labels (one-hot):\\n{y_true}\")\n",
    "print(f\"\\nCross-Entropy Loss: {loss:.4f}\")\n",
    "print(f\"\\nGradient:\\n{grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Optimizers\n",
    "\n",
    "**Time:** 20-25 minutes  \n",
    "**Difficulty:** Medium\n",
    "\n",
    "Different optimization algorithms for parameter updates.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent\n",
    "    \n",
    "    θ = θ - α * ∇L(θ)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, params: dict, grads: dict):\n",
    "        \"\"\"\n",
    "        Update parameters using gradients\n",
    "        \n",
    "        Args:\n",
    "            params: Dictionary of parameters\n",
    "            grads: Dictionary of gradients\n",
    "        \"\"\"\n",
    "        for key in params:\n",
    "            params[key] -= self.learning_rate * grads[key]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"SGD(lr={self.learning_rate})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 SGD with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum:\n",
    "    \"\"\"\n",
    "    SGD with Momentum\n",
    "    \n",
    "    v = β * v + ∇L(θ)\n",
    "    θ = θ - α * v\n",
    "    \n",
    "    Helps accelerate in relevant direction and dampen oscillations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01, momentum: float = 0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocities = {}\n",
    "    \n",
    "    def update(self, params: dict, grads: dict):\n",
    "        # Initialize velocities on first call\n",
    "        if not self.velocities:\n",
    "            for key in params:\n",
    "                self.velocities[key] = np.zeros_like(params[key])\n",
    "        \n",
    "        # Update with momentum\n",
    "        for key in params:\n",
    "            self.velocities[key] = (\n",
    "                self.momentum * self.velocities[key] + grads[key]\n",
    "            )\n",
    "            params[key] -= self.learning_rate * self.velocities[key]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"SGDMomentum(lr={self.learning_rate}, momentum={self.momentum})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \"\"\"\n",
    "    Adam Optimizer (Adaptive Moment Estimation)\n",
    "    \n",
    "    Combines momentum (first moment) and RMSProp (second moment)\n",
    "    \n",
    "    m = β1 * m + (1 - β1) * ∇L(θ)         # First moment (momentum)\n",
    "    v = β2 * v + (1 - β2) * (∇L(θ))^2     # Second moment (variance)\n",
    "    m_hat = m / (1 - β1^t)                 # Bias correction\n",
    "    v_hat = v / (1 - β2^t)                 # Bias correction\n",
    "    θ = θ - α * m_hat / (sqrt(v_hat) + ε)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.001, beta1: float = 0.9, \n",
    "                 beta2: float = 0.999, epsilon: float = 1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.m = {}  # First moment\n",
    "        self.v = {}  # Second moment\n",
    "        self.t = 0   # Time step\n",
    "    \n",
    "    def update(self, params: dict, grads: dict):\n",
    "        # Initialize moments on first call\n",
    "        if not self.m:\n",
    "            for key in params:\n",
    "                self.m[key] = np.zeros_like(params[key])\n",
    "                self.v[key] = np.zeros_like(params[key])\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        for key in params:\n",
    "            # Update biased first moment estimate\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "            \n",
    "            # Update biased second moment estimate\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)\n",
    "            \n",
    "            # Bias correction\n",
    "            m_hat = self.m[key] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[key] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # Update parameters\n",
    "            params[key] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Adam(lr={self.learning_rate}, β1={self.beta1}, β2={self.beta2})\"\n",
    "\n",
    "# Demo comparison\n",
    "print(\"Optimizers created:\")\n",
    "print(f\"1. {SGD(learning_rate=0.01)}\")\n",
    "print(f\"2. {SGDMomentum(learning_rate=0.01, momentum=0.9)}\")\n",
    "print(f\"3. {Adam(learning_rate=0.001)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Complete Neural Network\n",
    "\n",
    "**Time:** 40-50 minutes  \n",
    "**Difficulty:** Hard\n",
    "\n",
    "Putting it all together: A complete multi-layer neural network from scratch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Complete feedforward neural network from scratch\n",
    "    \n",
    "    Architecture: Input → Linear → ReLU → Linear → ReLU → Linear → Softmax\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_sizes: list, output_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_sizes: List of hidden layer sizes\n",
    "            output_size: Number of output classes\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "        \n",
    "        # Build network architecture\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Add linear layer\n",
    "            self.layers.append(Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            \n",
    "            # Add activation (ReLU for hidden, Softmax for output)\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                self.activations.append(ReLU())\n",
    "            else:\n",
    "                self.activations.append(Softmax())\n",
    "        \n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass through network\n",
    "        \n",
    "        Args:\n",
    "            X: Input (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            Output probabilities (output_size, batch_size)\n",
    "        \"\"\"\n",
    "        out = X\n",
    "        \n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            out = layer.forward(out)\n",
    "            out = activation.forward(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray):\n",
    "        \"\"\"\n",
    "        Backward pass through network\n",
    "        \n",
    "        Args:\n",
    "            grad_output: Gradient from loss (output_size, batch_size)\n",
    "        \"\"\"\n",
    "        grad = grad_output\n",
    "        \n",
    "        # Backward through layers in reverse\n",
    "        for layer, activation in zip(reversed(self.layers), reversed(self.activations)):\n",
    "            grad = activation.backward(grad)\n",
    "            grad = layer.backward(grad)\n",
    "    \n",
    "    def update_params(self, optimizer):\n",
    "        \"\"\"\n",
    "        Update parameters using optimizer\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            params = {'W': layer.W, 'b': layer.b}\n",
    "            grads = {'W': layer.dW, 'b': layer.db}\n",
    "            optimizer.update(params, grads)\n",
    "    \n",
    "    def train_step(self, X: np.ndarray, y: np.ndarray, optimizer) -> float:\n",
    "        \"\"\"\n",
    "        Single training step\n",
    "        \n",
    "        Args:\n",
    "            X: Input batch (batch_size, input_size)\n",
    "            y: One-hot labels (output_size, batch_size)\n",
    "            optimizer: Optimizer instance\n",
    "        \n",
    "        Returns:\n",
    "            loss: Scalar loss value\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        logits = self.forward(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.loss_fn.forward(logits, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        grad = self.loss_fn.backward()\n",
    "        self.backward(grad)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.update_params(optimizer)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class labels\n",
    "        \n",
    "        Args:\n",
    "            X: Input (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            Predicted class indices (batch_size,)\n",
    "        \"\"\"\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=0)\n",
    "    \n",
    "    def accuracy(self, X: np.ndarray, y_labels: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute accuracy\n",
    "        \n",
    "        Args:\n",
    "            X: Input (batch_size, input_size)\n",
    "            y_labels: True class indices (batch_size,)\n",
    "        \n",
    "        Returns:\n",
    "            Accuracy score\n",
    "        \"\"\"\n",
    "        preds = self.predict(X)\n",
    "        return np.mean(preds == y_labels)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        arch = []\n",
    "        for layer in self.layers:\n",
    "            arch.append(f\"Linear({layer.in_features}→{layer.out_features})\")\n",
    "        return \"NeuralNetwork(\\n  \" + \"\\n  \".join(arch) + \"\\n)\"\n",
    "\n",
    "# Test the complete network\n",
    "print(\"Building neural network...\\n\")\n",
    "model = NeuralNetwork(input_size=4, hidden_sizes=[8, 8], output_size=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot\n",
    "def to_one_hot(y, num_classes):\n",
    "    one_hot = np.zeros((num_classes, len(y)))\n",
    "    one_hot[y, np.arange(len(y))] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_train_onehot = to_one_hot(y_train, 3)\n",
    "y_test_onehot = to_one_hot(y_test, 3)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Number of classes: 3 (Iris species)\\n\")\n",
    "\n",
    "# Create model and optimizer\n",
    "model = NeuralNetwork(input_size=4, hidden_sizes=[16, 8], output_size=3)\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "print(\"Training neural network...\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Mini-batch training\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train_onehot[:, batch_indices]\n",
    "        \n",
    "        loss = model.train_step(X_batch, y_batch, optimizer)\n",
    "        epoch_loss += loss\n",
    "        num_batches += 1\n",
    "    \n",
    "    epoch_loss /= num_batches\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    # Compute accuracies\n",
    "    train_acc = model.accuracy(X_train, y_train)\n",
    "    test_acc = model.accuracy(X_test, y_test)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Loss = {epoch_loss:.4f}, \"\n",
    "              f\"Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {test_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Loss curve\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accs, label='Train Accuracy')\n",
    "plt.plot(test_accs, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Confusion matrix\n",
    "plt.subplot(1, 3, 3)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.imshow(cm, cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        plt.text(j, i, str(cm[i, j]), ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Discussion Points\n",
    "\n",
    "### Q1: Why use Xavier/He initialization instead of zeros or random?\n",
    "\n",
    "**Answer:**\n",
    "- **Zeros:** All neurons learn the same features (symmetry problem)\n",
    "- **Random (large):** Activations explode or vanish\n",
    "- **Xavier:** Variance of outputs = variance of inputs (for sigmoid/tanh)\n",
    "- **He:** Xavier × 2 (better for ReLU because it kills half the neurons)\n",
    "\n",
    "**Xavier:** $W \\sim U(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}})$\n",
    "\n",
    "**He:** $W \\sim N(0, \\sqrt{\\frac{2}{n_{in}}})$\n",
    "\n",
    "---\n",
    "\n",
    "### Q2: Why do we need the backward pass?\n",
    "\n",
    "**Answer:**\n",
    "The backward pass computes gradients using **backpropagation** (chain rule):\n",
    "\n",
    "1. **Chain Rule:** $\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial W}$\n",
    "\n",
    "2. **Flow:** Loss → Output Layer → Hidden Layers → Input\n",
    "\n",
    "3. **Purpose:** Tell each parameter how to change to reduce loss\n",
    "\n",
    "Without backprop, we'd need to compute gradients numerically (extremely slow!).\n",
    "\n",
    "---\n",
    "\n",
    "### Q3: ReLU vs Sigmoid - which is better and why?\n",
    "\n",
    "**ReLU Advantages:**\n",
    "- No vanishing gradient problem (gradient is 0 or 1)\n",
    "- Faster computation (no expensive exp)\n",
    "- Sparse activations (some neurons output 0)\n",
    "- Works better in practice for deep networks\n",
    "\n",
    "**ReLU Disadvantages:**\n",
    "- Dying ReLU problem (neurons output 0 forever)\n",
    "- Not zero-centered\n",
    "\n",
    "**When to use Sigmoid:**\n",
    "- Binary classification output layer\n",
    "- When you need outputs in (0, 1)\n",
    "- Gates in LSTMs/GRUs\n",
    "\n",
    "**When to use ReLU:**\n",
    "- Hidden layers in deep networks (default choice)\n",
    "\n",
    "---\n",
    "\n",
    "### Q4: Why combine Softmax with Cross-Entropy?\n",
    "\n",
    "**Answer:**\n",
    "1. **Numerical Stability:** Computing them separately can cause overflow/underflow\n",
    "2. **Simplified Gradient:** Combined gradient is just `y_pred - y_true`\n",
    "3. **Efficiency:** One backward pass instead of two\n",
    "\n",
    "**Mathematical reason:**\n",
    "```\n",
    "Softmax: p_i = e^(z_i) / Σ e^(z_j)\n",
    "Cross-Entropy: L = -Σ y_i * log(p_i)\n",
    "\n",
    "Combined gradient: ∂L/∂z_i = p_i - y_i\n",
    "```\n",
    "\n",
    "This is **much simpler** than computing each gradient separately!\n",
    "\n",
    "---\n",
    "\n",
    "### Q5: Adam vs SGD - when to use each?\n",
    "\n",
    "**Adam:**\n",
    "- **Pros:** Adaptive learning rates, fast convergence, less tuning\n",
    "- **Cons:** Can overfit, higher memory usage\n",
    "- **Use when:** Training deep networks, need fast iteration\n",
    "\n",
    "**SGD (+Momentum):**\n",
    "- **Pros:** Better generalization, more stable\n",
    "- **Cons:** Requires learning rate tuning, slower convergence\n",
    "- **Use when:** Final production model, have time to tune\n",
    "\n",
    "**Common Practice:**\n",
    "- Start with Adam for quick experimentation\n",
    "- Switch to SGD+Momentum for final model\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've implemented from scratch:\n",
    "\n",
    "✅ **Linear Layer** - Forward and backward pass  \n",
    "✅ **Activations** - ReLU, Sigmoid, Tanh, Softmax  \n",
    "✅ **Loss Functions** - MSE, BCE, Cross-Entropy  \n",
    "✅ **Optimizers** - SGD, Momentum, Adam  \n",
    "✅ **Complete Neural Network** - End-to-end training\n",
    "\n",
    "**You're now ready for deep learning coding interviews!** 🚀\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
