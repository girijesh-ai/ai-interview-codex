{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Agents and Deep Reasoning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Research agents represent a sophisticated evolution of agentic AI systems, designed to handle complex information gathering, analysis, and synthesis tasks. Unlike simple question-answering systems, research agents employ **deep reasoning architectures** that break down complex queries, explore multiple reasoning paths, and synthesize comprehensive answers with proper citations.\n",
    "\n",
    "### Key Characteristics of Research Agents\n",
    "\n",
    "1. **Query Decomposition** - Breaking complex questions into manageable sub-queries\n",
    "2. **Multi-Source Search** - Gathering information from diverse sources simultaneously\n",
    "3. **Deep Reasoning** - Exploring multiple reasoning paths (CoT, ToT, GoT)\n",
    "4. **Synthesis & Attribution** - Combining insights with proper citations\n",
    "5. **Quality Evaluation** - Self-assessing answer completeness and accuracy\n",
    "\n",
    "### Research Agents vs. Simple Search\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph \"Simple Search System\"\n",
    "        SS1[User Query] --> SS2[Single Search]\n",
    "        SS2 --> SS3[Top Results]\n",
    "        SS3 --> SS4[Direct Answer]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Research Agent System\"\n",
    "        RA1[Complex Query] --> RA2[Query Analysis]\n",
    "        RA2 --> RA3[Decompose into Sub-Queries]\n",
    "        RA3 --> RA4[Parallel Search]\n",
    "        RA4 --> RA5[Deep Reasoning]\n",
    "        RA5 --> RA6{Quality Check}\n",
    "        RA6 -->|Insufficient| RA4\n",
    "        RA6 -->|Sufficient| RA7[Synthesize with Citations]\n",
    "        RA7 --> RA8[Comprehensive Answer]\n",
    "    end\n",
    "```\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- **Academic Research** - Literature reviews, hypothesis generation\n",
    "- **Market Intelligence** - Competitive analysis, trend identification\n",
    "- **Technical Investigation** - Root cause analysis, technology evaluation\n",
    "- **Legal Research** - Case law analysis, precedent discovery\n",
    "- **Medical Research** - Clinical guideline synthesis, drug interaction analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deep Reasoning Architectures\n",
    "\n",
    "### 1.1 Chain of Thought (CoT)\n",
    "\n",
    "Chain of Thought prompting encourages LLMs to show their reasoning process step-by-step, improving accuracy on complex reasoning tasks.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Question] --> B[Step 1: Identify Given Info]\n",
    "    B --> C[Step 2: Determine Approach]\n",
    "    C --> D[Step 3: Calculate]\n",
    "    D --> E[Step 4: Verify]\n",
    "    E --> F[Final Answer]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style F fill:#c8e6c9\n",
    "    style B fill:#fff9c4\n",
    "    style C fill:#fff9c4\n",
    "    style D fill:#fff9c4\n",
    "    style E fill:#fff9c4\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Improved accuracy on complex reasoning tasks\n",
    "- Transparency in reasoning process\n",
    "- Easier to debug and identify errors\n",
    "\n",
    "**When to Use:**\n",
    "- Math word problems\n",
    "- Multi-step logical reasoning\n",
    "- Tasks requiring intermediate calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import anthropic\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "# Initialize Anthropic client\n",
    "client = anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "def chain_of_thought_reasoning(question: str) -> dict:\n",
    "    \"\"\"\n",
    "    Demonstrate Chain of Thought reasoning\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CHAIN OF THOUGHT REASONING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    cot_prompt = f\"\"\"Let's solve this step by step:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please show your reasoning process:\n",
    "1. Identify what information is given\n",
    "2. Determine what approach to use\n",
    "3. Work through the calculation/reasoning\n",
    "4. Verify the answer makes sense\n",
    "5. State the final answer\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=2048,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": cot_prompt\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    reasoning = response.content[0].text\n",
    "    print(\"\\n\" + reasoning)\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"reasoning_steps\": reasoning,\n",
    "        \"tokens_used\": response.usage.input_tokens + response.usage.output_tokens\n",
    "    }\n",
    "\n",
    "# Example: Complex reasoning problem\n",
    "result = chain_of_thought_reasoning(\n",
    "    \"A store sells apples for $2 each. If you buy 10 or more, you get 20% off. \"\n",
    "    \"If you buy 15 apples and have a $5 coupon, how much do you pay?\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTokens used: {result['tokens_used']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tree of Thoughts (ToT)\n",
    "\n",
    "Tree of Thoughts extends CoT by exploring multiple reasoning paths simultaneously, evaluating each path, and selecting the best one.\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Problem] --> B[Generate Multiple Approaches]\n",
    "    B --> C1[Path 1: Approach A]\n",
    "    B --> C2[Path 2: Approach B]\n",
    "    B --> C3[Path 3: Approach C]\n",
    "    \n",
    "    C1 --> D1[Evaluate Path 1]\n",
    "    C2 --> D2[Evaluate Path 2]\n",
    "    C3 --> D3[Evaluate Path 3]\n",
    "    \n",
    "    D1 --> E{Select Best Path}\n",
    "    D2 --> E\n",
    "    D3 --> E\n",
    "    \n",
    "    E --> F[Expand Best Path]\n",
    "    F --> G[Solution]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style E fill:#ffccbc\n",
    "    style G fill:#c8e6c9\n",
    "```\n",
    "\n",
    "**Key Differences from CoT:**\n",
    "- Explores multiple reasoning paths in parallel\n",
    "- Evaluates and prunes less promising paths\n",
    "- Can backtrack and explore alternative paths\n",
    "- More computationally expensive but more thorough\n",
    "\n",
    "**When to Use:**\n",
    "- Strategic planning and game-playing\n",
    "- Problems with multiple valid approaches\n",
    "- High-stakes decisions requiring exploration of alternatives\n",
    "- Creative problem-solving tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_of_thoughts_reasoning(problem: str, num_paths: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    Implement Tree of Thoughts reasoning pattern\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TREE OF THOUGHTS REASONING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Generate multiple reasoning paths\n",
    "    print(f\"\\n[Step 1] Generating {num_paths} different approaches...\\n\")\n",
    "    \n",
    "    paths = []\n",
    "    for i in range(num_paths):\n",
    "        path_prompt = f\"\"\"Problem: {problem}\n",
    "\n",
    "Generate approach #{i+1} to solve this problem. Think of a different strategy or perspective.\n",
    "Provide a brief outline of the approach (2-3 sentences).\"\"\"\n",
    "        \n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=512,\n",
    "            messages=[{\"role\": \"user\", \"content\": path_prompt}]\n",
    "        )\n",
    "        \n",
    "        approach = response.content[0].text\n",
    "        print(f\"Path {i+1}:\\n{approach}\\n\")\n",
    "        paths.append(approach)\n",
    "    \n",
    "    # Step 2: Evaluate each path\n",
    "    print(\"[Step 2] Evaluating each approach...\\n\")\n",
    "    \n",
    "    evaluations = []\n",
    "    for i, path in enumerate(paths):\n",
    "        eval_prompt = f\"\"\"Evaluate this approach for solving the problem:\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Approach: {path}\n",
    "\n",
    "Rate this approach on:\n",
    "1. Feasibility (1-10)\n",
    "2. Completeness (1-10)\n",
    "3. Efficiency (1-10)\n",
    "\n",
    "Provide scores and brief justification.\"\"\"\n",
    "        \n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=512,\n",
    "            messages=[{\"role\": \"user\", \"content\": eval_prompt}]\n",
    "        )\n",
    "        \n",
    "        evaluation = response.content[0].text\n",
    "        print(f\"Evaluation {i+1}:\\n{evaluation}\\n\")\n",
    "        evaluations.append(evaluation)\n",
    "    \n",
    "    # Step 3: Select best path and expand\n",
    "    print(\"[Step 3] Selecting best approach and developing full solution...\\n\")\n",
    "    \n",
    "    selection_prompt = f\"\"\"Based on these approaches and evaluations:\n",
    "\n",
    "{chr(10).join([f'Approach {i+1}: {p}\\nEvaluation: {e}\\n' for i, (p, e) in enumerate(zip(paths, evaluations))])}\n",
    "\n",
    "Select the best approach and develop a complete solution to: {problem}\"\"\"\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=2048,\n",
    "        messages=[{\"role\": \"user\", \"content\": selection_prompt}]\n",
    "    )\n",
    "    \n",
    "    final_solution = response.content[0].text\n",
    "    print(f\"Final Solution:\\n{final_solution}\")\n",
    "    \n",
    "    return {\n",
    "        \"problem\": problem,\n",
    "        \"paths_explored\": paths,\n",
    "        \"evaluations\": evaluations,\n",
    "        \"final_solution\": final_solution\n",
    "    }\n",
    "\n",
    "# Example: Strategic problem\n",
    "result = tree_of_thoughts_reasoning(\n",
    "    \"Design a system to reduce customer churn for a SaaS product by 30% within 6 months\",\n",
    "    num_paths=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Graph of Thoughts (GoT)\n",
    "\n",
    "Graph of Thoughts represents reasoning as a graph structure where thoughts can be combined, refined, and connected in non-linear ways.\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Initial Problem] --> B[Thought 1]\n",
    "    A --> C[Thought 2]\n",
    "    A --> D[Thought 3]\n",
    "    \n",
    "    B --> E[Refined Thought 1]\n",
    "    C --> E\n",
    "    \n",
    "    C --> F[Refined Thought 2]\n",
    "    D --> F\n",
    "    \n",
    "    E --> G[Synthesized Insight]\n",
    "    F --> G\n",
    "    \n",
    "    G --> H[Final Solution]\n",
    "    B --> H\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style G fill:#ffccbc\n",
    "    style H fill:#c8e6c9\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- Non-linear reasoning paths\n",
    "- Thoughts can be aggregated and refined\n",
    "- Supports iterative refinement\n",
    "- More flexible than tree structure\n",
    "\n",
    "**When to Use:**\n",
    "- Complex research synthesis\n",
    "- Multi-faceted problem analysis\n",
    "- Iterative refinement scenarios\n",
    "- When insights from different paths can be combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research Agent Architecture\n",
    "\n",
    "### Complete Research Agent System\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant U as User\n",
    "    participant QA as Query Analyzer\n",
    "    participant QD as Query Decomposer\n",
    "    participant S as Search Coordinator\n",
    "    participant R as Reasoner\n",
    "    participant Syn as Synthesizer\n",
    "    participant E as Evaluator\n",
    "    \n",
    "    U->>QA: Complex Research Query\n",
    "    QA->>QD: Analyze & Decompose\n",
    "    QD->>S: Sub-queries\n",
    "    \n",
    "    par Parallel Search\n",
    "        S->>S: Search Source 1\n",
    "        S->>S: Search Source 2\n",
    "        S->>S: Search Source 3\n",
    "    end\n",
    "    \n",
    "    S->>R: Raw Results\n",
    "    R->>R: Deep Reasoning (CoT/ToT/GoT)\n",
    "    R->>Syn: Analyzed Information\n",
    "    Syn->>Syn: Synthesize with Citations\n",
    "    Syn->>E: Draft Answer\n",
    "    \n",
    "    E->>E: Quality Check\n",
    "    alt Insufficient Quality\n",
    "        E->>QD: Request More Info\n",
    "        QD->>S: Additional Sub-queries\n",
    "    else Sufficient Quality\n",
    "        E->>U: Comprehensive Answer\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Complete Research Agent\n",
    "\n",
    "### 3.1 Query Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_research_query(complex_query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Break down a complex research query into manageable sub-queries\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"QUERY DECOMPOSITION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nOriginal Query: {complex_query}\\n\")\n",
    "    \n",
    "    decomposition_prompt = f\"\"\"You are a research query analyzer. Break down this complex query into 3-5 specific, searchable sub-queries.\n",
    "\n",
    "Complex Query: {complex_query}\n",
    "\n",
    "For each sub-query:\n",
    "1. Make it specific and searchable\n",
    "2. Ensure it addresses one aspect of the main query\n",
    "3. Order them logically (foundational concepts first)\n",
    "\n",
    "Return ONLY a JSON array of sub-queries, like:\n",
    "[\"sub-query 1\", \"sub-query 2\", \"sub-query 3\"]\"\"\"\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1024,\n",
    "        messages=[{\"role\": \"user\", \"content\": decomposition_prompt}]\n",
    "    )\n",
    "    \n",
    "    result_text = response.content[0].text\n",
    "    \n",
    "    # Extract JSON array from response\n",
    "    try:\n",
    "        # Find JSON array in response\n",
    "        start = result_text.find('[')\n",
    "        end = result_text.rfind(']') + 1\n",
    "        sub_queries = json.loads(result_text[start:end])\n",
    "    except:\n",
    "        # Fallback: split by newlines\n",
    "        sub_queries = [line.strip().strip('\"').strip() \n",
    "                      for line in result_text.split('\\n') \n",
    "                      if line.strip() and not line.strip().startswith('[') \n",
    "                      and not line.strip().startswith(']')]\n",
    "    \n",
    "    print(\"Sub-queries generated:\")\n",
    "    for i, sq in enumerate(sub_queries, 1):\n",
    "        print(f\"  {i}. {sq}\")\n",
    "    \n",
    "    return sub_queries\n",
    "\n",
    "# Example\n",
    "sub_queries = decompose_research_query(\n",
    "    \"What are the current best practices for implementing multi-agent AI systems in production, \"\n",
    "    \"including performance optimization, monitoring, and cost management?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multi-Source Search Coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "# Simulated search tools (in production, these would be real integrations)\n",
    "def search_web(query: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Simulate web search (would integrate with real search API)\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"title\": f\"Result for: {query}\",\n",
    "            \"snippet\": f\"Web content about {query}...\",\n",
    "            \"url\": f\"https://example.com/{query.replace(' ', '-')}\",\n",
    "            \"source\": \"web\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "def search_arxiv(query: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Simulate arXiv academic search\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"title\": f\"Academic paper: {query}\",\n",
    "            \"snippet\": f\"Research findings on {query}...\",\n",
    "            \"url\": f\"https://arxiv.org/abs/{hash(query) % 10000}\",\n",
    "            \"source\": \"arxiv\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "def search_github(query: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Simulate GitHub code search\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"title\": f\"Code repository: {query}\",\n",
    "            \"snippet\": f\"Implementation examples for {query}...\",\n",
    "            \"url\": f\"https://github.com/search?q={query.replace(' ', '+')}\",\n",
    "            \"source\": \"github\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "def parallel_multi_source_search(sub_queries: List[str]) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Search multiple sources in parallel for each sub-query\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MULTI-SOURCE PARALLEL SEARCH\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for i, query in enumerate(sub_queries, 1):\n",
    "        print(f\"\\n[Query {i}] {query}\")\n",
    "        print(\"  Searching: Web, arXiv, GitHub...\")\n",
    "        \n",
    "        # In production, these would run in parallel using asyncio or threads\n",
    "        web_results = search_web(query)\n",
    "        arxiv_results = search_arxiv(query)\n",
    "        github_results = search_github(query)\n",
    "        \n",
    "        all_results[query] = {\n",
    "            \"web\": web_results,\n",
    "            \"arxiv\": arxiv_results,\n",
    "            \"github\": github_results\n",
    "        }\n",
    "        \n",
    "        print(f\"  Found: {len(web_results)} web, {len(arxiv_results)} papers, {len(github_results)} repos\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Example\n",
    "search_results = parallel_multi_source_search(sub_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Information Synthesis with Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_research_with_citations(\n",
    "    original_query: str,\n",
    "    sub_queries: List[str],\n",
    "    search_results: Dict[str, List[Dict]]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Synthesize information from multiple sources with proper attribution\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SYNTHESIS WITH CITATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Prepare search results for LLM\n",
    "    formatted_results = []\n",
    "    citation_map = {}\n",
    "    citation_num = 1\n",
    "    \n",
    "    for query, sources in search_results.items():\n",
    "        formatted_results.append(f\"\\n### Sub-query: {query}\\n\")\n",
    "        \n",
    "        for source_type, results in sources.items():\n",
    "            for result in results:\n",
    "                citation_id = f\"[{citation_num}]\"\n",
    "                citation_map[citation_num] = {\n",
    "                    \"title\": result[\"title\"],\n",
    "                    \"url\": result[\"url\"],\n",
    "                    \"source\": result[\"source\"]\n",
    "                }\n",
    "                \n",
    "                formatted_results.append(\n",
    "                    f\"{citation_id} {result['title']}\\n\"\n",
    "                    f\"Source: {result['source']}\\n\"\n",
    "                    f\"Content: {result['snippet']}\\n\"\n",
    "                )\n",
    "                \n",
    "                citation_num += 1\n",
    "    \n",
    "    # Synthesis prompt\n",
    "    synthesis_prompt = f\"\"\"You are a research synthesis expert. Synthesize the following research into a comprehensive answer.\n",
    "\n",
    "Original Question: {original_query}\n",
    "\n",
    "Research Findings:\n",
    "{''.join(formatted_results)}\n",
    "\n",
    "Instructions:\n",
    "1. Create a comprehensive, well-structured answer\n",
    "2. Use citations [1], [2], etc. after claims to reference sources\n",
    "3. Organize by themes, not by sub-query\n",
    "4. Highlight key insights and best practices\n",
    "5. Note any conflicting information or gaps\n",
    "6. Include a brief conclusion with actionable takeaways\n",
    "\n",
    "Format your response in markdown with clear sections.\"\"\"\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=4096,\n",
    "        messages=[{\"role\": \"user\", \"content\": synthesis_prompt}]\n",
    "    )\n",
    "    \n",
    "    synthesized_answer = response.content[0].text\n",
    "    \n",
    "    print(\"\\n\" + synthesized_answer)\n",
    "    \n",
    "    # Append references\n",
    "    references = \"\\n\\n## References\\n\\n\"\n",
    "    for num, citation in citation_map.items():\n",
    "        references += f\"[{num}] {citation['title']} ({citation['source']})\\n    {citation['url']}\\n\\n\"\n",
    "    \n",
    "    print(references)\n",
    "    \n",
    "    return {\n",
    "        \"original_query\": original_query,\n",
    "        \"synthesized_answer\": synthesized_answer,\n",
    "        \"references\": citation_map,\n",
    "        \"num_sources\": len(citation_map)\n",
    "    }\n",
    "\n",
    "# Example\n",
    "synthesis = synthesize_research_with_citations(\n",
    "    \"What are the current best practices for implementing multi-agent AI systems in production?\",\n",
    "    sub_queries,\n",
    "    search_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Quality Evaluation and Iterative Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_research_quality(\n",
    "    original_query: str,\n",
    "    synthesized_answer: str,\n",
    "    num_sources: int\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate the quality and completeness of research answer\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"QUALITY EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    eval_prompt = f\"\"\"Evaluate this research answer for quality and completeness.\n",
    "\n",
    "Original Query: {original_query}\n",
    "\n",
    "Answer:\n",
    "{synthesized_answer}\n",
    "\n",
    "Number of sources used: {num_sources}\n",
    "\n",
    "Evaluate on:\n",
    "1. **Completeness** (1-10): Does it fully address all aspects of the query?\n",
    "2. **Accuracy** (1-10): Are claims properly supported with citations?\n",
    "3. **Clarity** (1-10): Is it well-organized and easy to understand?\n",
    "4. **Depth** (1-10): Does it provide sufficient detail and insight?\n",
    "5. **Balance** (1-10): Does it present multiple perspectives?\n",
    "\n",
    "For each score below 8, suggest specific improvements.\n",
    "\n",
    "Return your evaluation in this format:\n",
    "- Completeness: X/10 - [reasoning]\n",
    "- Accuracy: X/10 - [reasoning]\n",
    "- Clarity: X/10 - [reasoning]\n",
    "- Depth: X/10 - [reasoning]\n",
    "- Balance: X/10 - [reasoning]\n",
    "\n",
    "Overall Assessment: [PASS/NEEDS_IMPROVEMENT]\n",
    "If NEEDS_IMPROVEMENT, list specific gaps or areas to research further.\"\"\"\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1024,\n",
    "        messages=[{\"role\": \"user\", \"content\": eval_prompt}]\n",
    "    )\n",
    "    \n",
    "    evaluation = response.content[0].text\n",
    "    print(\"\\n\" + evaluation)\n",
    "    \n",
    "    # Determine if answer passes quality threshold\n",
    "    passes_quality = \"PASS\" in evaluation.upper()\n",
    "    \n",
    "    return {\n",
    "        \"evaluation\": evaluation,\n",
    "        \"passes_quality\": passes_quality,\n",
    "        \"requires_iteration\": not passes_quality\n",
    "    }\n",
    "\n",
    "# Example\n",
    "quality_eval = evaluate_research_quality(\n",
    "    \"What are the current best practices for implementing multi-agent AI systems in production?\",\n",
    "    synthesis[\"synthesized_answer\"],\n",
    "    synthesis[\"num_sources\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Research Agent (End-to-End)\n",
    "\n",
    "### Research Agent Decision Flow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Receive Complex Query] --> B[Decompose Query]\n",
    "    B --> C[Parallel Multi-Source Search]\n",
    "    C --> D[Deep Reasoning & Analysis]\n",
    "    D --> E[Synthesize with Citations]\n",
    "    E --> F{Quality Evaluation}\n",
    "    \n",
    "    F -->|Score < 8| G[Identify Gaps]\n",
    "    G --> H[Generate Follow-up Queries]\n",
    "    H --> C\n",
    "    \n",
    "    F -->|Score >= 8| I[Return Comprehensive Answer]\n",
    "    \n",
    "    F -->|Max Iterations| J[Return Best Available Answer]\n",
    "    J --> K[Flag Limitations]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style F fill:#ffccbc\n",
    "    style I fill:#c8e6c9\n",
    "    style J fill:#ffe0b2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAgent:\n",
    "    \"\"\"\n",
    "    Complete research agent with iterative refinement\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_iterations: int = 2):\n",
    "        self.max_iterations = max_iterations\n",
    "        self.iteration_count = 0\n",
    "    \n",
    "    def research(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Conduct comprehensive research with quality checks\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RESEARCH AGENT INITIATED\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Query: {query}\\n\")\n",
    "        \n",
    "        all_sub_queries = []\n",
    "        all_results = {}\n",
    "        \n",
    "        while self.iteration_count < self.max_iterations:\n",
    "            self.iteration_count += 1\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ITERATION {self.iteration_count}/{self.max_iterations}\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "            \n",
    "            # Step 1: Decompose query\n",
    "            if self.iteration_count == 1:\n",
    "                sub_queries = decompose_research_query(query)\n",
    "            else:\n",
    "                # Generate follow-up queries based on gaps\n",
    "                sub_queries = self._generate_followup_queries(\n",
    "                    query, \n",
    "                    synthesis_result[\"synthesized_answer\"],\n",
    "                    quality_result[\"evaluation\"]\n",
    "                )\n",
    "            \n",
    "            all_sub_queries.extend(sub_queries)\n",
    "            \n",
    "            # Step 2: Multi-source search\n",
    "            search_results = parallel_multi_source_search(sub_queries)\n",
    "            all_results.update(search_results)\n",
    "            \n",
    "            # Step 3: Synthesize\n",
    "            synthesis_result = synthesize_research_with_citations(\n",
    "                query,\n",
    "                all_sub_queries,\n",
    "                all_results\n",
    "            )\n",
    "            \n",
    "            # Step 4: Evaluate quality\n",
    "            quality_result = evaluate_research_quality(\n",
    "                query,\n",
    "                synthesis_result[\"synthesized_answer\"],\n",
    "                synthesis_result[\"num_sources\"]\n",
    "            )\n",
    "            \n",
    "            # Check if we can stop\n",
    "            if quality_result[\"passes_quality\"]:\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"RESEARCH COMPLETE - Quality threshold met\")\n",
    "                print(\"=\"*60)\n",
    "                break\n",
    "            \n",
    "            if self.iteration_count >= self.max_iterations:\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"RESEARCH COMPLETE - Max iterations reached\")\n",
    "                print(\"=\"*60)\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"iterations\": self.iteration_count,\n",
    "            \"total_sub_queries\": len(all_sub_queries),\n",
    "            \"total_sources\": synthesis_result[\"num_sources\"],\n",
    "            \"final_answer\": synthesis_result[\"synthesized_answer\"],\n",
    "            \"references\": synthesis_result[\"references\"],\n",
    "            \"quality_evaluation\": quality_result[\"evaluation\"],\n",
    "            \"passed_quality\": quality_result[\"passes_quality\"]\n",
    "        }\n",
    "    \n",
    "    def _generate_followup_queries(\n",
    "        self, \n",
    "        original_query: str,\n",
    "        current_answer: str,\n",
    "        evaluation: str\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate follow-up queries to fill gaps identified in evaluation\n",
    "        \"\"\"\n",
    "        print(\"\\n[Generating follow-up queries based on gaps...]\\n\")\n",
    "        \n",
    "        followup_prompt = f\"\"\"Based on this evaluation, generate 2-3 follow-up queries to fill the gaps:\n",
    "\n",
    "Original Query: {original_query}\n",
    "\n",
    "Current Answer:\n",
    "{current_answer[:500]}...\n",
    "\n",
    "Evaluation:\n",
    "{evaluation}\n",
    "\n",
    "Generate specific queries that will address the identified gaps or weaknesses.\n",
    "Return ONLY a JSON array like: [\"query 1\", \"query 2\", \"query 3\"]\"\"\"\n",
    "        \n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=512,\n",
    "            messages=[{\"role\": \"user\", \"content\": followup_prompt}]\n",
    "        )\n",
    "        \n",
    "        result_text = response.content[0].text\n",
    "        \n",
    "        try:\n",
    "            start = result_text.find('[')\n",
    "            end = result_text.rfind(']') + 1\n",
    "            followup_queries = json.loads(result_text[start:end])\n",
    "        except:\n",
    "            followup_queries = [\"Additional context needed\"]\n",
    "        \n",
    "        print(\"Follow-up queries:\")\n",
    "        for i, fq in enumerate(followup_queries, 1):\n",
    "            print(f\"  {i}. {fq}\")\n",
    "        \n",
    "        return followup_queries\n",
    "\n",
    "# Example: Complete research workflow\n",
    "agent = ResearchAgent(max_iterations=2)\n",
    "\n",
    "result = agent.research(\n",
    "    \"What are the key differences between LangGraph and CrewAI for building multi-agent systems, \"\n",
    "    \"and when should I choose each framework?\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESEARCH SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Iterations: {result['iterations']}\")\n",
    "print(f\"Sub-queries explored: {result['total_sub_queries']}\")\n",
    "print(f\"Sources consulted: {result['total_sources']}\")\n",
    "print(f\"Quality passed: {result['passed_quality']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Research Patterns\n",
    "\n",
    "### 5.1 Anthropic's Multi-Agent Research System\n",
    "\n",
    "Anthropic's approach to research agents uses multiple specialized agents:\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Research Coordinator] --> B[Query Planner Agent]\n",
    "    B --> C1[Search Agent 1]\n",
    "    B --> C2[Search Agent 2]\n",
    "    B --> C3[Search Agent 3]\n",
    "    \n",
    "    C1 --> D[Relevance Filter Agent]\n",
    "    C2 --> D\n",
    "    C3 --> D\n",
    "    \n",
    "    D --> E[Analysis Agent]\n",
    "    E --> F[Synthesis Agent]\n",
    "    F --> G[Citation Validator]\n",
    "    G --> H[Quality Reviewer Agent]\n",
    "    \n",
    "    H -->|Insufficient| B\n",
    "    H -->|Sufficient| I[Final Report]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style H fill:#ffccbc\n",
    "    style I fill:#c8e6c9\n",
    "```\n",
    "\n",
    "**Agent Specializations:**\n",
    "\n",
    "1. **Query Planner** - Decomposes research questions strategically\n",
    "2. **Search Agents** - Specialized for different source types (web, papers, code)\n",
    "3. **Relevance Filter** - Scores and ranks information by relevance\n",
    "4. **Analysis Agent** - Performs deep reasoning on gathered information\n",
    "5. **Synthesis Agent** - Combines insights from multiple sources\n",
    "6. **Citation Validator** - Ensures proper attribution\n",
    "7. **Quality Reviewer** - Evaluates completeness and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Evaluation Metrics for Research Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_research_metrics(result: Dict[str, Any]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate quantitative metrics for research quality\n",
    "    \"\"\"\n",
    "    answer = result[\"final_answer\"]\n",
    "    references = result[\"references\"]\n",
    "    \n",
    "    # Count citations in answer\n",
    "    import re\n",
    "    citations = re.findall(r'\\[(\\d+)\\]', answer)\n",
    "    unique_citations = len(set(citations))\n",
    "    total_citations = len(citations)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"answer_length\": len(answer),\n",
    "        \"num_sources\": len(references),\n",
    "        \"unique_citations\": unique_citations,\n",
    "        \"total_citations\": total_citations,\n",
    "        \"citation_density\": total_citations / (len(answer.split()) / 100),  # per 100 words\n",
    "        \"source_diversity\": len(set(ref[\"source\"] for ref in references.values())),\n",
    "        \"iterations_needed\": result[\"iterations\"],\n",
    "        \"efficiency_score\": unique_citations / result[\"iterations\"]\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESEARCH METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.2f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Example\n",
    "metrics = calculate_research_metrics(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production Best Practices\n",
    "\n",
    "### 6.1 Cost Optimization Strategies\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Research Request] --> B{Check Cache}\n",
    "    B -->|Hit| C[Return Cached Result]\n",
    "    B -->|Miss| D{Query Complexity}\n",
    "    \n",
    "    D -->|Simple| E[Single-Pass Research]\n",
    "    D -->|Complex| F[Multi-Iteration Research]\n",
    "    \n",
    "    E --> G[Use Haiku for Search]\n",
    "    F --> H[Use Sonnet for Analysis]\n",
    "    \n",
    "    G --> I{Quality Sufficient?}\n",
    "    H --> I\n",
    "    \n",
    "    I -->|No| J[Targeted Follow-up]\n",
    "    I -->|Yes| K[Cache Result]\n",
    "    \n",
    "    J --> F\n",
    "    K --> L[Return Answer]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style C fill:#c8e6c9\n",
    "    style L fill:#c8e6c9\n",
    "```\n",
    "\n",
    "**Cost Optimization Tips:**\n",
    "\n",
    "1. **Model Selection**\n",
    "   - Use Claude Haiku for query decomposition and search\n",
    "   - Use Claude Sonnet for deep reasoning and synthesis\n",
    "   - Use Claude Opus only for critical analysis\n",
    "\n",
    "2. **Caching Strategy**\n",
    "   - Cache sub-query decompositions for similar questions\n",
    "   - Cache search results with TTL\n",
    "   - Cache synthesized answers for exact query matches\n",
    "\n",
    "3. **Parallel Execution**\n",
    "   - Run all sub-query searches in parallel\n",
    "   - Use async/await for I/O-bound operations\n",
    "\n",
    "4. **Iterative Refinement**\n",
    "   - Set quality thresholds to avoid unnecessary iterations\n",
    "   - Generate targeted follow-ups instead of full re-research\n",
    "\n",
    "5. **Token Management**\n",
    "   - Truncate search results to relevant snippets\n",
    "   - Use summarization for lengthy documents\n",
    "   - Implement context window management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Monitoring and Observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class MonitoredResearchAgent(ResearchAgent):\n",
    "    \"\"\"\n",
    "    Research agent with comprehensive monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_iterations: int = 2):\n",
    "        super().__init__(max_iterations)\n",
    "        self.metrics = {\n",
    "            \"total_queries\": 0,\n",
    "            \"total_api_calls\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"total_time\": 0,\n",
    "            \"cache_hits\": 0,\n",
    "            \"cache_misses\": 0\n",
    "        }\n",
    "        self.query_history = []\n",
    "    \n",
    "    def research(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Research with monitoring and logging\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Record query\n",
    "        self.metrics[\"total_queries\"] += 1\n",
    "        query_record = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"query\": query,\n",
    "            \"status\": \"started\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Execute research\n",
    "            result = super().research(query)\n",
    "            \n",
    "            # Record success\n",
    "            elapsed = time.time() - start_time\n",
    "            self.metrics[\"total_time\"] += elapsed\n",
    "            \n",
    "            query_record.update({\n",
    "                \"status\": \"completed\",\n",
    "                \"elapsed_time\": elapsed,\n",
    "                \"iterations\": result[\"iterations\"],\n",
    "                \"sources\": result[\"total_sources\"],\n",
    "                \"quality_passed\": result[\"passed_quality\"]\n",
    "            })\n",
    "            \n",
    "            self.query_history.append(query_record)\n",
    "            \n",
    "            # Log metrics\n",
    "            self._log_metrics()\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            query_record.update({\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": str(e),\n",
    "                \"elapsed_time\": time.time() - start_time\n",
    "            })\n",
    "            self.query_history.append(query_record)\n",
    "            raise\n",
    "    \n",
    "    def _log_metrics(self):\n",
    "        \"\"\"\n",
    "        Log current metrics\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MONITORING METRICS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total Queries: {self.metrics['total_queries']}\")\n",
    "        print(f\"Average Time: {self.metrics['total_time'] / max(1, self.metrics['total_queries']):.2f}s\")\n",
    "        print(f\"Total API Calls: {self.metrics['total_api_calls']}\")\n",
    "        print(f\"Cache Hit Rate: {self.metrics['cache_hits'] / max(1, self.metrics['cache_hits'] + self.metrics['cache_misses']) * 100:.1f}%\")\n",
    "    \n",
    "    def get_performance_report(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate performance report\n",
    "        \"\"\"\n",
    "        successful = sum(1 for q in self.query_history if q[\"status\"] == \"completed\")\n",
    "        avg_time = sum(q[\"elapsed_time\"] for q in self.query_history) / len(self.query_history)\n",
    "        \n",
    "        report = f\"\"\"\n",
    "RESEARCH AGENT PERFORMANCE REPORT\n",
    "{'='*60}\n",
    "Total Queries: {len(self.query_history)}\n",
    "Successful: {successful}\n",
    "Failed: {len(self.query_history) - successful}\n",
    "Success Rate: {successful / len(self.query_history) * 100:.1f}%\n",
    "Average Time: {avg_time:.2f}s\n",
    "Total API Calls: {self.metrics['total_api_calls']}\n",
    "Total Tokens: {self.metrics['total_tokens']}\n",
    "        \"\"\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Example\n",
    "monitored_agent = MonitoredResearchAgent(max_iterations=2)\n",
    "print(\"Monitored research agent ready for production use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-World Use Cases\n",
    "\n",
    "### Use Case 1: Academic Literature Review\n",
    "\n",
    "```python\n",
    "# Configure agent for academic research\n",
    "academic_agent = ResearchAgent(max_iterations=3)\n",
    "\n",
    "result = academic_agent.research(\n",
    "    \"What are the latest advances in transformer architecture efficiency \"\n",
    "    \"for large language models, focusing on papers from 2024?\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Use Case 2: Technical Due Diligence\n",
    "\n",
    "```python\n",
    "# Configure for technical analysis\n",
    "tech_agent = ResearchAgent(max_iterations=2)\n",
    "\n",
    "result = tech_agent.research(\n",
    "    \"Evaluate the technical architecture, scalability, and security \"\n",
    "    \"considerations of deploying a multi-agent AI system in a financial services environment\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Use Case 3: Competitive Intelligence\n",
    "\n",
    "```python\n",
    "# Configure for market research\n",
    "market_agent = ResearchAgent(max_iterations=2)\n",
    "\n",
    "result = market_agent.research(\n",
    "    \"Compare the features, pricing, and market positioning of top 5 \"\n",
    "    \"AI coding assistants as of 2025\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interview Q&A\n",
    "\n",
    "### Q1: What's the difference between Chain of Thought and Tree of Thoughts?\n",
    "\n",
    "**Answer:**\n",
    "- **Chain of Thought (CoT)**: Linear reasoning through a single path with intermediate steps shown. Good for straightforward problems with one clear approach.\n",
    "- **Tree of Thoughts (ToT)**: Explores multiple reasoning paths in parallel, evaluates each, and selects the best. Better for complex problems with multiple valid approaches or requiring strategic planning.\n",
    "\n",
    "**Example:** For \"calculate 15% tip on $50\", CoT is sufficient. For \"design a go-to-market strategy\", ToT allows exploring multiple strategic approaches.\n",
    "\n",
    "### Q2: How do you prevent hallucinations in research agents?\n",
    "\n",
    "**Answer:**\n",
    "1. **Citation Requirements**: Force the agent to cite sources for every claim\n",
    "2. **Multi-Source Verification**: Cross-reference information from multiple sources\n",
    "3. **Citation Validation**: Verify that citations actually support the claims\n",
    "4. **Quality Evaluation**: Have a separate agent review for unsupported claims\n",
    "5. **Source Credibility**: Weight information by source reliability\n",
    "\n",
    "### Q3: What are the main challenges in production research agents?\n",
    "\n",
    "**Answer:**\n",
    "1. **Cost Management**: Research agents can make many API calls. Need caching, model selection strategy, and iteration limits.\n",
    "2. **Latency**: Multi-iteration research can be slow. Use parallel execution and set reasonable timeouts.\n",
    "3. **Quality Consistency**: Results can vary. Implement quality thresholds and monitoring.\n",
    "4. **Source Reliability**: Not all web sources are trustworthy. Need source ranking and credibility assessment.\n",
    "5. **Context Window Limits**: Large research results exceed context windows. Need summarization and chunking strategies.\n",
    "\n",
    "### Q4: When should you use iterative refinement vs. single-pass research?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Single-Pass:**\n",
    "- Simple, well-defined questions\n",
    "- Time-sensitive queries\n",
    "- Cost-constrained scenarios\n",
    "- Questions with abundant readily available information\n",
    "\n",
    "**Iterative Refinement:**\n",
    "- Complex, multi-faceted questions\n",
    "- High-stakes decisions\n",
    "- When initial results reveal gaps\n",
    "- Academic or technical research requiring thoroughness\n",
    "\n",
    "### Q5: How do you measure research agent quality?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Quantitative Metrics:**\n",
    "- Citation density (citations per 100 words)\n",
    "- Source diversity (number of unique source types)\n",
    "- Answer completeness (coverage of sub-queries)\n",
    "- Response time and token usage\n",
    "\n",
    "**Qualitative Metrics:**\n",
    "- Accuracy (claims supported by citations)\n",
    "- Clarity (organization and readability)\n",
    "- Depth (level of insight and analysis)\n",
    "- Balance (multiple perspectives presented)\n",
    "\n",
    "**Evaluation Method:**\n",
    "Use a separate LLM call to score each dimension, plus human evaluation for critical applications.\n",
    "\n",
    "### Q6: How does this compare to RAG?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Traditional RAG:**\n",
    "- Single query → retrieve documents → generate answer\n",
    "- Works on a fixed knowledge base\n",
    "- Fast, deterministic retrieval\n",
    "- Good for document Q&A\n",
    "\n",
    "**Research Agents:**\n",
    "- Complex query → decompose → multi-source search → reasoning → synthesis\n",
    "- Can access live web data and multiple sources\n",
    "- Iterative refinement based on quality\n",
    "- Good for comprehensive research and analysis\n",
    "\n",
    "**When to Use Each:**\n",
    "- RAG: Internal documentation, customer support, known knowledge base\n",
    "- Research Agents: Competitive intelligence, literature reviews, technical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "### Deep Reasoning Architectures\n",
    "1. **Chain of Thought** - Show step-by-step reasoning for transparency\n",
    "2. **Tree of Thoughts** - Explore multiple paths for complex problems\n",
    "3. **Graph of Thoughts** - Non-linear reasoning with thought aggregation\n",
    "\n",
    "### Research Agent Components\n",
    "1. **Query Decomposition** - Break complex questions into sub-queries\n",
    "2. **Multi-Source Search** - Gather information from diverse sources in parallel\n",
    "3. **Deep Reasoning** - Apply appropriate reasoning architecture\n",
    "4. **Synthesis** - Combine insights with proper citations\n",
    "5. **Quality Evaluation** - Assess completeness and accuracy\n",
    "6. **Iterative Refinement** - Fill gaps through targeted follow-ups\n",
    "\n",
    "### Production Best Practices\n",
    "1. **Cost Optimization** - Model selection, caching, parallel execution\n",
    "2. **Quality Thresholds** - Define when research is \"good enough\"\n",
    "3. **Monitoring** - Track metrics, errors, and performance\n",
    "4. **Citation Validation** - Prevent hallucinations through source verification\n",
    "5. **Context Management** - Handle large result sets effectively\n",
    "\n",
    "### When to Use Research Agents\n",
    "- Complex, open-ended questions\n",
    "- Multi-source information synthesis\n",
    "- High-stakes decisions requiring thoroughness\n",
    "- Academic or technical research\n",
    "- Competitive intelligence and market analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Explore Agentic RAG Patterns** - See how research agents integrate with retrieval systems\n",
    "2. **Study Framework Comparisons** - Understand which framework fits your use case\n",
    "3. **Build Production Systems** - Apply these patterns to real-world applications\n",
    "4. **Optimize Performance** - Implement caching, monitoring, and cost management\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Anthropic: Building Effective Agents](https://www.anthropic.com/research/building-effective-agents)\n",
    "- [Tree of Thoughts Paper](https://arxiv.org/abs/2305.10601)\n",
    "- [Graph of Thoughts Paper](https://arxiv.org/abs/2308.09687)\n",
    "- [LangChain Research Agents](https://python.langchain.com/docs/use_cases/research)\n",
    "- [Deep Research with AI](https://www.deepmind.com/research)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
