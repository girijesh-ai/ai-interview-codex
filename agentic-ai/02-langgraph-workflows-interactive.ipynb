{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph Agentic Workflows: Interactive Implementation\n",
        "\n",
        "**Framework:** LangGraph by LangChain  \n",
        "**Focus:** Graph-based agent orchestration and stateful workflows  \n",
        "**Format:** Interactive Jupyter Notebook with executable examples\n",
        "\n",
        "## Overview\n",
        "\n",
        "LangGraph enables building stateful, multi-actor applications with LLMs using graph-based workflows. This notebook demonstrates:\n",
        "\n",
        "1. **Core Concepts** - StateGraph, nodes, edges, conditional routing\n",
        "2. **Workflow Patterns** - All 6 patterns from Anthropic implemented in LangGraph\n",
        "3. **Multi-Agent Systems** - Supervisor-worker and collaborative patterns\n",
        "4. **Advanced Features** - Persistence, human-in-the-loop, memory\n",
        "5. **Production Patterns** - Error handling, monitoring, deployment\n",
        "\n",
        "## Why LangGraph?\n",
        "\n",
        "**Advantages over plain API calls:**\n",
        "- **State Management:** Built-in state tracking across workflow steps\n",
        "- **Graph-based Flow:** Cyclical workflows, conditional routing\n",
        "- **Persistence:** Save/resume workflows, human-in-the-loop\n",
        "- **Debugging:** Visual graph inspection, step-by-step execution\n",
        "- **Streaming:** Real-time output streaming\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langgraph langchain langchain-anthropic langchain-openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import TypedDict, Annotated, Literal\n",
        "from langgraph.graph import StateGraph, END, START\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from dotenv import load_dotenv\n",
        "import operator\n",
        "\n",
        "# Load API key\n",
        "load_dotenv()\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\", temperature=0.7)\n",
        "\n",
        "print(\"\u2713 LangGraph setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 1: LangGraph Fundamentals\n",
        "\n",
        "### Core Concepts\n",
        "\n",
        "**StateGraph:** Defines the workflow structure\n",
        "- **State:** Typed dictionary tracking workflow data\n",
        "- **Nodes:** Functions that process state\n",
        "- **Edges:** Connections between nodes (sequential, conditional, parallel)\n",
        "\n",
        "### Basic State Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LangGraph Architecture: Core Concepts\n\n### The Graph Structure\n\n```mermaid\ngraph LR\n    START([START]) --> N1[Node 1]\n    N1 --> N2[Node 2]\n    N2 --> D{Decision}\n    D -->|Condition A| N3[Node 3]\n    D -->|Condition B| N4[Node 4]\n    N3 --> END([END])\n    N4 --> END\n    \n    style START fill:#c8e6c9\n    style END fill:#c8e6c9\n    style N1 fill:#e1f5ff\n    style N2 fill:#e1f5ff\n    style D fill:#fff9c4\n    style N3 fill:#e1f5ff\n    style N4 fill:#e1f5ff\n```\n\n### State Flow\n\n```mermaid\nsequenceDiagram\n    participant App\n    participant Graph\n    participant N1 as Node 1\n    participant N2 as Node 2\n    participant N3 as Node 3\n\n    App->>Graph: invoke(initial_state)\n    Graph->>N1: state_v1\n    N1-->>Graph: updated_state_v2\n    Graph->>N2: state_v2\n    N2-->>Graph: updated_state_v3\n    Graph->>N3: state_v3\n    N3-->>Graph: final_state\n    Graph-->>App: return final_state\n    \n    Note over Graph,N3: State immutably<br/>transformed at each node\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangGraph vs Traditional Approaches\n\n#### Comparison Table\n\n| Aspect | Traditional Functions | LangGraph |\n|--------|----------------------|------------|\n| **State Management** | Manual variable passing | Built-in StateGraph |\n| **Conditional Logic** | if/else statements | Conditional edges |\n| **Loops** | while loops, recursion | Cycles in graph |\n| **Persistence** | Manual save/load | Checkpointer |\n| **Visualization** | None | Graph rendering |\n| **Debugging** | print(), logging | Step-by-step execution |\n| **Human-in-Loop** | Complex implementation | Built-in interrupt |\n| **Scalability** | Hard to maintain | Declarative structure |\n\n#### Example: Conditional Workflow\n\n**Traditional Approach:**\n```python\ndef process(data):\n    result = step1(data)\n    category = classify(result)\n    \n    if category == 'A':\n        output = handle_a(result)\n    elif category == 'B':\n        output = handle_b(result)\n    else:\n        output = handle_default(result)\n    \n    return finalize(output)\n```\n\n**LangGraph Approach:**\n```python\nworkflow = StateGraph(State)\nworkflow.add_node('step1', step1_fn)\nworkflow.add_node('classify', classify_fn)\nworkflow.add_node('handle_a', handle_a_fn)\nworkflow.add_node('handle_b', handle_b_fn)\nworkflow.add_node('handle_default', default_fn)\nworkflow.add_node('finalize', finalize_fn)\n\nworkflow.add_edge(START, 'step1')\nworkflow.add_edge('step1', 'classify')\nworkflow.add_conditional_edges('classify', router_fn, {\n    'A': 'handle_a',\n    'B': 'handle_b',\n    'default': 'handle_default'\n})\nworkflow.add_edge('handle_a', 'finalize')\nworkflow.add_edge('handle_b', 'finalize')\nworkflow.add_edge('handle_default', 'finalize')\nworkflow.add_edge('finalize', END)\n\napp = workflow.compile()\n```\n\n**Benefits of LangGraph:**\n- \u2705 Declarative and visual\n- \u2705 Easy to modify routing\n- \u2705 Can add persistence\n- \u2705 Supports streaming\n- \u2705 Built-in state management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define workflow state\n",
        "class BasicState(TypedDict):\n",
        "    \"\"\"\n",
        "    State tracks data flowing through the workflow\n",
        "    \"\"\"\n",
        "    input: str\n",
        "    output: str\n",
        "    step_count: int\n",
        "\n",
        "# Simple node function\n",
        "def process_node(state: BasicState) -> BasicState:\n",
        "    \"\"\"Node function: receives state, returns updated state\"\"\"\n",
        "    print(f\"Processing: {state['input']}\")\n",
        "    return {\n",
        "        **state,\n",
        "        \"output\": f\"Processed: {state['input']}\",\n",
        "        \"step_count\": state.get(\"step_count\", 0) + 1\n",
        "    }\n",
        "\n",
        "# Create simple graph\n",
        "def create_simple_graph():\n",
        "    workflow = StateGraph(BasicState)\n",
        "    \n",
        "    # Add node\n",
        "    workflow.add_node(\"process\", process_node)\n",
        "    \n",
        "    # Define edges\n",
        "    workflow.add_edge(START, \"process\")\n",
        "    workflow.add_edge(\"process\", END)\n",
        "    \n",
        "    # Compile\n",
        "    return workflow.compile()\n",
        "\n",
        "# Test simple graph\n",
        "simple_app = create_simple_graph()\n",
        "\n",
        "result = simple_app.invoke({\n",
        "    \"input\": \"Hello LangGraph\",\n",
        "    \"step_count\": 0\n",
        "})\n",
        "\n",
        "print(f\"\\nResult: {result['output']}\")\n",
        "print(f\"Steps: {result['step_count']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2: Pattern 1 - Prompt Chaining with LangGraph\n",
        "\n",
        "Sequential processing where each step builds on the previous output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# State for prompt chaining\n",
        "class ChainState(TypedDict):\n",
        "    product_description: str\n",
        "    key_features: str\n",
        "    marketing_copy: str\n",
        "    translation: str\n",
        "    current_step: str\n",
        "\n",
        "# Step 1: Extract features\n",
        "def extract_features(state: ChainState) -> ChainState:\n",
        "    print(\"[Step 1] Extracting features...\")\n",
        "    \n",
        "    messages = [HumanMessage(content=f\"\"\"Extract 3-5 key features from this product:\n",
        "    \n",
        "{state['product_description']}\n",
        "\n",
        "List as bullet points.\"\"\")]\n",
        "    \n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"key_features\": response.content,\n",
        "        \"current_step\": \"extract\"\n",
        "    }\n",
        "\n",
        "# Step 2: Generate marketing copy\n",
        "def generate_copy(state: ChainState) -> ChainState:\n",
        "    print(\"[Step 2] Generating marketing copy...\")\n",
        "    \n",
        "    messages = [HumanMessage(content=f\"\"\"Create compelling marketing copy from these features:\n",
        "    \n",
        "{state['key_features']}\n",
        "\n",
        "Write 2-3 engaging paragraphs.\"\"\")]\n",
        "    \n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"marketing_copy\": response.content,\n",
        "        \"current_step\": \"generate\"\n",
        "    }\n",
        "\n",
        "# Step 3: Translate\n",
        "def translate_copy(state: ChainState) -> ChainState:\n",
        "    print(\"[Step 3] Translating to Spanish...\")\n",
        "    \n",
        "    messages = [HumanMessage(content=f\"\"\"Translate this marketing copy to Spanish:\n",
        "    \n",
        "{state['marketing_copy']}\n",
        "\n",
        "Maintain tone and style.\"\"\")]\n",
        "    \n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"translation\": response.content,\n",
        "        \"current_step\": \"translate\"\n",
        "    }\n",
        "\n",
        "# Build prompt chaining graph\n",
        "def create_chain_graph():\n",
        "    workflow = StateGraph(ChainState)\n",
        "    \n",
        "    # Add nodes in sequence\n",
        "    workflow.add_node(\"extract\", extract_features)\n",
        "    workflow.add_node(\"generate\", generate_copy)\n",
        "    workflow.add_node(\"translate\", translate_copy)\n",
        "    \n",
        "    # Sequential edges\n",
        "    workflow.add_edge(START, \"extract\")\n",
        "    workflow.add_edge(\"extract\", \"generate\")\n",
        "    workflow.add_edge(\"generate\", \"translate\")\n",
        "    workflow.add_edge(\"translate\", END)\n",
        "    \n",
        "    return workflow.compile()\n",
        "\n",
        "# Test prompt chaining\n",
        "print(\"=\" * 60)\n",
        "print(\"PROMPT CHAINING WITH LANGGRAPH\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "chain_app = create_chain_graph()\n",
        "\n",
        "result = chain_app.invoke({\n",
        "    \"product_description\": \"\"\"SmartWatch Pro X: Advanced fitness tracker with 7-day battery,\n",
        "    heart rate monitoring, GPS, water-resistant to 50m, smartphone integration.\"\"\",\n",
        "    \"key_features\": \"\",\n",
        "    \"marketing_copy\": \"\",\n",
        "    \"translation\": \"\",\n",
        "    \"current_step\": \"\"\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n\ud83d\udcdd Features:\\n{result['key_features']}\")\n",
        "print(f\"\\n\ud83d\udce2 Marketing Copy:\\n{result['marketing_copy']}\")\n",
        "print(f\"\\n\ud83c\udf10 Spanish Translation:\\n{result['translation']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3: Pattern 2 - Routing with Conditional Edges\n",
        "\n",
        "Dynamic routing based on classification. This is where LangGraph shines!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# State for routing\n",
        "class RoutingState(TypedDict):\n",
        "    query: str\n",
        "    category: str\n",
        "    response: str\n",
        "\n",
        "# Router node\n",
        "def router(state: RoutingState) -> RoutingState:\n",
        "    \"\"\"Classify query into category\"\"\"\n",
        "    print(f\"[Router] Classifying: {state['query']}\")\n",
        "    \n",
        "    messages = [HumanMessage(content=f\"\"\"Classify this query into ONE category:\n",
        "    \n",
        "Query: {state['query']}\n",
        "\n",
        "Categories:\n",
        "- technical: Product issues, bugs, setup\n",
        "- billing: Payment, invoices, subscriptions\n",
        "- general: General questions, info\n",
        "\n",
        "Respond with ONLY the category name.\"\"\")]\n",
        "    \n",
        "    response = llm.invoke(messages)\n",
        "    category = response.content.strip().lower()\n",
        "    \n",
        "    print(f\"[Router] Category: {category}\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"category\": category\n",
        "    }\n",
        "\n",
        "# Conditional edge function\n",
        "def route_query(state: RoutingState) -> Literal[\"technical\", \"billing\", \"general\"]:\n",
        "    \"\"\"Determine next node based on category\"\"\"\n",
        "    category = state[\"category\"]\n",
        "    \n",
        "    # Map to valid node names\n",
        "    if \"technical\" in category:\n",
        "        return \"technical\"\n",
        "    elif \"billing\" in category:\n",
        "        return \"billing\"\n",
        "    else:\n",
        "        return \"general\"\n",
        "\n",
        "# Handler nodes\n",
        "def handle_technical(state: RoutingState) -> RoutingState:\n",
        "    print(\"[Technical Handler] Processing...\")\n",
        "    \n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a technical support specialist. Provide detailed troubleshooting.\"),\n",
        "        HumanMessage(content=state['query'])\n",
        "    ]\n",
        "    \n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"response\": response.content\n",
        "    }\n",
        "\n",
        "def handle_billing(state: RoutingState) -> RoutingState:\n",
        "    print(\"[Billing Handler] Processing...\")\n",
        "    \n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a billing specialist. Explain charges clearly.\"),\n",
        "        HumanMessage(content=state['query'])\n",
        "    ]\n",
        "    \n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"response\": response.content\n",
        "    }\n",
        "\n",
        "def handle_general(state: RoutingState) -> RoutingState:\n",
        "    print(\"[General Handler] Processing...\")\n",
        "    \n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a helpful customer service rep. Be friendly and concise.\"),\n",
        "        HumanMessage(content=state['query'])\n",
        "    ]\n",
        "    \n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"response\": response.content\n",
        "    }\n",
        "\n",
        "# Build routing graph\n",
        "def create_routing_graph():\n",
        "    workflow = StateGraph(RoutingState)\n",
        "    \n",
        "    # Add nodes\n",
        "    workflow.add_node(\"router\", router)\n",
        "    workflow.add_node(\"technical\", handle_technical)\n",
        "    workflow.add_node(\"billing\", handle_billing)\n",
        "    workflow.add_node(\"general\", handle_general)\n",
        "    \n",
        "    # Start -> router\n",
        "    workflow.add_edge(START, \"router\")\n",
        "    \n",
        "    # Conditional routing from router\n",
        "    workflow.add_conditional_edges(\n",
        "        \"router\",\n",
        "        route_query,\n",
        "        {\n",
        "            \"technical\": \"technical\",\n",
        "            \"billing\": \"billing\",\n",
        "            \"general\": \"general\"\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # All handlers -> END\n",
        "    workflow.add_edge(\"technical\", END)\n",
        "    workflow.add_edge(\"billing\", END)\n",
        "    workflow.add_edge(\"general\", END)\n",
        "    \n",
        "    return workflow.compile()\n",
        "\n",
        "# Test routing\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ROUTING WITH CONDITIONAL EDGES\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "routing_app = create_routing_graph()\n",
        "\n",
        "test_queries = [\n",
        "    \"My app keeps crashing when I export data\",\n",
        "    \"I was charged twice this month\",\n",
        "    \"What are your business hours?\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    result = routing_app.invoke({\n",
        "        \"query\": query,\n",
        "        \"category\": \"\",\n",
        "        \"response\": \"\"\n",
        "    })\n",
        "    \n",
        "    print(f\"\\nResponse:\\n{result['response'][:300]}...\\n\")\n",
        "    print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4: Pattern 3 - Orchestrator-Workers with Dynamic Dispatch\n",
        "\n",
        "Using LangGraph's `Send` API for dynamic worker creation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.constants import Send\n",
        "import json\n",
        "\n",
        "# State definitions\n",
        "class OrchestratorState(TypedDict):\n",
        "    task: str\n",
        "    subtasks: list\n",
        "    results: Annotated[list, operator.add]  # Accumulate results\n",
        "    final_output: str\n",
        "\n",
        "class WorkerState(TypedDict):\n",
        "    subtask: dict\n",
        "    result: str\n",
        "\n",
        "# Orchestrator: Plan subtasks\n",
        "def orchestrator_plan(state: OrchestratorState) -> OrchestratorState:\n",
        "    print(\"[Orchestrator] Planning subtasks...\")\n",
        "    \n",
        "    messages = [HumanMessage(content=f\"\"\"Break down this task into 3-4 independent subtasks:\n",
        "    \n",
        "Task: {state['task']}\n",
        "\n",
        "For each subtask provide:\n",
        "- id (number)\n",
        "- description\n",
        "- expertise_needed\n",
        "\n",
        "Format as JSON array.\"\"\")]\n",
        "    \n",
        "    response = llm.invoke(messages)\n",
        "    content = response.content\n",
        "    \n",
        "    # Extract JSON\n",
        "    if \"```json\" in content:\n",
        "        content = content.split(\"```json\")[1].split(\"```\")[0]\n",
        "    elif \"```\" in content:\n",
        "        content = content.split(\"```\")[1].split(\"```\")[0]\n",
        "    \n",
        "    subtasks = json.loads(content.strip())\n",
        "    \n",
        "    print(f\"[Orchestrator] Created {len(subtasks)} subtasks\")\n",
        "    for st in subtasks:\n",
        "        print(f\"  {st['id']}. {st['description']}\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"subtasks\": subtasks,\n",
        "        \"results\": []\n",
        "    }\n",
        "\n",
        "# Dynamic dispatch to workers\n",
        "def dispatch_workers(state: OrchestratorState):\n",
        "    \"\"\"Send each subtask to a worker\"\"\"\n",
        "    return [Send(\"worker\", {\"subtask\": st}) for st in state[\"subtasks\"]]\n",
        "\n",
        "# Worker: Execute subtask\n",
        "def worker_execute(state: WorkerState) -> dict:\n",
        "    subtask = state[\"subtask\"]\n",
        "    print(f\"[Worker {subtask['id']}] Executing...\")\n",
        "    \n",
        "    messages = [HumanMessage(content=f\"\"\"You are a specialist in: {subtask['expertise_needed']}\n",
        "    \n",
        "Execute this subtask:\n",
        "{subtask['description']}\n",
        "\n",
        "Provide detailed, high-quality work.\"\"\")]\n",
        "    \n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        \"results\": [{\n",
        "            \"id\": subtask['id'],\n",
        "            \"description\": subtask['description'],\n",
        "            \"result\": response.content\n",
        "        }]\n",
        "    }\n",
        "\n",
        "# Synthesizer: Combine results\n",
        "def synthesize_results(state: OrchestratorState) -> OrchestratorState:\n",
        "    print(\"[Orchestrator] Synthesizing results...\")\n",
        "    \n",
        "    results_text = \"\\n\\n\".join([\n",
        "        f\"Subtask {r['id']}: {r['description']}\\nResult:\\n{r['result']}\"\n",
        "        for r in state['results']\n",
        "    ])\n",
        "    \n",
        "    messages = [HumanMessage(content=f\"\"\"Combine these results into a cohesive response:\n",
        "    \n",
        "Original Task: {state['task']}\n",
        "\n",
        "Results:\n",
        "{results_text}\n",
        "\n",
        "Create a unified, well-structured response.\"\"\")]\n",
        "    \n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"final_output\": response.content\n",
        "    }\n",
        "\n",
        "# Build orchestrator-worker graph\n",
        "def create_orchestrator_graph():\n",
        "    workflow = StateGraph(OrchestratorState)\n",
        "    \n",
        "    # Add nodes\n",
        "    workflow.add_node(\"plan\", orchestrator_plan)\n",
        "    workflow.add_node(\"worker\", worker_execute)\n",
        "    workflow.add_node(\"synthesize\", synthesize_results)\n",
        "    \n",
        "    # Edges\n",
        "    workflow.add_edge(START, \"plan\")\n",
        "    \n",
        "    # Dynamic dispatch to workers\n",
        "    workflow.add_conditional_edges(\"plan\", dispatch_workers, [\"worker\"])\n",
        "    \n",
        "    # Workers -> synthesize\n",
        "    workflow.add_edge(\"worker\", \"synthesize\")\n",
        "    workflow.add_edge(\"synthesize\", END)\n",
        "    \n",
        "    return workflow.compile()\n",
        "\n",
        "# Test orchestrator-worker\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ORCHESTRATOR-WORKERS WITH DYNAMIC DISPATCH\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "orchestrator_app = create_orchestrator_graph()\n",
        "\n",
        "result = orchestrator_app.invoke({\n",
        "    \"task\": \"\"\"Create a comprehensive marketing strategy for a new AI productivity app \n",
        "    targeting remote teams. Include competitive analysis, pricing, and channels.\"\"\",\n",
        "    \"subtasks\": [],\n",
        "    \"results\": [],\n",
        "    \"final_output\": \"\"\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL OUTPUT\")\n",
        "print(\"=\" * 60)\n",
        "print(result['final_output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 5: Multi-Agent Collaboration (Supervisor Pattern)\n",
        "\n",
        "Multiple specialized agents coordinated by a supervisor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-agent state\n",
        "class MultiAgentState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    next_agent: str\n",
        "    final_report: str\n",
        "\n",
        "# Agent nodes\n",
        "def researcher_agent(state: MultiAgentState) -> MultiAgentState:\n",
        "    print(\"[Researcher] Gathering information...\")\n",
        "    \n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a research specialist. Gather and organize information.\"),\n",
        "        *state['messages']\n",
        "    ]\n",
        "    \n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        \"messages\": [AIMessage(content=f\"[Researcher] {response.content}\", name=\"researcher\")]\n",
        "    }\n",
        "\n",
        "def writer_agent(state: MultiAgentState) -> MultiAgentState:\n",
        "    print(\"[Writer] Drafting content...\")\n",
        "    \n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a professional writer. Create clear, engaging content.\"),\n",
        "        *state['messages']\n",
        "    ]\n",
        "    \n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        \"messages\": [AIMessage(content=f\"[Writer] {response.content}\", name=\"writer\")]\n",
        "    }\n",
        "\n",
        "def critic_agent(state: MultiAgentState) -> MultiAgentState:\n",
        "    print(\"[Critic] Reviewing quality...\")\n",
        "    \n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a critical editor. Provide constructive feedback.\"),\n",
        "        *state['messages']\n",
        "    ]\n",
        "    \n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        \"messages\": [AIMessage(content=f\"[Critic] {response.content}\", name=\"critic\")]\n",
        "    }\n",
        "\n",
        "# Supervisor decides next agent\n",
        "def supervisor(state: MultiAgentState) -> MultiAgentState:\n",
        "    print(\"[Supervisor] Deciding next step...\")\n",
        "    \n",
        "    messages = [\n",
        "        SystemMessage(content=\"\"\"You are a supervisor coordinating agents: researcher, writer, critic.\n",
        "        \n",
        "Based on the conversation, decide who should act next.\n",
        "Respond with ONLY: researcher, writer, critic, or FINISH\n",
        "\n",
        "Typical flow: researcher -> writer -> critic -> FINISH\"\"\"),\n",
        "        *state['messages']\n",
        "    ]\n",
        "    \n",
        "    response = llm.invoke(messages)\n",
        "    next_agent = response.content.strip().lower()\n",
        "    \n",
        "    print(f\"[Supervisor] Next: {next_agent}\")\n",
        "    \n",
        "    return {\n",
        "        \"next_agent\": next_agent\n",
        "    }\n",
        "\n",
        "def route_supervisor(state: MultiAgentState) -> str:\n",
        "    \"\"\"Route based on supervisor decision\"\"\"\n",
        "    next_agent = state.get(\"next_agent\", \"\").lower()\n",
        "    \n",
        "    if \"researcher\" in next_agent:\n",
        "        return \"researcher\"\n",
        "    elif \"writer\" in next_agent:\n",
        "        return \"writer\"\n",
        "    elif \"critic\" in next_agent:\n",
        "        return \"critic\"\n",
        "    else:\n",
        "        return \"finish\"\n",
        "\n",
        "def finalize(state: MultiAgentState) -> MultiAgentState:\n",
        "    print(\"[Finalizing] Compilation complete\")\n",
        "    \n",
        "    # Extract final content from messages\n",
        "    final = \"\\n\\n\".join([m.content for m in state['messages'] if isinstance(m, AIMessage)])\n",
        "    \n",
        "    return {\n",
        "        \"final_report\": final\n",
        "    }\n",
        "\n",
        "# Build multi-agent graph\n",
        "def create_multiagent_graph():\n",
        "    workflow = StateGraph(MultiAgentState)\n",
        "    \n",
        "    # Add agents\n",
        "    workflow.add_node(\"supervisor\", supervisor)\n",
        "    workflow.add_node(\"researcher\", researcher_agent)\n",
        "    workflow.add_node(\"writer\", writer_agent)\n",
        "    workflow.add_node(\"critic\", critic_agent)\n",
        "    workflow.add_node(\"finish\", finalize)\n",
        "    \n",
        "    # Start -> supervisor\n",
        "    workflow.add_edge(START, \"supervisor\")\n",
        "    \n",
        "    # Supervisor routes to agents\n",
        "    workflow.add_conditional_edges(\n",
        "        \"supervisor\",\n",
        "        route_supervisor,\n",
        "        {\n",
        "            \"researcher\": \"researcher\",\n",
        "            \"writer\": \"writer\",\n",
        "            \"critic\": \"critic\",\n",
        "            \"finish\": \"finish\"\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Agents back to supervisor\n",
        "    workflow.add_edge(\"researcher\", \"supervisor\")\n",
        "    workflow.add_edge(\"writer\", \"supervisor\")\n",
        "    workflow.add_edge(\"critic\", \"supervisor\")\n",
        "    \n",
        "    # Finish -> END\n",
        "    workflow.add_edge(\"finish\", END)\n",
        "    \n",
        "    return workflow.compile()\n",
        "\n",
        "# Test multi-agent system\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MULTI-AGENT COLLABORATION (Supervisor Pattern)\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "multiagent_app = create_multiagent_graph()\n",
        "\n",
        "result = multiagent_app.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"\"\"Write a 300-word article about the benefits \n",
        "    of AI agents in enterprise software development.\"\"\")],\n",
        "    \"next_agent\": \"\",\n",
        "    \"final_report\": \"\"\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL REPORT\")\n",
        "print(\"=\" * 60)\n",
        "print(result['final_report'][:1000] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 6: Human-in-the-Loop & Persistence\n",
        "\n",
        "One of LangGraph's most powerful features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# State with approval tracking\n",
        "class ApprovalState(TypedDict):\n",
        "    content: str\n",
        "    needs_approval: bool\n",
        "    approved: bool\n",
        "    feedback: str\n",
        "\n",
        "def generate_content(state: ApprovalState) -> ApprovalState:\n",
        "    print(\"[Generator] Creating content...\")\n",
        "    \n",
        "    messages = [HumanMessage(content=\"Write a professional email announcing a product delay.\")]\n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"content\": response.content,\n",
        "        \"needs_approval\": True\n",
        "    }\n",
        "\n",
        "def approval_gate(state: ApprovalState) -> ApprovalState:\n",
        "    \"\"\"Pause here for human approval\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"APPROVAL REQUIRED\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nContent:\\n{state['content']}\\n\")\n",
        "    print(\"Waiting for approval...\")\n",
        "    print(\"(In production, this would interrupt and wait for human input)\")\n",
        "    \n",
        "    # In real implementation, this would pause execution\n",
        "    # User would resume with: app.invoke(None, config={\"configurable\": {\"thread_id\": \"1\"}})\n",
        "    \n",
        "    return state\n",
        "\n",
        "def check_approval(state: ApprovalState) -> str:\n",
        "    \"\"\"Route based on approval status\"\"\"\n",
        "    if state.get(\"approved\", False):\n",
        "        return \"send\"\n",
        "    else:\n",
        "        # In real implementation, would check user input\n",
        "        # For demo, auto-approve\n",
        "        return \"send\"\n",
        "\n",
        "def send_email(state: ApprovalState) -> ApprovalState:\n",
        "    print(\"\\n[Sender] Email sent!\")\n",
        "    return state\n",
        "\n",
        "# Build graph with checkpoints\n",
        "def create_approval_graph():\n",
        "    # Memory saver enables persistence\n",
        "    memory = MemorySaver()\n",
        "    \n",
        "    workflow = StateGraph(ApprovalState)\n",
        "    \n",
        "    workflow.add_node(\"generate\", generate_content)\n",
        "    workflow.add_node(\"approve\", approval_gate)\n",
        "    workflow.add_node(\"send\", send_email)\n",
        "    \n",
        "    workflow.add_edge(START, \"generate\")\n",
        "    workflow.add_edge(\"generate\", \"approve\")\n",
        "    \n",
        "    # Conditional: approved -> send, else -> regenerate\n",
        "    workflow.add_conditional_edges(\n",
        "        \"approve\",\n",
        "        check_approval,\n",
        "        {\"send\": \"send\"}\n",
        "    )\n",
        "    \n",
        "    workflow.add_edge(\"send\", END)\n",
        "    \n",
        "    # Compile with checkpointer\n",
        "    return workflow.compile(checkpointer=memory)\n",
        "\n",
        "# Test with persistence\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"HUMAN-IN-THE-LOOP WITH PERSISTENCE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "approval_app = create_approval_graph()\n",
        "\n",
        "# Execute with thread ID for persistence\n",
        "config = {\"configurable\": {\"thread_id\": \"demo-1\"}}\n",
        "\n",
        "result = approval_app.invoke({\n",
        "    \"content\": \"\",\n",
        "    \"needs_approval\": False,\n",
        "    \"approved\": True,  # Auto-approve for demo\n",
        "    \"feedback\": \"\"\n",
        "}, config=config)\n",
        "\n",
        "print(\"\\nWorkflow completed with persistence support!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 7: Streaming & Real-time Output\n",
        "\n",
        "Stream outputs as they're generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test streaming\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STREAMING WORKFLOW EXECUTION\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "# Use the chain graph from earlier\n",
        "chain_app = create_chain_graph()\n",
        "\n",
        "# Stream execution\n",
        "for event in chain_app.stream({\n",
        "    \"product_description\": \"AI-powered noise-cancelling headphones with 40-hour battery\",\n",
        "    \"key_features\": \"\",\n",
        "    \"marketing_copy\": \"\",\n",
        "    \"translation\": \"\",\n",
        "    \"current_step\": \"\"\n",
        "}):\n",
        "    # Each event shows the node that executed and its output\n",
        "    for node_name, node_output in event.items():\n",
        "        print(f\"\\n\ud83d\udccd Node: {node_name}\")\n",
        "        if \"current_step\" in node_output:\n",
        "            print(f\"   Step: {node_output['current_step']}\")\n",
        "        print(\"   Output updated\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "print(\"\\n\u2713 Streaming complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: LangGraph vs Plain API Calls\n",
        "\n",
        "### Advantages of LangGraph\n",
        "\n",
        "| Feature | Plain API | LangGraph |\n",
        "|---------|-----------|----------|\n",
        "| **State Management** | Manual dict passing | Built-in StateGraph |\n",
        "| **Conditional Flow** | If/else logic | Conditional edges |\n",
        "| **Cycles/Loops** | Complex code | Native support |\n",
        "| **Persistence** | Custom implementation | Built-in checkpointer |\n",
        "| **Human-in-Loop** | Difficult | Native interrupt support |\n",
        "| **Visualization** | None | Graph rendering |\n",
        "| **Streaming** | Manual | Built-in streaming |\n",
        "| **Debugging** | Print statements | Step-by-step inspection |\n",
        "\n",
        "### When to Use LangGraph\n",
        "\n",
        "**Use LangGraph when:**\n",
        "- Complex workflows with conditional routing\n",
        "- Need to support cycles/loops\n",
        "- Human-in-the-loop required\n",
        "- Long-running workflows needing persistence\n",
        "- Multiple agents coordinating\n",
        "\n",
        "**Use Plain API calls when:**\n",
        "- Simple linear workflows\n",
        "- Minimal state tracking\n",
        "- No conditional logic\n",
        "- Quick prototypes\n",
        "\n",
        "### Key Concepts Recap\n",
        "\n",
        "1. **StateGraph:** Workflow definition with typed state\n",
        "2. **Nodes:** Functions that process and update state\n",
        "3. **Edges:** Connections (sequential, conditional, dynamic)\n",
        "4. **Checkpointer:** Persistence layer for interrupting/resuming\n",
        "5. **Send API:** Dynamic worker dispatch\n",
        "6. **Streaming:** Real-time output as nodes execute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Production Best Practices\n",
        "\n",
        "### 1. Error Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Robust node with error handling\n",
        "def robust_node(state: dict) -> dict:\n",
        "    try:\n",
        "        # Your node logic\n",
        "        response = llm.invoke([HumanMessage(content=state['input'])])\n",
        "        \n",
        "        return {\n",
        "            **state,\n",
        "            \"output\": response.content,\n",
        "            \"error\": None\n",
        "        }\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error in node: {e}\")\n",
        "        \n",
        "        return {\n",
        "            **state,\n",
        "            \"output\": \"\",\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# Conditional routing based on errors\n",
        "def check_for_errors(state: dict) -> str:\n",
        "    if state.get(\"error\"):\n",
        "        return \"error_handler\"\n",
        "    else:\n",
        "        return \"next_step\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Monitoring & Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import logging\n",
        "\n",
        "class MonitoredNode:\n",
        "    \"\"\"Wrapper for monitoring node execution\"\"\"\n",
        "    \n",
        "    def __init__(self, node_func, node_name: str):\n",
        "        self.node_func = node_func\n",
        "        self.node_name = node_name\n",
        "        self.metrics = []\n",
        "    \n",
        "    def __call__(self, state):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            result = self.node_func(state)\n",
        "            latency = time.time() - start_time\n",
        "            \n",
        "            self.metrics.append({\n",
        "                \"node\": self.node_name,\n",
        "                \"latency\": latency,\n",
        "                \"status\": \"success\"\n",
        "            })\n",
        "            \n",
        "            logging.info(f\"{self.node_name} completed in {latency:.2f}s\")\n",
        "            \n",
        "            return result\n",
        "            \n",
        "        except Exception as e:\n",
        "            latency = time.time() - start_time\n",
        "            \n",
        "            self.metrics.append({\n",
        "                \"node\": self.node_name,\n",
        "                \"latency\": latency,\n",
        "                \"status\": \"error\",\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "            \n",
        "            logging.error(f\"{self.node_name} failed after {latency:.2f}s: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def get_metrics(self):\n",
        "        return self.metrics\n",
        "\n",
        "# Usage\n",
        "# monitored_extract = MonitoredNode(extract_features, \"extract\")\n",
        "# workflow.add_node(\"extract\", monitored_extract)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Interview Preparation\n",
        "\n",
        "### Common Questions\n",
        "\n",
        "**Q1: Why use LangGraph instead of plain API calls?**\n",
        "\n",
        "**Answer:**\n",
        "- **State management:** Built-in state tracking vs manual dict passing\n",
        "- **Conditional routing:** Native conditional edges vs complex if/else\n",
        "- **Cycles:** Supports loops natively (e.g., evaluator-optimizer)\n",
        "- **Persistence:** Built-in checkpointing for long-running workflows\n",
        "- **Human-in-loop:** Native interrupt support\n",
        "- **Visualization:** Can render workflow graphs\n",
        "- **Debugging:** Step-by-step execution inspection\n",
        "\n",
        "**Q2: Explain StateGraph vs regular function composition?**\n",
        "\n",
        "**Answer:**\n",
        "- **StateGraph:** Declarative workflow definition with typed state, nodes, and edges. Supports conditional routing and cycles.\n",
        "- **Function composition:** Imperative, linear flow. Complex logic becomes hard to maintain.\n",
        "- **Example:** Multi-agent system with dynamic routing is simple in StateGraph but requires complex orchestration code otherwise.\n",
        "\n",
        "**Q3: How do you handle errors in LangGraph workflows?**\n",
        "\n",
        "**Answer:**\n",
        "1. **Try-catch in nodes:** Catch exceptions, return error state\n",
        "2. **Conditional routing:** Route to error handler based on error flag\n",
        "3. **Retry logic:** Add retry nodes for transient failures\n",
        "4. **Fallback paths:** Define alternative routes when primary fails\n",
        "5. **Monitoring:** Wrap nodes with monitoring for tracking\n",
        "\n",
        "**Q4: When would you use Send API for dynamic workers?**\n",
        "\n",
        "**Answer:**\n",
        "- **Orchestrator-worker pattern:** Dynamic number of subtasks\n",
        "- **Parallel processing:** Process list items independently\n",
        "- **Fan-out/fan-in:** Dispatch to multiple workers, aggregate results\n",
        "- **Example:** Code review where number of files unknown upfront\n",
        "\n",
        "**Q5: How do you implement human-in-the-loop?**\n",
        "\n",
        "**Answer:**\n",
        "1. Use **checkpointer** (MemorySaver or database)\n",
        "2. Add **approval gate node** that requires human input\n",
        "3. Execute with **thread_id** for persistence\n",
        "4. Workflow **pauses** at gate\n",
        "5. Human reviews, provides input\n",
        "6. **Resume** workflow with updated state\n",
        "7. Conditional routing based on approval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "This notebook covered LangGraph fundamentals and all major patterns. Next notebooks:\n",
        "\n",
        "1. **LangChain Agents** - ReAct, tool calling, OpenAI functions\n",
        "2. **Multi-Agent Systems** - CrewAI, AutoGen deep dive\n",
        "3. **Research Agents** - Deep reasoning, synthesis, citation\n",
        "4. **Agentic RAG** - Agent-enhanced retrieval patterns\n",
        "5. **Framework Comparison** - When to use what\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
        "- [LangGraph Tutorials](https://langchain-ai.github.io/langgraph/tutorials/)\n",
        "- [LangGraph Multi-Agent](https://blog.langchain.com/langgraph-multi-agent-workflows/)\n",
        "- [LangGraph GitHub](https://github.com/langchain-ai/langgraph)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}