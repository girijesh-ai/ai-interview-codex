{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG Patterns\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) has become the standard approach for grounding LLM responses in specific knowledge bases. However, **traditional RAG** has limitations when dealing with complex queries, multiple data sources, or scenarios requiring reasoning over retrieved information.\n",
    "\n",
    "**Agentic RAG** extends traditional RAG by giving the LLM agency to:\n",
    "- Plan and decompose complex queries\n",
    "- Route queries to appropriate data sources\n",
    "- Adaptively retrieve additional information\n",
    "- Reason over retrieved documents\n",
    "- Verify and validate generated answers\n",
    "\n",
    "### Traditional RAG vs. Agentic RAG\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph \"Traditional RAG (Static)\"\n",
    "        T1[User Query] --> T2[Embed Query]\n",
    "        T2 --> T3[Retrieve Top K Docs]\n",
    "        T3 --> T4[Single LLM Call]\n",
    "        T4 --> T5[Generate Answer]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Agentic RAG (Dynamic)\"\n",
    "        A1[User Query] --> A2{Query Analyzer}\n",
    "        A2 --> A3[Query Planning]\n",
    "        A3 --> A4{Router}\n",
    "        A4 -->|Source 1| A5[Retrieve]\n",
    "        A4 -->|Source 2| A6[Retrieve]\n",
    "        A4 -->|Source 3| A7[Retrieve]\n",
    "        A5 --> A8{Sufficient?}\n",
    "        A6 --> A8\n",
    "        A7 --> A8\n",
    "        A8 -->|No| A9[Adaptive Retrieval]\n",
    "        A9 --> A8\n",
    "        A8 -->|Yes| A10[Reasoning]\n",
    "        A10 --> A11[Generate & Verify]\n",
    "        A11 --> A12[Final Answer]\n",
    "    end\n",
    "    \n",
    "    style T1 fill:#e1f5ff\n",
    "    style T5 fill:#ffccbc\n",
    "    style A1 fill:#e1f5ff\n",
    "    style A12 fill:#c8e6c9\n",
    "```\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Aspect | Traditional RAG | Agentic RAG |\n",
    "|--------|----------------|-------------|\n",
    "| **Query Processing** | Single embedding | Query decomposition & planning |\n",
    "| **Retrieval** | Fixed top-k | Adaptive, iterative |\n",
    "| **Data Sources** | Single vector store | Multi-source routing |\n",
    "| **Reasoning** | Direct generation | Multi-step reasoning over docs |\n",
    "| **Verification** | None | Self-verification & validation |\n",
    "| **Complexity** | Low | High |\n",
    "| **Cost** | Low | Higher |\n",
    "| **Use Cases** | Simple Q&A | Complex analysis, research |\n",
    "\n",
    "### When to Use Agentic RAG\n",
    "\n",
    "- **Complex queries** requiring multi-step reasoning\n",
    "- **Multiple data sources** (databases, APIs, documents, graphs)\n",
    "- **High-stakes applications** requiring verification\n",
    "- **Research tasks** needing comprehensive synthesis\n",
    "- **Dynamic scenarios** where query complexity varies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agentic RAG Architecture Patterns\n",
    "\n",
    "### 1.1 Query Planning Agent\n",
    "\n",
    "The Query Planning Agent analyzes incoming queries and creates a retrieval strategy.\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant U as User\n",
    "    participant QP as Query Planner\n",
    "    participant R as Retriever\n",
    "    participant G as Generator\n",
    "    \n",
    "    U->>QP: \"What were Q3 revenue and why did it increase?\"\n",
    "    QP->>QP: Analyze Query Type\n",
    "    QP->>QP: Identify: Multi-part (fact + reasoning)\n",
    "    QP->>QP: Create Plan\n",
    "    Note over QP: Plan:<br/>1. Retrieve Q3 revenue data<br/>2. Retrieve Q3 performance analysis<br/>3. Synthesize answer\n",
    "    \n",
    "    QP->>R: Execute Step 1\n",
    "    R-->>QP: Revenue: $10M\n",
    "    QP->>R: Execute Step 2\n",
    "    R-->>QP: New product launch increased sales\n",
    "    QP->>G: Generate with context\n",
    "    G-->>U: Comprehensive Answer\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Handles complex multi-part queries\n",
    "- Optimizes retrieval strategy\n",
    "- Reduces unnecessary retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import anthropic\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "\n",
    "# Initialize Anthropic client\n",
    "client = anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "def query_planning_agent(user_query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze query and create a retrieval plan\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"QUERY PLANNING AGENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nUser Query: {user_query}\\n\")\n",
    "    \n",
    "    planning_prompt = f\"\"\"You are a query planning agent for a RAG system. Analyze this query and create a retrieval plan.\n",
    "\n",
    "Query: {user_query}\n",
    "\n",
    "Analyze:\n",
    "1. Query type (factual, analytical, comparative, multi-part)\n",
    "2. Information needed (data, context, reasoning)\n",
    "3. Number of retrieval steps required\n",
    "4. Optimal retrieval strategy\n",
    "\n",
    "Return a JSON plan:\n",
    "{{\n",
    "  \"query_type\": \"type\",\n",
    "  \"complexity\": \"simple|moderate|complex\",\n",
    "  \"retrieval_steps\": [\n",
    "    {{\"step\": 1, \"action\": \"retrieve X\", \"source\": \"documents|database|api\"}},\n",
    "    {{\"step\": 2, \"action\": \"retrieve Y\", \"source\": \"documents|database|api\"}}\n",
    "  ],\n",
    "  \"reasoning_required\": true|false\n",
    "}}\"\"\"\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1024,\n",
    "        messages=[{\"role\": \"user\", \"content\": planning_prompt}]\n",
    "    )\n",
    "    \n",
    "    result_text = response.content[0].text\n",
    "    \n",
    "    # Extract JSON from response\n",
    "    try:\n",
    "        start = result_text.find('{')\n",
    "        end = result_text.rfind('}') + 1\n",
    "        plan = json.loads(result_text[start:end])\n",
    "    except:\n",
    "        plan = {\n",
    "            \"query_type\": \"unknown\",\n",
    "            \"complexity\": \"simple\",\n",
    "            \"retrieval_steps\": [{\"step\": 1, \"action\": \"retrieve relevant documents\", \"source\": \"documents\"}],\n",
    "            \"reasoning_required\": False\n",
    "        }\n",
    "    \n",
    "    print(\"Query Analysis:\")\n",
    "    print(f\"  Type: {plan['query_type']}\")\n",
    "    print(f\"  Complexity: {plan['complexity']}\")\n",
    "    print(f\"  Reasoning Required: {plan['reasoning_required']}\")\n",
    "    print(f\"\\nRetrieval Plan ({len(plan['retrieval_steps'])} steps):\")\n",
    "    for step in plan['retrieval_steps']:\n",
    "        print(f\"  Step {step['step']}: {step['action']} from {step['source']}\")\n",
    "    \n",
    "    return plan\n",
    "\n",
    "# Example: Simple query\n",
    "plan1 = query_planning_agent(\"What is the company's revenue for Q3 2024?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Example: Complex query\n",
    "plan2 = query_planning_agent(\n",
    "    \"Compare our Q3 2024 revenue with Q3 2023, explain the main drivers of change, \"\n",
    "    \"and identify which product lines contributed most to growth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Routing Agent\n",
    "\n",
    "The Routing Agent directs queries to the most appropriate data source(s).\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Query] --> B{Routing Agent}\n",
    "    \n",
    "    B -->|Financial Data| C[SQL Database]\n",
    "    B -->|Product Docs| D[Vector Store]\n",
    "    B -->|Real-time Info| E[API/Web]\n",
    "    B -->|Code/Technical| F[Code Repository]\n",
    "    B -->|Relationships| G[Knowledge Graph]\n",
    "    \n",
    "    C --> H[Response Aggregator]\n",
    "    D --> H\n",
    "    E --> H\n",
    "    F --> H\n",
    "    G --> H\n",
    "    \n",
    "    H --> I[Synthesized Answer]\n",
    "    \n",
    "    style B fill:#ffccbc\n",
    "    style H fill:#fff9c4\n",
    "    style I fill:#c8e6c9\n",
    "```\n",
    "\n",
    "**Routing Strategies:**\n",
    "1. **Semantic Routing** - Based on query meaning\n",
    "2. **Keyword Routing** - Based on specific terms\n",
    "3. **LLM-based Routing** - LLM decides best source\n",
    "4. **Multi-source Routing** - Query multiple sources in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource:\n",
    "    \"\"\"Base class for data sources\"\"\"\n",
    "    def __init__(self, name: str, description: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "    \n",
    "    def query(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Execute query against this data source\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class VectorStoreSource(DataSource):\n",
    "    \"\"\"Simulated vector store for documents\"\"\"\n",
    "    def query(self, query: str) -> List[Dict[str, Any]]:\n",
    "        return [\n",
    "            {\n",
    "                \"content\": f\"Document about {query} from vector store\",\n",
    "                \"score\": 0.95,\n",
    "                \"source\": \"vector_store\",\n",
    "                \"metadata\": {\"type\": \"documentation\"}\n",
    "            }\n",
    "        ]\n",
    "\n",
    "class SQLDatabaseSource(DataSource):\n",
    "    \"\"\"Simulated SQL database\"\"\"\n",
    "    def query(self, query: str) -> List[Dict[str, Any]]:\n",
    "        return [\n",
    "            {\n",
    "                \"content\": f\"Structured data about {query} from database\",\n",
    "                \"score\": 1.0,\n",
    "                \"source\": \"sql_database\",\n",
    "                \"metadata\": {\"type\": \"financial_data\"}\n",
    "            }\n",
    "        ]\n",
    "\n",
    "class APISource(DataSource):\n",
    "    \"\"\"Simulated API for real-time data\"\"\"\n",
    "    def query(self, query: str) -> List[Dict[str, Any]]:\n",
    "        return [\n",
    "            {\n",
    "                \"content\": f\"Real-time information about {query} from API\",\n",
    "                \"score\": 0.90,\n",
    "                \"source\": \"api\",\n",
    "                \"metadata\": {\"type\": \"live_data\"}\n",
    "            }\n",
    "        ]\n",
    "\n",
    "def routing_agent(query: str, available_sources: List[DataSource]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Route query to appropriate data source(s)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ROUTING AGENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nQuery: {query}\\n\")\n",
    "    \n",
    "    # Build source descriptions\n",
    "    source_descriptions = \"\\n\".join([\n",
    "        f\"- {src.name}: {src.description}\"\n",
    "        for src in available_sources\n",
    "    ])\n",
    "    \n",
    "    routing_prompt = f\"\"\"You are a routing agent. Determine which data source(s) to query.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Available Sources:\n",
    "{source_descriptions}\n",
    "\n",
    "Select one or more sources. Return JSON:\n",
    "{{\n",
    "  \"selected_sources\": [\"source_name1\", \"source_name2\"],\n",
    "  \"reasoning\": \"why these sources\"\n",
    "}}\"\"\"\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=512,\n",
    "        messages=[{\"role\": \"user\", \"content\": routing_prompt}]\n",
    "    )\n",
    "    \n",
    "    result_text = response.content[0].text\n",
    "    \n",
    "    try:\n",
    "        start = result_text.find('{')\n",
    "        end = result_text.rfind('}') + 1\n",
    "        routing_decision = json.loads(result_text[start:end])\n",
    "    except:\n",
    "        routing_decision = {\n",
    "            \"selected_sources\": [available_sources[0].name],\n",
    "            \"reasoning\": \"Default to first source\"\n",
    "        }\n",
    "    \n",
    "    print(f\"Routing Decision:\")\n",
    "    print(f\"  Selected Sources: {', '.join(routing_decision['selected_sources'])}\")\n",
    "    print(f\"  Reasoning: {routing_decision['reasoning']}\")\n",
    "    \n",
    "    # Execute queries on selected sources\n",
    "    results = []\n",
    "    source_map = {src.name: src for src in available_sources}\n",
    "    \n",
    "    print(f\"\\nQuerying selected sources:\")\n",
    "    for source_name in routing_decision['selected_sources']:\n",
    "        if source_name in source_map:\n",
    "            source = source_map[source_name]\n",
    "            source_results = source.query(query)\n",
    "            results.extend(source_results)\n",
    "            print(f\"  {source_name}: {len(source_results)} results\")\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"routing_decision\": routing_decision,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "# Setup data sources\n",
    "sources = [\n",
    "    VectorStoreSource(\"vector_store\", \"Document embeddings for product documentation and guides\"),\n",
    "    SQLDatabaseSource(\"sql_database\", \"Structured financial and operational data\"),\n",
    "    APISource(\"api\", \"Real-time market data and external information\")\n",
    "]\n",
    "\n",
    "# Example: Financial query\n",
    "result1 = routing_agent(\"What was our Q3 2024 revenue?\", sources)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Example: Product documentation query\n",
    "result2 = routing_agent(\"How do I configure authentication in the API?\", sources)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Example: Real-time query\n",
    "result3 = routing_agent(\"What is the current stock price of our main competitor?\", sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Adaptive Retrieval Agent\n",
    "\n",
    "The Adaptive Retrieval Agent decides whether retrieved information is sufficient or if additional retrieval is needed.\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant Q as Query\n",
    "    participant A as Adaptive Agent\n",
    "    participant R as Retriever\n",
    "    participant E as Evaluator\n",
    "    \n",
    "    Q->>A: User Query\n",
    "    A->>R: Initial Retrieval (top-5)\n",
    "    R-->>A: 5 documents\n",
    "    A->>E: Evaluate Sufficiency\n",
    "    \n",
    "    alt Insufficient\n",
    "        E-->>A: Need more context\n",
    "        A->>R: Expanded Retrieval (top-10)\n",
    "        R-->>A: 10 documents\n",
    "        A->>E: Re-evaluate\n",
    "    end\n",
    "    \n",
    "    alt Still Insufficient\n",
    "        E-->>A: Need different query\n",
    "        A->>A: Reformulate Query\n",
    "        A->>R: New Retrieval\n",
    "        R-->>A: New documents\n",
    "    end\n",
    "    \n",
    "    E-->>A: Sufficient\n",
    "    A->>Q: Generate Answer\n",
    "```\n",
    "\n",
    "**Adaptive Strategies:**\n",
    "1. **Expand k** - Retrieve more documents\n",
    "2. **Query Reformulation** - Rephrase search query\n",
    "3. **Multi-hop Retrieval** - Follow references\n",
    "4. **Source Expansion** - Query additional sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_retrieval_agent(\n",
    "    query: str,\n",
    "    max_iterations: int = 3,\n",
    "    sufficiency_threshold: float = 0.8\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Adaptively retrieve information until sufficient context is obtained\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ADAPTIVE RETRIEVAL AGENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Max Iterations: {max_iterations}\")\n",
    "    print(f\"Sufficiency Threshold: {sufficiency_threshold}\\n\")\n",
    "    \n",
    "    all_retrieved = []\n",
    "    iteration = 0\n",
    "    current_query = query\n",
    "    k = 3  # Start with retrieving 3 documents\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        print(f\"\\n[Iteration {iteration}]\")\n",
    "        print(f\"  Retrieving top-{k} for: {current_query}\")\n",
    "        \n",
    "        # Simulate retrieval (in production, this would query real vector store)\n",
    "        retrieved_docs = [\n",
    "            {\n",
    "                \"content\": f\"Document {i} about {current_query} (iteration {iteration})\",\n",
    "                \"score\": 0.9 - (i * 0.1),\n",
    "                \"id\": f\"doc_{iteration}_{i}\"\n",
    "            }\n",
    "            for i in range(1, k + 1)\n",
    "        ]\n",
    "        \n",
    "        all_retrieved.extend(retrieved_docs)\n",
    "        print(f\"  Retrieved: {len(retrieved_docs)} documents\")\n",
    "        \n",
    "        # Evaluate sufficiency\n",
    "        sufficiency_prompt = f\"\"\"Evaluate if these retrieved documents are sufficient to answer the query.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Retrieved Documents:\n",
    "{chr(10).join([f\"- {doc['content']} (score: {doc['score']})\" for doc in all_retrieved])}\n",
    "\n",
    "Evaluate:\n",
    "1. Do these documents contain information to answer the query?\n",
    "2. Is additional retrieval needed?\n",
    "3. If insufficient, what strategy should be used?\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"sufficiency_score\": 0.0-1.0,\n",
    "  \"is_sufficient\": true|false,\n",
    "  \"reasoning\": \"explanation\",\n",
    "  \"next_action\": \"done|expand_k|reformulate_query|different_source\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=512,\n",
    "            messages=[{\"role\": \"user\", \"content\": sufficiency_prompt}]\n",
    "        )\n",
    "        \n",
    "        result_text = response.content[0].text\n",
    "        \n",
    "        try:\n",
    "            start = result_text.find('{')\n",
    "            end = result_text.rfind('}') + 1\n",
    "            evaluation = json.loads(result_text[start:end])\n",
    "        except:\n",
    "            evaluation = {\n",
    "                \"sufficiency_score\": 0.9,\n",
    "                \"is_sufficient\": True,\n",
    "                \"reasoning\": \"Assumed sufficient\",\n",
    "                \"next_action\": \"done\"\n",
    "            }\n",
    "        \n",
    "        print(f\"  Sufficiency Score: {evaluation['sufficiency_score']:.2f}\")\n",
    "        print(f\"  Status: {'Sufficient' if evaluation['is_sufficient'] else 'Insufficient'}\")\n",
    "        print(f\"  Reasoning: {evaluation['reasoning']}\")\n",
    "        \n",
    "        # Check if we should continue\n",
    "        if evaluation['is_sufficient'] or evaluation['sufficiency_score'] >= sufficiency_threshold:\n",
    "            print(f\"\\n[Complete] Retrieved sufficient information in {iteration} iteration(s)\")\n",
    "            break\n",
    "        \n",
    "        # Adapt retrieval strategy\n",
    "        next_action = evaluation.get('next_action', 'expand_k')\n",
    "        print(f\"  Next Action: {next_action}\")\n",
    "        \n",
    "        if next_action == 'expand_k':\n",
    "            k += 2  # Retrieve 2 more documents\n",
    "        elif next_action == 'reformulate_query':\n",
    "            # Reformulate query (simplified here)\n",
    "            reformulation_prompt = f\"Reformulate this query to retrieve different information: {current_query}\"\n",
    "            ref_response = client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=256,\n",
    "                messages=[{\"role\": \"user\", \"content\": reformulation_prompt}]\n",
    "            )\n",
    "            current_query = ref_response.content[0].text.strip()\n",
    "            print(f\"  Reformulated to: {current_query}\")\n",
    "        \n",
    "        if iteration >= max_iterations:\n",
    "            print(f\"\\n[Complete] Max iterations reached\")\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"iterations\": iteration,\n",
    "        \"total_documents\": len(all_retrieved),\n",
    "        \"documents\": all_retrieved,\n",
    "        \"final_sufficiency\": evaluation.get('sufficiency_score', 0.0)\n",
    "    }\n",
    "\n",
    "# Example\n",
    "result = adaptive_retrieval_agent(\n",
    "    \"What are the best practices for implementing authentication in microservices?\",\n",
    "    max_iterations=3,\n",
    "    sufficiency_threshold=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Agentic RAG Patterns\n",
    "\n",
    "### 2.1 Self-RAG: Self-Reflective Retrieval\n",
    "\n",
    "Self-RAG adds reflection steps where the agent evaluates its own generation quality.\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Query] --> B[Retrieve Documents]\n",
    "    B --> C[Generate Answer]\n",
    "    C --> D{Self-Reflection}\n",
    "    \n",
    "    D -->|Relevant & Supported| E[Return Answer]\n",
    "    \n",
    "    D -->|Not Relevant| F[Retrieve Different Docs]\n",
    "    F --> C\n",
    "    \n",
    "    D -->|Not Supported| G[Request More Evidence]\n",
    "    G --> B\n",
    "    \n",
    "    D -->|Hallucination Detected| H[Regenerate]\n",
    "    H --> C\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style D fill:#ffccbc\n",
    "    style E fill:#c8e6c9\n",
    "```\n",
    "\n",
    "**Reflection Criteria:**\n",
    "1. **Relevance** - Are retrieved docs relevant to query?\n",
    "2. **Support** - Is answer supported by retrieved docs?\n",
    "3. **Completeness** - Does answer fully address query?\n",
    "4. **Consistency** - Are there contradictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_rag_system(query: str, documents: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Self-RAG with reflection and verification\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SELF-RAG SYSTEM\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nQuery: {query}\\n\")\n",
    "    \n",
    "    max_attempts = 3\n",
    "    attempt = 0\n",
    "    \n",
    "    while attempt < max_attempts:\n",
    "        attempt += 1\n",
    "        print(f\"\\n[Attempt {attempt}]\")\n",
    "        \n",
    "        # Step 1: Generate answer\n",
    "        print(\"  Generating answer...\")\n",
    "        generation_prompt = f\"\"\"Answer this query using the provided documents:\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Documents:\n",
    "{chr(10).join([f\"- {doc}\" for doc in documents])}\n",
    "\n",
    "Provide a clear, concise answer citing the documents.\"\"\"\n",
    "        \n",
    "        gen_response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=1024,\n",
    "            messages=[{\"role\": \"user\", \"content\": generation_prompt}]\n",
    "        )\n",
    "        \n",
    "        generated_answer = gen_response.content[0].text\n",
    "        print(f\"  Generated answer (excerpt): {generated_answer[:150]}...\")\n",
    "        \n",
    "        # Step 2: Self-reflection\n",
    "        print(\"  Performing self-reflection...\")\n",
    "        reflection_prompt = f\"\"\"Evaluate this generated answer:\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Documents:\n",
    "{chr(10).join([f\"- {doc}\" for doc in documents])}\n",
    "\n",
    "Generated Answer:\n",
    "{generated_answer}\n",
    "\n",
    "Evaluate:\n",
    "1. Relevance (0-1): Are documents relevant to query?\n",
    "2. Support (0-1): Is answer supported by documents?\n",
    "3. Completeness (0-1): Does answer fully address query?\n",
    "4. Factuality (0-1): Are there hallucinations?\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"relevance\": 0.0-1.0,\n",
    "  \"support\": 0.0-1.0,\n",
    "  \"completeness\": 0.0-1.0,\n",
    "  \"factuality\": 0.0-1.0,\n",
    "  \"overall_quality\": 0.0-1.0,\n",
    "  \"accept\": true|false,\n",
    "  \"issues\": [\"list of issues if any\"]\n",
    "}}\"\"\"\n",
    "        \n",
    "        ref_response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=512,\n",
    "            messages=[{\"role\": \"user\", \"content\": reflection_prompt}]\n",
    "        )\n",
    "        \n",
    "        result_text = ref_response.content[0].text\n",
    "        \n",
    "        try:\n",
    "            start = result_text.find('{')\n",
    "            end = result_text.rfind('}') + 1\n",
    "            reflection = json.loads(result_text[start:end])\n",
    "        except:\n",
    "            reflection = {\n",
    "                \"relevance\": 0.9,\n",
    "                \"support\": 0.9,\n",
    "                \"completeness\": 0.9,\n",
    "                \"factuality\": 0.9,\n",
    "                \"overall_quality\": 0.9,\n",
    "                \"accept\": True,\n",
    "                \"issues\": []\n",
    "            }\n",
    "        \n",
    "        print(f\"  Reflection Scores:\")\n",
    "        print(f\"    Relevance: {reflection['relevance']:.2f}\")\n",
    "        print(f\"    Support: {reflection['support']:.2f}\")\n",
    "        print(f\"    Completeness: {reflection['completeness']:.2f}\")\n",
    "        print(f\"    Factuality: {reflection['factuality']:.2f}\")\n",
    "        print(f\"    Overall: {reflection['overall_quality']:.2f}\")\n",
    "        \n",
    "        # Step 3: Decide\n",
    "        if reflection['accept'] and reflection['overall_quality'] >= 0.8:\n",
    "            print(f\"\\n[Success] Answer accepted after {attempt} attempt(s)\")\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"answer\": generated_answer,\n",
    "                \"attempts\": attempt,\n",
    "                \"reflection\": reflection,\n",
    "                \"status\": \"accepted\"\n",
    "            }\n",
    "        \n",
    "        if reflection['issues']:\n",
    "            print(f\"  Issues identified: {', '.join(reflection['issues'])}\")\n",
    "        \n",
    "        if attempt >= max_attempts:\n",
    "            print(f\"\\n[Warning] Max attempts reached, returning best answer\")\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"answer\": generated_answer,\n",
    "                \"attempts\": attempt,\n",
    "                \"reflection\": reflection,\n",
    "                \"status\": \"max_attempts\"\n",
    "            }\n",
    "        \n",
    "        print(f\"  Retrying with refined approach...\")\n",
    "    \n",
    "    return {\"status\": \"failed\"}\n",
    "\n",
    "# Example\n",
    "example_docs = [\n",
    "    \"The company's Q3 2024 revenue was $15M, up 25% from Q3 2023.\",\n",
    "    \"Main growth drivers included new product launches and expanded market presence in APAC.\",\n",
    "    \"The SaaS division contributed 60% of total revenue, growing 40% YoY.\"\n",
    "]\n",
    "\n",
    "result = self_rag_system(\n",
    "    \"What was Q3 2024 revenue and what drove the growth?\",\n",
    "    example_docs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Corrective RAG (CRAG)\n",
    "\n",
    "Corrective RAG evaluates retrieved document quality and takes corrective actions.\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Query] --> B[Initial Retrieval]\n",
    "    B --> C{Evaluate Relevance}\n",
    "    \n",
    "    C -->|High Relevance| D[Use Retrieved Docs]\n",
    "    C -->|Medium Relevance| E[Filter & Refine]\n",
    "    C -->|Low Relevance| F[Web Search Fallback]\n",
    "    \n",
    "    E --> G[Knowledge Refinement]\n",
    "    F --> G\n",
    "    \n",
    "    D --> H[Generate Answer]\n",
    "    G --> H\n",
    "    \n",
    "    H --> I[Final Answer]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style C fill:#ffccbc\n",
    "    style I fill:#c8e6c9\n",
    "```\n",
    "\n",
    "**Corrective Actions:**\n",
    "1. **Filter** - Remove low-relevance documents\n",
    "2. **Refine** - Extract only relevant portions\n",
    "3. **Augment** - Add external knowledge\n",
    "4. **Fallback** - Use web search if internal docs insufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Multi-Hop RAG\n",
    "\n",
    "Multi-hop RAG performs multiple retrieval steps, using information from each step to inform the next.\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant Q as Query\n",
    "    participant A as Agent\n",
    "    participant R as Retriever\n",
    "    \n",
    "    Q->>A: \"How did Company X's Q3 performance compare to competitors?\"\n",
    "    \n",
    "    Note over A: Hop 1: Get Company X data\n",
    "    A->>R: Retrieve Company X Q3 results\n",
    "    R-->>A: Company X: $10M revenue\n",
    "    \n",
    "    Note over A: Hop 2: Identify competitors\n",
    "    A->>R: Who are Company X's main competitors?\n",
    "    R-->>A: Competitors: Company Y, Company Z\n",
    "    \n",
    "    Note over A: Hop 3: Get competitor data\n",
    "    A->>R: Retrieve Company Y, Z Q3 results\n",
    "    R-->>A: Company Y: $8M, Company Z: $12M\n",
    "    \n",
    "    Note over A: Synthesize\n",
    "    A->>Q: Comparative analysis\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hop_rag(query: str, max_hops: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Multi-hop retrieval where each step informs the next\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MULTI-HOP RAG\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Max Hops: {max_hops}\\n\")\n",
    "    \n",
    "    all_retrieved = []\n",
    "    hop_history = []\n",
    "    current_context = \"\"\n",
    "    \n",
    "    for hop in range(1, max_hops + 1):\n",
    "        print(f\"\\n[Hop {hop}]\")\n",
    "        \n",
    "        # Determine what to retrieve next\n",
    "        planning_prompt = f\"\"\"Based on the query and retrieved information so far, what should we retrieve next?\n",
    "\n",
    "Original Query: {query}\n",
    "\n",
    "Retrieved So Far:\n",
    "{current_context if current_context else '(Nothing yet)'}\n",
    "\n",
    "What information is still needed? Generate a specific retrieval query.\n",
    "If all information is available, respond with \"DONE\".\n",
    "\n",
    "Return just the retrieval query or \"DONE\".\"\"\"\n",
    "        \n",
    "        plan_response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=256,\n",
    "            messages=[{\"role\": \"user\", \"content\": planning_prompt}]\n",
    "        )\n",
    "        \n",
    "        next_query = plan_response.content[0].text.strip()\n",
    "        \n",
    "        if \"DONE\" in next_query.upper():\n",
    "            print(f\"  Agent determined all necessary information has been retrieved\")\n",
    "            break\n",
    "        \n",
    "        print(f\"  Retrieval query: {next_query}\")\n",
    "        \n",
    "        # Simulate retrieval (in production, this would query real data sources)\n",
    "        retrieved = {\n",
    "            \"hop\": hop,\n",
    "            \"query\": next_query,\n",
    "            \"results\": [\n",
    "                f\"Information for hop {hop} about: {next_query}\",\n",
    "                f\"Additional details for hop {hop}\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        all_retrieved.append(retrieved)\n",
    "        hop_history.append(next_query)\n",
    "        \n",
    "        # Update context\n",
    "        current_context += f\"\\n\\nHop {hop} ({next_query}):\\n\"\n",
    "        current_context += \"\\n\".join(retrieved[\"results\"])\n",
    "        \n",
    "        print(f\"  Retrieved: {len(retrieved['results'])} results\")\n",
    "    \n",
    "    # Final synthesis\n",
    "    print(f\"\\n[Synthesis]\")\n",
    "    print(f\"  Synthesizing answer from {len(all_retrieved)} hops...\")\n",
    "    \n",
    "    synthesis_prompt = f\"\"\"Synthesize a comprehensive answer using all retrieved information:\n",
    "\n",
    "Original Query: {query}\n",
    "\n",
    "Retrieved Information:\n",
    "{current_context}\n",
    "\n",
    "Provide a clear, comprehensive answer that integrates all the retrieved information.\"\"\"\n",
    "    \n",
    "    synthesis_response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1024,\n",
    "        messages=[{\"role\": \"user\", \"content\": synthesis_prompt}]\n",
    "    )\n",
    "    \n",
    "    final_answer = synthesis_response.content[0].text\n",
    "    \n",
    "    print(f\"\\n[Complete] Multi-hop retrieval finished in {len(all_retrieved)} hops\")\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"num_hops\": len(all_retrieved),\n",
    "        \"hop_history\": hop_history,\n",
    "        \"all_retrieved\": all_retrieved,\n",
    "        \"final_answer\": final_answer\n",
    "    }\n",
    "\n",
    "# Example\n",
    "result = multi_hop_rag(\n",
    "    \"How does the performance of our flagship product compare to the top 3 competitors \"\n",
    "    \"in terms of both market share and customer satisfaction?\",\n",
    "    max_hops=4\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Answer (excerpt):\\n{result['final_answer'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Complete Agentic RAG System\n",
    "\n",
    "### Integrated Architecture\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[User Query] --> B[Query Planning Agent]\n",
    "    B --> C{Complexity Analysis}\n",
    "    \n",
    "    C -->|Simple| D[Direct RAG]\n",
    "    C -->|Complex| E[Agentic RAG]\n",
    "    \n",
    "    E --> F[Routing Agent]\n",
    "    F --> G1[Vector Store]\n",
    "    F --> G2[SQL Database]\n",
    "    F --> G3[External APIs]\n",
    "    \n",
    "    G1 --> H[Adaptive Retrieval]\n",
    "    G2 --> H\n",
    "    G3 --> H\n",
    "    \n",
    "    H --> I{Multi-hop Needed?}\n",
    "    I -->|Yes| J[Multi-hop Retrieval]\n",
    "    I -->|No| K[Single-pass]\n",
    "    \n",
    "    J --> L[Self-RAG Reflection]\n",
    "    K --> L\n",
    "    D --> L\n",
    "    \n",
    "    L --> M{Quality Check}\n",
    "    M -->|Pass| N[Final Answer]\n",
    "    M -->|Fail| H\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style C fill:#fff9c4\n",
    "    style M fill:#ffccbc\n",
    "    style N fill:#c8e6c9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticRAGSystem:\n",
    "    \"\"\"\n",
    "    Complete agentic RAG system with all patterns integrated\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_sources: List[DataSource]):\n",
    "        self.data_sources = data_sources\n",
    "        self.query_count = 0\n",
    "        self.total_retrievals = 0\n",
    "    \n",
    "    def query(self, user_query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process query through complete agentic RAG pipeline\n",
    "        \"\"\"\n",
    "        self.query_count += 1\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"AGENTIC RAG SYSTEM\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Query #{self.query_count}: {user_query}\\n\")\n",
    "        \n",
    "        # Step 1: Query Planning\n",
    "        plan = query_planning_agent(user_query)\n",
    "        \n",
    "        # Step 2: Route to appropriate sources\n",
    "        routing_result = routing_agent(user_query, self.data_sources)\n",
    "        \n",
    "        # Step 3: Adaptive Retrieval\n",
    "        retrieval_result = adaptive_retrieval_agent(\n",
    "            user_query,\n",
    "            max_iterations=2 if plan['complexity'] == 'simple' else 3\n",
    "        )\n",
    "        \n",
    "        self.total_retrievals += retrieval_result['total_documents']\n",
    "        \n",
    "        # Step 4: Multi-hop if needed\n",
    "        if plan.get('reasoning_required', False):\n",
    "            print(\"\\n[Multi-hop reasoning enabled]\")\n",
    "            multihop_result = multi_hop_rag(user_query, max_hops=2)\n",
    "            context = multihop_result['final_answer']\n",
    "        else:\n",
    "            context = \"\\n\".join([\n",
    "                doc['content'] for doc in retrieval_result['documents']\n",
    "            ])\n",
    "        \n",
    "        # Step 5: Generate answer\n",
    "        generation_prompt = f\"\"\"Answer this query using the retrieved information:\n",
    "\n",
    "Query: {user_query}\n",
    "\n",
    "Retrieved Context:\n",
    "{context}\n",
    "\n",
    "Provide a comprehensive answer with citations.\"\"\"\n",
    "        \n",
    "        gen_response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=2048,\n",
    "            messages=[{\"role\": \"user\", \"content\": generation_prompt}]\n",
    "        )\n",
    "        \n",
    "        answer = gen_response.content[0].text\n",
    "        \n",
    "        # Step 6: Self-reflection quality check\n",
    "        reflection_prompt = f\"\"\"Evaluate this RAG answer:\n",
    "\n",
    "Query: {user_query}\n",
    "Answer: {answer}\n",
    "\n",
    "Rate overall quality (0-1) and identify any issues.\n",
    "Return JSON: {{\"quality_score\": 0.0-1.0, \"issues\": []}}\"\"\"\n",
    "        \n",
    "        ref_response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=256,\n",
    "            messages=[{\"role\": \"user\", \"content\": reflection_prompt}]\n",
    "        )\n",
    "        \n",
    "        result_text = ref_response.content[0].text\n",
    "        \n",
    "        try:\n",
    "            start = result_text.find('{')\n",
    "            end = result_text.rfind('}') + 1\n",
    "            quality = json.loads(result_text[start:end])\n",
    "        except:\n",
    "            quality = {\"quality_score\": 0.85, \"issues\": []}\n",
    "        \n",
    "        print(f\"\\n[Quality Check] Score: {quality['quality_score']:.2f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FINAL ANSWER\")\n",
    "        print(\"=\"*60)\n",
    "        print(answer)\n",
    "        \n",
    "        return {\n",
    "            \"query\": user_query,\n",
    "            \"plan\": plan,\n",
    "            \"routing\": routing_result['routing_decision'],\n",
    "            \"retrieval_iterations\": retrieval_result['iterations'],\n",
    "            \"total_documents\": retrieval_result['total_documents'],\n",
    "            \"answer\": answer,\n",
    "            \"quality_score\": quality['quality_score'],\n",
    "            \"issues\": quality.get('issues', [])\n",
    "        }\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get system statistics\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"total_queries\": self.query_count,\n",
    "            \"total_retrievals\": self.total_retrievals,\n",
    "            \"avg_retrievals_per_query\": self.total_retrievals / max(1, self.query_count)\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "sources = [\n",
    "    VectorStoreSource(\"vector_store\", \"Document embeddings for product documentation\"),\n",
    "    SQLDatabaseSource(\"sql_database\", \"Structured financial and operational data\"),\n",
    "    APISource(\"api\", \"Real-time external data\")\n",
    "]\n",
    "\n",
    "rag_system = AgenticRAGSystem(data_sources=sources)\n",
    "\n",
    "# Test with different query types\n",
    "result1 = rag_system.query(\"What was our Q3 2024 revenue?\")\n",
    "\n",
    "print(\"\\n\\n\" + \"#\"*60 + \"\\n\")\n",
    "\n",
    "result2 = rag_system.query(\n",
    "    \"Compare our Q3 2024 performance to competitors and explain the key factors \"\n",
    "    \"that differentiated our results\"\n",
    ")\n",
    "\n",
    "# Show statistics\n",
    "stats = rag_system.get_stats()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SYSTEM STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Queries: {stats['total_queries']}\")\n",
    "print(f\"Total Retrievals: {stats['total_retrievals']}\")\n",
    "print(f\"Avg Retrievals/Query: {stats['avg_retrievals_per_query']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Production Best Practices\n",
    "\n",
    "### 4.1 Performance Optimization\n",
    "\n",
    "**Caching Strategy:**\n",
    "```python\n",
    "# Cache at multiple levels\n",
    "- Query embeddings cache (1 hour TTL)\n",
    "- Retrieved documents cache (30 min TTL)\n",
    "- Generated answers cache (15 min TTL)\n",
    "- LLM routing decisions cache (1 hour TTL)\n",
    "```\n",
    "\n",
    "**Parallel Execution:**\n",
    "```python\n",
    "# Execute independent retrievals in parallel\n",
    "import asyncio\n",
    "\n",
    "async def parallel_retrieve():\n",
    "    tasks = [\n",
    "        retrieve_from_vector_store(query),\n",
    "        retrieve_from_database(query),\n",
    "        retrieve_from_api(query)\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "```\n",
    "\n",
    "**Model Selection:**\n",
    "- Query planning: Claude Haiku (fast, cheap)\n",
    "- Routing: Claude Haiku\n",
    "- Generation: Claude Sonnet (quality)\n",
    "- Reflection: Claude Sonnet\n",
    "\n",
    "### 4.2 Cost Management\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Incoming Query] --> B{Check Cache}\n",
    "    B -->|Hit| C[Return Cached]\n",
    "    B -->|Miss| D{Complexity}\n",
    "    \n",
    "    D -->|Simple| E[Direct RAG with Haiku]\n",
    "    D -->|Medium| F[Agentic RAG with Haiku + Sonnet]\n",
    "    D -->|Complex| G[Full Agentic with Sonnet]\n",
    "    \n",
    "    E --> H[Cache Result]\n",
    "    F --> H\n",
    "    G --> H\n",
    "    \n",
    "    H --> I[Return to User]\n",
    "    \n",
    "    style C fill:#c8e6c9\n",
    "    style D fill:#ffccbc\n",
    "```\n",
    "\n",
    "**Cost Optimization Tips:**\n",
    "1. Use complexity detection to avoid over-engineering simple queries\n",
    "2. Implement aggressive caching for common queries\n",
    "3. Set iteration limits to prevent runaway costs\n",
    "4. Use cheaper models for routing and planning\n",
    "5. Monitor token usage per query type\n",
    "\n",
    "### 4.3 Monitoring and Observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonitoredAgenticRAG(AgenticRAGSystem):\n",
    "    \"\"\"\n",
    "    Agentic RAG with comprehensive monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_sources: List[DataSource]):\n",
    "        super().__init__(data_sources)\n",
    "        self.metrics = {\n",
    "            \"queries_by_complexity\": {\"simple\": 0, \"moderate\": 0, \"complex\": 0},\n",
    "            \"avg_retrieval_iterations\": [],\n",
    "            \"avg_quality_scores\": [],\n",
    "            \"cache_hits\": 0,\n",
    "            \"cache_misses\": 0,\n",
    "            \"errors\": []\n",
    "        }\n",
    "    \n",
    "    def query(self, user_query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Query with monitoring\n",
    "        \"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            result = super().query(user_query)\n",
    "            \n",
    "            # Record metrics\n",
    "            complexity = result['plan']['complexity']\n",
    "            self.metrics['queries_by_complexity'][complexity] += 1\n",
    "            self.metrics['avg_retrieval_iterations'].append(result['retrieval_iterations'])\n",
    "            self.metrics['avg_quality_scores'].append(result['quality_score'])\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            print(f\"\\n[Monitoring] Query processed in {elapsed:.2f}s\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.metrics['errors'].append({\n",
    "                \"query\": user_query,\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": time.time()\n",
    "            })\n",
    "            raise\n",
    "    \n",
    "    def get_performance_report(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate performance report\n",
    "        \"\"\"\n",
    "        avg_iterations = sum(self.metrics['avg_retrieval_iterations']) / max(1, len(self.metrics['avg_retrieval_iterations']))\n",
    "        avg_quality = sum(self.metrics['avg_quality_scores']) / max(1, len(self.metrics['avg_quality_scores']))\n",
    "        \n",
    "        report = f\"\"\"\n",
    "{'='*60}\n",
    "AGENTIC RAG PERFORMANCE REPORT\n",
    "{'='*60}\n",
    "\n",
    "Query Statistics:\n",
    "  Total Queries: {self.query_count}\n",
    "  By Complexity:\n",
    "    Simple: {self.metrics['queries_by_complexity']['simple']}\n",
    "    Moderate: {self.metrics['queries_by_complexity']['moderate']}\n",
    "    Complex: {self.metrics['queries_by_complexity']['complex']}\n",
    "\n",
    "Retrieval Statistics:\n",
    "  Total Retrievals: {self.total_retrievals}\n",
    "  Avg Iterations/Query: {avg_iterations:.1f}\n",
    "  Avg Documents/Query: {self.total_retrievals / max(1, self.query_count):.1f}\n",
    "\n",
    "Quality Metrics:\n",
    "  Avg Quality Score: {avg_quality:.2f}\n",
    "\n",
    "Cache Performance:\n",
    "  Hits: {self.metrics['cache_hits']}\n",
    "  Misses: {self.metrics['cache_misses']}\n",
    "  Hit Rate: {self.metrics['cache_hits'] / max(1, self.metrics['cache_hits'] + self.metrics['cache_misses']) * 100:.1f}%\n",
    "\n",
    "Errors: {len(self.metrics['errors'])}\n",
    "        \"\"\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Example\n",
    "print(\"Monitored Agentic RAG system ready for production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interview Q&A\n",
    "\n",
    "### Q1: What's the main difference between traditional RAG and agentic RAG?\n",
    "\n",
    "**Answer:**\n",
    "Traditional RAG follows a fixed pipeline: embed query → retrieve top-k documents → generate answer. It's static and doesn't adapt to query complexity.\n",
    "\n",
    "Agentic RAG gives the LLM agency to:\n",
    "- **Plan** - Analyze query complexity and create retrieval strategy\n",
    "- **Route** - Select appropriate data sources dynamically\n",
    "- **Adapt** - Iteratively retrieve more information if needed\n",
    "- **Reason** - Perform multi-hop reasoning over retrieved docs\n",
    "- **Verify** - Self-evaluate answer quality and retry if insufficient\n",
    "\n",
    "Trade-off: Agentic RAG is more expensive and slower but handles complex queries much better.\n",
    "\n",
    "### Q2: When should you use agentic RAG vs. traditional RAG?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Traditional RAG:**\n",
    "- Simple factual queries with clear answers\n",
    "- Single data source\n",
    "- Latency-critical applications\n",
    "- Cost-constrained scenarios\n",
    "- High-volume, low-complexity queries (customer support)\n",
    "\n",
    "**Agentic RAG:**\n",
    "- Complex multi-part questions\n",
    "- Multiple heterogeneous data sources\n",
    "- High-stakes decisions requiring verification\n",
    "- Research and analysis tasks\n",
    "- Queries requiring reasoning over retrieved information\n",
    "\n",
    "**Best Practice:** Use complexity detection to route simple queries to traditional RAG and complex queries to agentic RAG.\n",
    "\n",
    "### Q3: What is Self-RAG and why is it important?\n",
    "\n",
    "**Answer:**\n",
    "Self-RAG adds self-reflection steps where the agent evaluates its own output:\n",
    "\n",
    "1. **Relevance Check** - Are retrieved docs relevant to query?\n",
    "2. **Support Check** - Is answer supported by retrieved docs?\n",
    "3. **Factuality Check** - Are there hallucinations?\n",
    "4. **Completeness Check** - Does answer fully address query?\n",
    "\n",
    "If evaluation fails, the agent:\n",
    "- Retrieves different/additional documents\n",
    "- Reformulates the query\n",
    "- Regenerates the answer\n",
    "\n",
    "**Why Important:**\n",
    "- Reduces hallucinations significantly\n",
    "- Improves answer quality automatically\n",
    "- Essential for high-stakes applications (medical, legal, financial)\n",
    "\n",
    "### Q4: Explain multi-hop RAG with an example.\n",
    "\n",
    "**Answer:**\n",
    "Multi-hop RAG performs multiple retrieval steps where each step informs the next.\n",
    "\n",
    "**Example Query:** \"How did our Q3 revenue compare to our main competitors?\"\n",
    "\n",
    "**Multi-hop Process:**\n",
    "1. **Hop 1:** Retrieve our Q3 revenue → \"$10M\"\n",
    "2. **Hop 2:** Identify main competitors → \"Company X, Company Y\"\n",
    "3. **Hop 3:** Retrieve competitor Q3 revenues → \"X: $8M, Y: $12M\"\n",
    "4. **Synthesis:** \"We ranked 2nd with $10M, between Y ($12M) and X ($8M)\"\n",
    "\n",
    "Each retrieval step uses information from previous steps. You can't get competitor revenue without first knowing who the competitors are.\n",
    "\n",
    "**Use Cases:**\n",
    "- Comparative analysis\n",
    "- Research requiring background context\n",
    "- Questions with implicit dependencies\n",
    "\n",
    "### Q5: How do you handle multiple data sources in agentic RAG?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**1. Source Registration:**\n",
    "- Register each source with description of what it contains\n",
    "- Examples: vector store (docs), SQL DB (structured data), API (real-time)\n",
    "\n",
    "**2. Routing Strategy:**\n",
    "- Use LLM to route query to appropriate source(s)\n",
    "- Can query multiple sources in parallel\n",
    "- Example: Financial query → SQL DB, Product query → Vector store\n",
    "\n",
    "**3. Result Aggregation:**\n",
    "- Collect results from all queried sources\n",
    "- Deduplicate and rank by relevance\n",
    "- Synthesize into coherent answer\n",
    "\n",
    "**4. Source Attribution:**\n",
    "- Track which source provided which information\n",
    "- Include source citations in final answer\n",
    "\n",
    "### Q6: What are the main production challenges with agentic RAG?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**1. Cost Management:**\n",
    "- Multiple LLM calls per query (planning, routing, generation, reflection)\n",
    "- Mitigation: Complexity detection, caching, cheaper models for routing\n",
    "\n",
    "**2. Latency:**\n",
    "- Iterative retrieval and multi-hop reasoning are slow\n",
    "- Mitigation: Parallel execution, set iteration limits, async operations\n",
    "\n",
    "**3. Quality Consistency:**\n",
    "- Non-deterministic due to LLM decision-making\n",
    "- Mitigation: Quality thresholds, monitoring, fallback to traditional RAG\n",
    "\n",
    "**4. Complexity:**\n",
    "- More moving parts = more failure modes\n",
    "- Mitigation: Comprehensive monitoring, graceful degradation, circuit breakers\n",
    "\n",
    "**5. Context Window Management:**\n",
    "- Multi-hop retrieval can accumulate large context\n",
    "- Mitigation: Summarization, selective context passing, context pruning\n",
    "\n",
    "### Q7: How do you evaluate agentic RAG quality?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Retrieval Metrics:**\n",
    "- Retrieval precision (% relevant docs retrieved)\n",
    "- Retrieval recall (% of all relevant docs retrieved)\n",
    "- Average iterations to sufficiency\n",
    "\n",
    "**Generation Metrics:**\n",
    "- Answer relevance (does it address the query?)\n",
    "- Faithfulness (is answer supported by retrieved docs?)\n",
    "- Completeness (does it fully answer the query?)\n",
    "- Citation accuracy (are citations correct?)\n",
    "\n",
    "**System Metrics:**\n",
    "- Latency (p50, p95, p99)\n",
    "- Cost per query\n",
    "- Cache hit rate\n",
    "- Error rate\n",
    "\n",
    "**Evaluation Methods:**\n",
    "1. **LLM-as-judge** - Use LLM to score answers\n",
    "2. **Human evaluation** - Gold standard for critical applications\n",
    "3. **A/B testing** - Compare agentic vs traditional RAG\n",
    "4. **Regression testing** - Test on known good query-answer pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "### Agentic RAG Patterns\n",
    "1. **Query Planning** - Analyze complexity and create retrieval strategy\n",
    "2. **Routing** - Direct queries to appropriate data sources\n",
    "3. **Adaptive Retrieval** - Iteratively retrieve until sufficient\n",
    "4. **Self-RAG** - Self-evaluate and correct answers\n",
    "5. **Multi-hop** - Chain retrievals for complex reasoning\n",
    "\n",
    "### When to Use Agentic RAG\n",
    "- Complex, multi-faceted queries\n",
    "- Multiple heterogeneous data sources\n",
    "- High-stakes applications requiring verification\n",
    "- Research and analytical tasks\n",
    "- When traditional RAG quality is insufficient\n",
    "\n",
    "### Production Best Practices\n",
    "1. **Complexity Detection** - Route simple queries to traditional RAG\n",
    "2. **Cost Optimization** - Cache aggressively, use cheaper models for routing\n",
    "3. **Iteration Limits** - Prevent runaway costs and latency\n",
    "4. **Monitoring** - Track quality, cost, latency by query complexity\n",
    "5. **Graceful Degradation** - Fall back to traditional RAG on errors\n",
    "\n",
    "### Trade-offs\n",
    "| Aspect | Traditional RAG | Agentic RAG |\n",
    "|--------|----------------|-------------|\n",
    "| **Latency** | Fast (~1s) | Slow (~5-15s) |\n",
    "| **Cost** | Low | High (3-10x) |\n",
    "| **Quality** | Good for simple | Excellent for complex |\n",
    "| **Reliability** | Deterministic | Non-deterministic |\n",
    "| **Complexity** | Simple | Complex |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Explore Framework Comparison** - See how different frameworks implement these patterns\n",
    "2. **Build Production System** - Apply these patterns to your use case\n",
    "3. **Implement Monitoring** - Track quality and cost metrics\n",
    "4. **Optimize Performance** - Cache, parallelize, and use appropriate models\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Self-RAG Paper](https://arxiv.org/abs/2310.11511)\n",
    "- [Corrective RAG (CRAG)](https://arxiv.org/abs/2401.15884)\n",
    "- [LangChain RAG Patterns](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "- [LlamaIndex Advanced RAG](https://docs.llamaindex.ai/en/stable/examples/query_engine/)\n",
    "- [Anthropic RAG Best Practices](https://www.anthropic.com/index/claude-2-1-retrieval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
