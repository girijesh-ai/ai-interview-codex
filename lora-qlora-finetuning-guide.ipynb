{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Complete Guide to LoRA & QLoRA Fine-tuning (2025)\n",
    "\n",
    "## Practical Implementation: MITRE ATT&CK TTP Mapping\n",
    "\n",
    "This comprehensive notebook covers:\n",
    "1. **PEFT Methods** - LoRA, QLoRA, DoRA, AdaLoRA\n",
    "2. **Quantization** - 4-bit, 8-bit, NF4, bitsandbytes\n",
    "3. **Mixed Precision Training** - FP16, BF16, FP8\n",
    "4. **Practical Fine-tuning** - Qwen 1.8B on MITRE TTP dataset\n",
    "5. **SOTA Techniques** - Flash Attention, Paged Optimizers, Gradient Checkpointing\n",
    "\n",
    "**Model:** Qwen2.5-1.5B (state-of-the-art 1B+ model)\n",
    "\n",
    "**Task:** Map cybersecurity threat intelligence to MITRE ATT&CK TTPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "## Architecture Overview: LoRA vs QLoRA\n",
    "\n",
    "### LoRA (Low-Rank Adaptation) Architecture\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Input X] --> B[Frozen Pre-trained Weight W]\n",
    "    A --> C[LoRA Low-Rank Matrices]\n",
    "    \n",
    "    C --> D[Matrix A: d×r]\n",
    "    C --> E[Matrix B: r×d]\n",
    "    \n",
    "    D --> F[\"BA product (rank r)\"]\n",
    "    E --> F\n",
    "    \n",
    "    B --> G[\"W·x (frozen)\"]\n",
    "    F --> H[\"BA·x (trainable)\"]\n",
    "    \n",
    "    G --> I[\"Output: W·x + α/r·BA·x\"]\n",
    "    H --> I\n",
    "    \n",
    "    style B fill:#ff9999\n",
    "    style D fill:#99ff99\n",
    "    style E fill:#99ff99\n",
    "    style I fill:#9999ff\n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "- **W**: Frozen pre-trained weights (d × d)\n",
    "- **A**: Trainable low-rank matrix (d × r)\n",
    "- **B**: Trainable low-rank matrix (r × d)\n",
    "- **r**: Rank (typically 8, 16, 32, 64)\n",
    "- **α**: Scaling factor\n",
    "\n",
    "**Formula:** `h = W₀x + (α/r)·B·A·x`\n",
    "\n",
    "**Parameter Reduction:**\n",
    "- Original: d × d parameters\n",
    "- LoRA: 2 × d × r parameters\n",
    "- Example: d=4096, r=16 → 99.8% reduction!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "### QLoRA Architecture\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[\"Original Model (FP32/FP16)\"] --> B[\"4-bit Quantization (NF4)\"]\n",
    "    \n",
    "    B --> C[\"Quantized Weights (4-bit)\"]\n",
    "    C --> D[\"Double Quantization\"]\n",
    "    D --> E[\"Quantized Constants\"]\n",
    "    \n",
    "    C --> F[\"Dequantize to BF16\"]\n",
    "    \n",
    "    F --> G[\"Forward Pass\"]\n",
    "    G --> H[\"LoRA Adapters (BF16)\"]\n",
    "    \n",
    "    H --> I[\"Gradient Computation\"]\n",
    "    I --> J[\"Paged Optimizer\"]\n",
    "    \n",
    "    J --> K[\"Update LoRA Only\"]\n",
    "    K --> H\n",
    "    \n",
    "    style C fill:#ff9999\n",
    "    style H fill:#99ff99\n",
    "    style J fill:#9999ff\n",
    "```\n",
    "\n",
    "**QLoRA Innovations:**\n",
    "1. **4-bit NormalFloat (NF4)** - Information-theoretically optimal for normal distributions\n",
    "2. **Double Quantization** - Quantize the quantization constants\n",
    "3. **Paged Optimizers** - Handle memory spikes using CPU-GPU paging\n",
    "\n",
    "**Memory Savings:**\n",
    "- 7B model: ~28GB (FP32) → ~3.5GB (4-bit QLoRA)\n",
    "- 13B model: ~52GB (FP32) → ~6.5GB (4-bit QLoRA)\n",
    "- **Can fine-tune 13B on 24GB GPU!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "---\n## PEFT Methods Comparison (2025)\n\n```mermaid\ngraph LR\n    A[PEFT Methods] --> B[Adapter-based]\n    A --> C[Low-Rank]\n    A --> D[Prompt-based]\n    A --> E[Sparse]\n    \n    B --> B1[Adapters]\n    B --> B2[Parallel Adapters]\n    \n    C --> C1[LoRA]\n    C --> C2[QLoRA]\n    C --> C3[AdaLoRA]\n    C --> C4[DoRA]\n    \n    D --> D1[Prefix Tuning]\n    D --> D2[P-Tuning]\n    D --> D3[Prompt Tuning]\n    \n    E --> E1[IA³]\n    E --> E2[BitFit]\n    \n    style C1 fill:#90EE90\n    style C2 fill:#FFD700\n    style C3 fill:#87CEEB\n    style C4 fill:#FFA500\n```\n\n| Method | Trainable Params | Memory | Speed | Best For |\n|--------|-----------------|--------|-------|----------|\n| **LoRA** | 0.1-1% | Medium | Fast | General fine-tuning |\n| **QLoRA** | 0.1-1% | **Very Low** | Fast | Memory-constrained |\n| **AdaLoRA** | 0.05-0.5% | Medium | Medium | Adaptive rank allocation |\n| **DoRA** | 0.05-0.5% | Medium | Medium | More robust hyperparameter tuning |\n| **IA³** | 0.01% | **Lowest** | **Fastest** | Extremely low resource |\n| **Prefix Tuning** | 0.1% | Low | Medium | Few-shot learning |\n| **Full Fine-tuning** | 100% | **Highest** | Slow | Maximum performance |\n\n**Key Insights:**\n- **DoRA (Weight-Decomposed Low-Rank Adaptation)**: Separates magnitude and direction of weight updates, more robust to hyperparameter changes than LoRA\n- **QLoRA Paper Finding**: Very little difference between rank 8 and 256 when LoRA applied to all layers\n- **Target Modules**: Apply LoRA to both Attention AND MLP layers for best performance"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.44.0\n",
    "!pip install -q peft>=0.12.0\n",
    "!pip install -q accelerate>=0.33.0\n",
    "!pip install -q bitsandbytes>=0.43.0\n",
    "!pip install -q datasets>=2.20.0\n",
    "!pip install -q trl>=0.9.0\n",
    "!pip install -q flash-attn --no-build-isolation\n",
    "!pip install -q scipy\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    AdaLoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## Quantization Deep Dive\n",
    "\n",
    "### Quantization Comparison\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Precision Types] --> B[FP32: 32-bit float]\n",
    "    A --> C[FP16: 16-bit float]\n",
    "    A --> D[BF16: 16-bit bfloat]\n",
    "    A --> E[INT8: 8-bit integer]\n",
    "    A --> F[NF4: 4-bit normalfloat]\n",
    "    \n",
    "    B --> B1[\"Range: ±3.4×10³⁸\"]\n",
    "    B --> B2[\"Memory: 4 bytes\"]\n",
    "    \n",
    "    C --> C1[\"Range: ±65,504\"]\n",
    "    C --> C2[\"Memory: 2 bytes\"]\n",
    "    \n",
    "    D --> D1[\"Range: ±3.4×10³⁸\"]\n",
    "    D --> D2[\"Memory: 2 bytes\"]\n",
    "    D --> D3[\"Better for ML\"]\n",
    "    \n",
    "    E --> E1[\"Range: -128 to 127\"]\n",
    "    E --> E2[\"Memory: 1 byte\"]\n",
    "    \n",
    "    F --> F1[\"Optimal for normal dist\"]\n",
    "    F --> F2[\"Memory: 0.5 bytes\"]\n",
    "    F --> F3[\"QLoRA innovation\"]\n",
    "    \n",
    "    style B fill:#ff9999\n",
    "    style D fill:#99ff99\n",
    "    style F fill:#FFD700\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization Configuration Options\n",
    "\n",
    "# 1. 4-bit QLoRA Configuration (Most Memory Efficient)\n",
    "qlora_4bit_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",              # NormalFloat 4-bit\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Computation dtype\n",
    "    bnb_4bit_use_double_quant=True,         # Double quantization\n",
    ")\n",
    "\n",
    "# 2. 8-bit Configuration (Better Accuracy)\n",
    "int8_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    ")\n",
    "\n",
    "# 3. No Quantization (Standard LoRA)\n",
    "no_quant_config = None  # Load in FP16/BF16\n",
    "\n",
    "print(\"Quantization configs defined:\")\n",
    "print(\"1. 4-bit NF4 (QLoRA)\")\n",
    "print(\"2. 8-bit INT8\")\n",
    "print(\"3. No quantization (FP16/BF16)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Quantization Impact\n",
    "\n",
    "| Config | Memory (7B) | Speed | Accuracy | Use Case |\n",
    "|--------|------------|-------|----------|----------|\n",
    "| **FP32** | ~28 GB | 1x | 100% | Baseline |\n",
    "| **FP16** | ~14 GB | 2-3x | 99.9% | Standard training |\n",
    "| **BF16** | ~14 GB | 2-3x | 99.9% | **Recommended** |\n",
    "| **INT8** | ~7 GB | 3-4x | 99.5% | Good balance |\n",
    "| **NF4** | ~3.5 GB | 3-4x | 99%+ | **Memory critical** |\n",
    "\n",
    "**Key Findings (2024-2025):**\n",
    "- QLoRA with 8-bit can **converge faster** than BF16!\n",
    "- 4-bit NF4 maintains performance comparable to 16-bit\n",
    "- Double quantization saves additional 0.4 GB per 7B params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "---\n## MITRE ATT&CK Dataset Preparation\n\n**Note on Production Datasets:**\n\nThis notebook uses 8 synthetic examples for demonstration. For production deployment, use larger, high-quality datasets:\n\n### Recommended Public MITRE TTP Datasets:\n\n1. **AttackQA Dataset** (Recommended - 25,335 samples)\n   - Source: Research paper (arxiv.org/html/2411.01073v1)\n   - Format: Question-answer pairs from MITRE ATT&CK knowledge base\n   - Coverage: Comprehensive across all tactics and techniques\n   - Best for: General MITRE knowledge and instruction tuning\n\n2. **TRAM Dataset** (4,070 labeled sentences from 150 CTI reports)\n   - Source: github.com/center-for-threat-informed-defense/tram\n   - Coverage: 50 out of 625 techniques (limitation!)\n   - Best for: Real-world CTI text understanding\n\n3. **CTI-HAL Dataset** (2025 - Most recent)\n   - Source: Research paper (arxiv.org/html/2504.05866v1)\n   - Quality: Manually annotated with high inter-annotator agreement\n   - Best for: High-quality CTI report mapping\n\n4. **Adversary Emulation Library (AEL)**\n   - Source: MITRE\n   - Format: Concise campaign reports with technique IDs\n   - Best for: Real attack scenarios\n\n**Production Strategy:** Combine AttackQA (25K) + TRAM (4K) + Custom domain examples (500-1K) for robust 30K sample dataset."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MITRE ATT&CK TTP Mapping Dataset\n",
    "# Based on TRAM (Threat Report ATT&CK Mapper) format\n",
    "\n",
    "# Sample training data structure\n",
    "mitre_samples = [\n",
    "    {\n",
    "        \"text\": \"The adversary used PowerShell to execute a malicious script that downloaded additional payloads from a command and control server.\",\n",
    "        \"techniques\": [\"T1059.001\", \"T1105\"],\n",
    "        \"technique_names\": [\"PowerShell\", \"Ingress Tool Transfer\"],\n",
    "        \"tactics\": [\"Execution\", \"Command and Control\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Attackers gained initial access through a spear-phishing email containing a malicious attachment that exploited a vulnerability in Microsoft Office.\",\n",
    "        \"techniques\": [\"T1566.001\", \"T1203\"],\n",
    "        \"technique_names\": [\"Spearphishing Attachment\", \"Exploitation for Client Execution\"],\n",
    "        \"tactics\": [\"Initial Access\", \"Execution\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The threat actor created a scheduled task to maintain persistence on the compromised system, executing a backdoor every hour.\",\n",
    "        \"techniques\": [\"T1053.005\"],\n",
    "        \"technique_names\": [\"Scheduled Task\"],\n",
    "        \"tactics\": [\"Persistence\", \"Privilege Escalation\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Credential dumping was performed using Mimikatz to extract plaintext passwords and NTLM hashes from LSASS memory.\",\n",
    "        \"techniques\": [\"T1003.001\"],\n",
    "        \"technique_names\": [\"LSASS Memory\"],\n",
    "        \"tactics\": [\"Credential Access\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The malware enumerated all running processes and network connections to identify security tools and establish situational awareness.\",\n",
    "        \"techniques\": [\"T1057\", \"T1049\"],\n",
    "        \"technique_names\": [\"Process Discovery\", \"System Network Connections Discovery\"],\n",
    "        \"tactics\": [\"Discovery\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Sensitive files were collected and compressed into an archive before exfiltration over the encrypted C2 channel.\",\n",
    "        \"techniques\": [\"T1560\", \"T1041\"],\n",
    "        \"technique_names\": [\"Archive Collected Data\", \"Exfiltration Over C2 Channel\"],\n",
    "        \"tactics\": [\"Collection\", \"Exfiltration\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Ransomware was deployed across the network using PsExec to encrypt files with AES-256 encryption.\",\n",
    "        \"techniques\": [\"T1486\", \"T1021.002\"],\n",
    "        \"technique_names\": [\"Data Encrypted for Impact\", \"SMB/Windows Admin Shares\"],\n",
    "        \"tactics\": [\"Impact\", \"Lateral Movement\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The attacker disabled Windows Defender and tampered with event logs to evade detection and hide their activities.\",\n",
    "        \"techniques\": [\"T1562.001\", \"T1070.001\"],\n",
    "        \"technique_names\": [\"Disable or Modify Tools\", \"Clear Windows Event Logs\"],\n",
    "        \"tactics\": [\"Defense Evasion\"]\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Sample dataset size: {len(mitre_samples)} examples\")\n",
    "print(\"\\nExample:\")\n",
    "print(json.dumps(mitre_samples[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instruction-following format for LLM fine-tuning\n",
    "\n",
    "def format_mitre_example(example: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Format MITRE example for instruction fine-tuning\n",
    "    \"\"\"\n",
    "    instruction = \"Analyze the following cybersecurity incident description and identify the MITRE ATT&CK techniques (TTPs) used by the adversary.\"\n",
    "    \n",
    "    # Format techniques\n",
    "    techniques_str = \", \".join([\n",
    "        f\"{tech} ({name})\" \n",
    "        for tech, name in zip(example['techniques'], example['technique_names'])\n",
    "    ])\n",
    "    \n",
    "    tactics_str = \", \".join(example['tactics'])\n",
    "    \n",
    "    formatted = f\"\"\"<|im_start|>system\n",
    "You are a cybersecurity expert trained in MITRE ATT&CK framework. Analyze threat intelligence and map it to specific TTPs.<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}\n",
    "\n",
    "Incident Description:\n",
    "{example['text']}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Based on the incident description, the following MITRE ATT&CK techniques were identified:\n",
    "\n",
    "**Techniques:** {techniques_str}\n",
    "\n",
    "**Tactics:** {tactics_str}\n",
    "\n",
    "**Analysis:** The adversary employed these techniques to achieve their objectives, demonstrating a multi-stage attack pattern across the cyber kill chain.<|im_end|>\"\"\"\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "# Format dataset\n",
    "formatted_dataset = [\n",
    "    {\"text\": format_mitre_example(example)}\n",
    "    for example in mitre_samples\n",
    "]\n",
    "\n",
    "print(\"Formatted example:\")\n",
    "print(formatted_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_list(formatted_dataset)\n",
    "\n",
    "print(f\"Dataset created with {len(train_dataset)} examples\")\n",
    "print(f\"\\nDataset features: {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Loading with Different Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection: Qwen2.5-1.5B (state-of-the-art 1B model)\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",\n",
    ")\n",
    "\n",
    "# Set pad token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded: {MODEL_NAME}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "# Load model with 4-bit quantization (QLoRA)\n\n# Note on Flash Attention 2:\n# - Requires Ampere/Ada/Hopper GPU architecture (RTX 30xx/40xx, A100, H100)\n# - Not available on older GPUs (V100, T4, GTX series)\n# - Will fall back to standard attention if unavailable\n# - Remove attn_implementation if you encounter errors\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=qlora_4bit_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\",  # Use Flash Attention 2 (2-4x speedup)\n)\n\n# Prepare for k-bit training\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n\nprint(f\"\\nModel loaded with 4-bit quantization\")\nprint(f\"Device map: {model.hf_device_map}\")\n\n# Calculate trainable parameters\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params:,} || \"\n        f\"all params: {all_param:,} || \"\n        f\"trainable%: {100 * trainable_params / all_param:.4f}\"\n    )\n\nprint(\"\\nBefore adding LoRA:\")\nprint_trainable_parameters(model)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## LoRA Configuration Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Standard LoRA Configuration\nlora_config = LoraConfig(\n    r=16,                                    # Rank\n    lora_alpha=16,                           # Scaling factor (alpha=r is standard, alpha=r/2 for stronger effect)\n    target_modules=[                         # Which modules to apply LoRA\n        \"q_proj\",        # Query projection\n        \"k_proj\",        # Key projection\n        \"v_proj\",        # Value projection\n        \"o_proj\",        # Output projection\n        \"gate_proj\",     # MLP gate projection\n        \"up_proj\",       # MLP up projection\n        \"down_proj\",     # MLP down projection\n    ],\n    lora_dropout=0.05,                       # Dropout for LoRA layers\n    bias=\"none\",                             # Bias strategy\n    task_type=TaskType.CAUSAL_LM,            # Task type\n    inference_mode=False,                     # Training mode\n)\n\nprint(\"LoRA Configuration:\")\nprint(f\"  Rank (r): {lora_config.r}\")\nprint(f\"  Alpha: {lora_config.lora_alpha}\")\nprint(f\"  Alpha/r ratio: {lora_config.lora_alpha / lora_config.r}\")\nprint(f\"  Target modules: {lora_config.target_modules}\")\nprint(f\"  Dropout: {lora_config.lora_dropout}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# Alternative: AdaLoRA Configuration (Adaptive Rank)\nadalora_config = AdaLoraConfig(\n    r=16,\n    lora_alpha=16,                           # Fixed: alpha=r (was 32)\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n    ],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n    # AdaLoRA specific parameters\n    target_r=8,                              # Target average rank\n    init_r=12,                               # Initial rank\n    tinit=0,                                 # Start of rank adaptation\n    tfinal=1000,                             # End of rank adaptation\n    deltaT=10,                               # Rank update frequency\n)\n\nprint(\"\\nAdaLoRA Configuration (adaptive rank allocation):\")\nprint(f\"  Initial rank: {adalora_config.init_r}\")\nprint(f\"  Target rank: {adalora_config.target_r}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\nAfter adding LoRA:\")\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Print model structure\n",
    "print(\"\\nLoRA adapter structure:\")\n",
    "for name, module in model.named_modules():\n",
    "    if \"lora\" in name.lower():\n",
    "        print(f\"  {name}: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Configuration\n",
    "\n",
    "### Mixed Precision Training\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Mixed Precision Training] --> B[FP16]\n",
    "    A --> C[BF16]\n",
    "    A --> D[FP8]\n",
    "    \n",
    "    B --> B1[\"Standard mixed precision\"]\n",
    "    B --> B2[\"Loss scaling required\"]\n",
    "    B --> B3[\"Range: ±65,504\"]\n",
    "    \n",
    "    C --> C1[\"Better for ML/LLMs\"]\n",
    "    C --> C2[\"No loss scaling\"]\n",
    "    C --> C3[\"Range: ±3.4×10³⁸\"]\n",
    "    C --> C4[\"Recommended\"]\n",
    "    \n",
    "    D --> D1[\"Newest (2024+)\"]\n",
    "    D --> D2[\"H100/A100 GPUs\"]\n",
    "    D --> D3[\"2x faster\"]\n",
    "    \n",
    "    style C fill:#90EE90\n",
    "    style C4 fill:#FFD700\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments with SOTA optimizations\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=\"./qwen-mitre-qlora\",\n",
    "    run_name=\"qwen-1.5b-mitre-qlora\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,          # Effective batch size = 16\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Mixed precision\n",
    "    bf16=True,                               # Use BF16 (recommended)\n",
    "    tf32=True,                               # Use TF32 on Ampere GPUs\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"paged_adamw_8bit\",               # Paged optimizer for QLoRA\n",
    "    gradient_checkpointing=True,             # Save memory\n",
    "    max_grad_norm=1.0,                       # Gradient clipping\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",                        # Change to \"wandb\" for W&B logging\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"no\",\n",
    "    \n",
    "    # Performance\n",
    "    dataloader_num_workers=4,\n",
    "    group_by_length=True,                    # Group similar lengths\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Optimizer: {training_args.optim}\")\n",
    "print(f\"  Precision: BF16={training_args.bf16}, TF32={training_args.tf32}\")\n",
    "print(f\"  Gradient checkpointing: {training_args.gradient_checkpointing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Optimizer Comparison\n",
    "\n",
    "| Optimizer | Memory | Speed | Convergence | Use Case |\n",
    "|-----------|--------|-------|-------------|----------|\n",
    "| **AdamW** | High | Fast | Good | Standard |\n",
    "| **AdamW 8-bit** | Medium | Fast | Good | Memory saving |\n",
    "| **Paged AdamW 8-bit** | **Low** | Fast | Good | **QLoRA** |\n",
    "| **AdaFactor** | Very Low | Medium | Variable | Extreme memory constraints |\n",
    "| **Lion** | Low | **Fastest** | Good | New (2024+) |\n",
    "| **SGD** | Lowest | Slow | Poor | Not recommended for LLMs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text examples\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset: {tokenized_dataset}\")\n",
    "print(f\"\\nExample tokenized sequence length: {len(tokenized_dataset[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"Data collator created for causal language modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")\n",
    "print(f\"\\nTraining will start with:\")\n",
    "print(f\"  - {len(tokenized_dataset)} training examples\")\n",
    "print(f\"  - {training_args.num_train_epochs} epochs\")\n",
    "print(f\"  - Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  - Gradient accumulation: {training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "# Note: This is a demo with small dataset\n",
    "# For production, use larger MITRE TTP dataset from:\n",
    "# - github.com/tumeteor/mitre-ttp-mapping\n",
    "# - TRAM annotated dataset\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "output_dir = \"./qwen-mitre-qlora-final\"\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")\n",
    "print(\"\\nSaved files:\")\n",
    "!ls -lh {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "---\n",
    "## Inference and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "\n",
    "test_prompt = \"\"\"<|im_start|>system\n",
    "You are a cybersecurity expert trained in MITRE ATT&CK framework. Analyze threat intelligence and map it to specific TTPs.<|im_end|>\n",
    "<|im_start|>user\n",
    "Analyze the following cybersecurity incident description and identify the MITRE ATT&CK techniques (TTPs) used by the adversary.\n",
    "\n",
    "Incident Description:\n",
    "The attacker established persistence by modifying the Windows Registry Run key to execute a malicious DLL every time the user logs in. They also used WMI event subscriptions as a backup persistence mechanism.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "print(\"Generated Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "---\n",
    "## Advanced Topics\n",
    "\n",
    "### 1. Flash Attention 2\n",
    "\n",
    "```python\n",
    "# Flash Attention provides 2-4x speedup\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    attn_implementation=\"flash_attention_2\",  # Enable Flash Attention\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- 2-4x faster attention computation\n",
    "- Lower memory usage\n",
    "- Exact attention (not approximate)\n",
    "\n",
    "### 2. Gradient Checkpointing\n",
    "\n",
    "```python\n",
    "# Trade computation for memory\n",
    "model.gradient_checkpointing_enable()\n",
    "```\n",
    "\n",
    "**Impact:**\n",
    "- Reduces memory by ~40%\n",
    "- Increases training time by ~20%\n",
    "- Essential for large batch sizes\n",
    "\n",
    "### 3. DeepSpeed Integration\n",
    "\n",
    "```python\n",
    "# For multi-GPU training\n",
    "training_args = TrainingArguments(\n",
    "    ...\n",
    "    deepspeed=\"ds_config_zero3.json\",\n",
    ")\n",
    "```\n",
    "\n",
    "**ZeRO Stages:**\n",
    "- ZeRO-1: Optimizer state partitioning\n",
    "- ZeRO-2: + Gradient partitioning\n",
    "- ZeRO-3: + Parameter partitioning (most memory efficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "---\n",
    "## Memory Optimization Summary\n",
    "\n",
    "### Memory Reduction Techniques\n",
    "\n",
    "| Technique | Memory Saved | Speed Impact | Accuracy Impact |\n",
    "|-----------|-------------|--------------|----------------|\n",
    "| **4-bit Quantization** | 75% | None | <1% |\n",
    "| **8-bit Quantization** | 50% | None | <0.1% |\n",
    "| **Gradient Checkpointing** | 40% | -20% | None |\n",
    "| **Flash Attention 2** | 20% | +200% | None |\n",
    "| **LoRA (r=16)** | Minimal | +10% | Task-dependent |\n",
    "| **Paged Optimizer** | 10% | -5% | None |\n",
    "\n",
    "### Combined Impact (QLoRA)\n",
    "\n",
    "Combining all techniques:\n",
    "- **Total memory reduction: ~85%**\n",
    "- **Speed: Comparable or faster**\n",
    "- **Accuracy: 99%+ of full fine-tuning**\n",
    "\n",
    "**Example: 7B Model**\n",
    "- Full fine-tuning: ~60 GB (A100 required)\n",
    "- QLoRA: ~9 GB (RTX 3090/4090 sufficient!)\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "**For 24GB GPU (RTX 3090/4090):**\n",
    "- Use 4-bit QLoRA\n",
    "- Max model size: ~13B parameters\n",
    "- Batch size: 4-8 with gradient accumulation\n",
    "\n",
    "**For 16GB GPU (RTX 4080):**\n",
    "- Use 4-bit QLoRA\n",
    "- Max model size: ~7B parameters\n",
    "- Batch size: 2-4 with gradient accumulation\n",
    "\n",
    "**For 8GB GPU (RTX 3070):**\n",
    "- Use 4-bit QLoRA\n",
    "- Max model size: ~3B parameters\n",
    "- Batch size: 1 with gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "---\n",
    "## Rank Selection Guidelines\n",
    "\n",
    "### LoRA Rank Impact\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[LoRA Rank r] --> B[r=4]\n",
    "    A --> C[r=8]\n",
    "    A --> D[r=16]\n",
    "    A --> E[r=32]\n",
    "    A --> F[r=64]\n",
    "    \n",
    "    B --> B1[\"Params: 0.01%\"]\n",
    "    B --> B2[\"Fast training\"]\n",
    "    B --> B3[\"Limited capacity\"]\n",
    "    \n",
    "    C --> C1[\"Params: 0.02%\"]\n",
    "    C --> C2[\"Good for simple tasks\"]\n",
    "    \n",
    "    D --> D1[\"Params: 0.05%\"]\n",
    "    D --> D2[\"Recommended default\"]\n",
    "    D --> D3[\"Good balance\"]\n",
    "    \n",
    "    E --> E1[\"Params: 0.1%\"]\n",
    "    E --> E2[\"Complex tasks\"]\n",
    "    \n",
    "    F --> F1[\"Params: 0.2%\"]\n",
    "    F --> F2[\"Maximum capacity\"]\n",
    "    F --> F3[\"Risk of overfitting\"]\n",
    "    \n",
    "    style D fill:#90EE90\n",
    "    style D2 fill:#FFD700\n",
    "```\n",
    "\n",
    "### Rank Selection Guide\n",
    "\n",
    "| Task Complexity | Dataset Size | Recommended Rank |\n",
    "|----------------|-------------|------------------|\n",
    "| Simple (classification) | <1K samples | r=4-8 |\n",
    "| Medium (NER, QA) | 1K-10K | r=8-16 |\n",
    "| Complex (generation) | 10K-100K | r=16-32 |\n",
    "| Very complex (chat) | 100K+ | r=32-64 |\n",
    "\n",
    "**MITRE TTP Mapping:**\n",
    "- Multi-label classification\n",
    "- 600+ classes (hierarchical)\n",
    "- **Recommended: r=16-32**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fss96do21ji",
   "source": "---\n## Advanced Theoretical Concepts\n\n### 1. LoRA Hyperparameter Deep Dive\n\n#### Alpha Scaling Factor - Common Misconceptions\n\n**Formula Reminder:** `h = W₀x + (α/r)·B·A·x`\n\n**Key Research Finding (QLoRA Paper 2024):**\n- Alpha at **r/2** (50% of rank) and **r/4** (25% of rank) achieved excellent results\n- **Lower alpha relative to rank = STRONGER fine-tuning effect**\n- **Higher alpha relative to rank = WEAKER fine-tuning effect**\n\n**Recommended Alpha Values:**\n\n| Rank (r) | Alpha | Ratio | Effect | Use Case |\n|----------|-------|-------|--------|----------|\n| 8 | 4 | 0.5 | Strong | Small datasets, aggressive adaptation |\n| 8 | 8 | 1.0 | **Standard** | **Recommended default** |\n| 8 | 16 | 2.0 | Weak | Large datasets, subtle changes |\n| 16 | 8 | 0.5 | Strong | Domain shift |\n| 16 | 16 | 1.0 | **Standard** | **Recommended default** |\n| 16 | 32 | 2.0 | Weak | Minor refinement |\n\n**Practical Guidelines:**\n- Start with `alpha = r` (ratio = 1.0)\n- If model not adapting enough, reduce alpha to `r/2`\n- If model overfitting, increase alpha to `2*r`\n- Contrary to popular belief, `alpha = 2*r` is NOT typical - it's conservative!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "zlmgcfbnrll",
   "source": "# Example: Testing different alpha values\n\ndef create_lora_config_with_alpha(r=16, alpha_ratio=1.0):\n    \"\"\"\n    Create LoRA config with specified alpha ratio\n    \n    Args:\n        r: Rank\n        alpha_ratio: Ratio of alpha to rank (0.5, 1.0, or 2.0)\n    \"\"\"\n    alpha = int(r * alpha_ratio)\n    \n    config = LoraConfig(\n        r=r,\n        lora_alpha=alpha,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=TaskType.CAUSAL_LM,\n    )\n    \n    print(f\"Rank: {r}, Alpha: {alpha}, Ratio: {alpha_ratio} - Scaling factor: {alpha/r}\")\n    return config\n\n# Standard configuration (recommended)\nstandard_config = create_lora_config_with_alpha(r=16, alpha_ratio=1.0)\n\n# Strong fine-tuning (for difficult domain adaptation)\nstrong_config = create_lora_config_with_alpha(r=16, alpha_ratio=0.5)\n\n# Conservative fine-tuning (for minor refinements)\nconservative_config = create_lora_config_with_alpha(r=16, alpha_ratio=2.0)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wsutbg3b51f",
   "source": "---\n## Key Takeaways (Updated with Latest Best Practices)\n\n### LoRA vs QLoRA\n- **LoRA**: Fast, efficient, 0.1-1% trainable parameters\n- **QLoRA**: Same as LoRA + 75% memory reduction via 4-bit quantization\n- **Choice**: Use QLoRA when memory-constrained, LoRA otherwise\n\n### Critical Hyperparameter Insights\n\n**Alpha Scaling (CORRECTED):**\n- **NOT** `alpha = 2*r` as commonly stated\n- **Standard**: `alpha = r` (ratio = 1.0)\n- **Stronger effect**: `alpha = r/2` (ratio = 0.5) - for aggressive adaptation\n- **Weaker effect**: `alpha = 2*r` (ratio = 2.0) - for conservative refinement\n- **Research-backed**: QLoRA paper shows alpha=r/2 and alpha=r work excellently\n\n**Rank Selection (IMPORTANT):**\n- **Surprising finding**: Very little difference between r=8 and r=256 when applied to all layers\n- **Effective rank** is often much lower than specified rank\n- **Start with r=8**, only increase if performance plateaus\n- **Don't waste compute**: r=16 is usually more than enough\n\n**Target Modules:**\n- Apply LoRA to **both Attention AND MLP layers** for best performance\n- Targeting only attention saves memory but reduces capacity\n- This notebook targets all 7 key modules (q, k, v, o, gate, up, down)\n\n### Quantization\n- **4-bit NF4**: Best memory efficiency (75% reduction), <1% accuracy loss\n- **8-bit INT8**: Good balance (50% reduction), can converge faster than BF16\n- **Impact**: Minimal accuracy loss, sometimes even better convergence\n- **Double quantization**: Additional 0.4 GB savings per 7B params\n\n### PEFT Methods (2025)\n- **LoRA**: Most versatile, widely supported, proven\n- **AdaLoRA**: Adaptive rank allocation, can achieve same performance with fewer params\n- **DoRA**: Weight-Decomposed Low-Rank Adaptation, more robust to hyperparameters\n- **QLoRA**: LoRA + 4-bit quantization, enables fine-tuning on consumer GPUs\n- **IA³**: Ultra-efficient (0.01% params), but limited capacity\n\n### Training Optimizations\n- **Mixed Precision**: BF16 recommended for LLMs (no loss scaling needed, large range)\n- **Flash Attention 2**: 2-4x speedup, requires Ampere/Ada/Hopper GPUs\n- **Gradient Checkpointing**: 40% memory reduction, 20% time increase\n- **Paged Optimizer**: Essential for QLoRA, handles memory spikes\n\n### Best Practices for Production\n\n1. **Always use train/validation split** (85/15 recommended)\n2. **Enable early stopping** (patience=3 evaluations)\n3. **Mix in general examples** (20%) to prevent catastrophic forgetting\n4. **Use proper evaluation metrics** for multi-label classification\n5. **Start with r=8, alpha=8** before trying larger ranks\n6. **Merge adapters** for production deployment (faster inference)\n\n### For MITRE TTP Mapping\n- **Model**: Qwen 2.5-1.5B (32K context beats Gemma's 8K)\n- **Config**: 4-bit QLoRA, r=16, alpha=16\n- **Data**: AttackQA (25K) + TRAM (4K) + custom (1K) = 30K samples\n- **Evaluation**: Exact match ratio, precision/recall, Jaccard similarity\n- **Deployment**: Merge adapters, use vLLM or TGI\n\n### Production Deployment\n1. Fine-tune with QLoRA on consumer GPU (RTX 3090/4090)\n2. Merge adapters into base model\n3. Optionally quantize merged model to 4-bit/8-bit\n4. Deploy with vLLM (10-20x faster) or TGI\n5. Monitor performance drift, retrain with new MITRE versions\n\n### Memory Requirements\n- **24GB GPU**: Can fine-tune up to 13B models with 4-bit QLoRA\n- **16GB GPU**: Can fine-tune up to 7B models with 4-bit QLoRA\n- **8GB GPU**: Can fine-tune up to 3B models with 4-bit QLoRA\n- Use gradient accumulation for effective larger batch sizes\n\n### Common Mistakes to Avoid\n1. Setting `alpha = 2*r` without understanding the effect (too conservative)\n2. Using very high rank (r=256) without testing r=8 first (wasted compute)\n3. No validation set (can't detect overfitting)\n4. Training too long (catastrophic forgetting)\n5. Not applying LoRA to MLP layers (limited capacity)\n6. Ignoring Flash Attention warnings (falls back silently on old GPUs)\n\nThis notebook reflects 2024-2025 best practices based on latest research!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "c7tf6nabxs8",
   "source": "---\n## Model Comparison: Qwen 2.5-1.5B vs Gemma 2-2B\n\n### Why Qwen 2.5-1.5B Was Chosen for This Notebook\n\n| Feature | Qwen 2.5-1.5B | Gemma 2-2B | Winner |\n|---------|---------------|------------|--------|\n| **Parameters** | 1.54B | 2.6B | Gemma (larger) |\n| **Context Length** | **32K tokens** | 8K tokens | **Qwen** |\n| **Training Data** | 18T tokens | ~6T tokens | **Qwen** |\n| **Math/Coding** | Excellent | Good | **Qwen** |\n| **License** | Apache 2.0 | Gemma Terms | **Qwen** (more permissive) |\n| **Efficiency** | Better (smaller) | Needs more resources | **Qwen** |\n| **Architecture** | Qwen 2.5 | Gemma 2 | Both modern |\n| **Release** | Sept 2024 | Feb 2024 | Qwen (newer) |\n\n### Performance on General Benchmarks\n\n**Qwen 2.5-1.5B-Instruct:**\n- MMLU: 61.4%\n- HumanEval (coding): 37.8%\n- GSM8K (math): 63.0%\n- Overall: Strong all-around performance\n\n**Gemma 2-2B-Instruct:**\n- MMLU: ~55-58%\n- HumanEval (coding): ~30-35%\n- GSM8K (math): ~45-50%\n- Overall: Good but slightly behind\n\n### Cybersecurity-Specific Considerations\n\n**For MITRE TTP Mapping:**\n\n1. **Context Length is Critical**\n   - CTI reports often 5K-20K tokens\n   - Qwen's 32K context >> Gemma's 8K\n   - **Verdict: Qwen wins for long documents**\n\n2. **Structured Output Quality**\n   - Both support JSON mode\n   - Qwen slightly better at following formats\n   - **Verdict: Slight edge to Qwen**\n\n3. **Training Data Diversity**\n   - Qwen: 18T tokens, multilingual\n   - Gemma: Smaller dataset\n   - **Verdict: Qwen has more diverse knowledge**\n\n4. **Fine-tuning Efficiency**\n   - Qwen: 1.54B params = faster training\n   - Gemma: 2.6B params = 70% more memory\n   - **Verdict: Qwen more efficient**\n\n### When to Choose Gemma 2-2B Instead\n\n**Consider Gemma if:**\n- You're in Google ecosystem (Vertex AI, Colab)\n- You need stronger safety filtering\n- 8K context is sufficient for your use case\n- You prefer Google's research lineage\n- You want slightly more capacity (2.6B params)\n\n**Stick with Qwen if:**\n- Long CTI reports (>8K tokens)\n- Need Apache 2.0 licensing\n- Want better coding/structured output\n- Prefer latest model (Sept 2024 vs Feb 2024)\n- Need maximum efficiency\n\n### Alternative Models Worth Considering\n\n1. **Llama 3.2-3B-Instruct**\n   - 3B params, 128K context\n   - Strong general performance\n   - Trade-off: Larger, slower\n\n2. **Phi-4 (3.8B)**\n   - Excellent reasoning\n   - 16K context\n   - Trade-off: Much larger\n\n3. **SecurityLLM-8B** (Purpose-built for cybersecurity)\n   - Source: arxiv.org/html/2504.21039\n   - Pre-trained on security corpus\n   - Trade-off: Less general purpose\n\n### Final Recommendation\n\n**For MITRE TTP Mapping: Stick with Qwen 2.5-1.5B**\n\nReasons:\n- 32K context handles full CTI reports\n- Better coding ability for structured JSON output\n- More efficient fine-tuning (1.54B vs 2.6B)\n- Newer model with latest training techniques\n- Apache 2.0 license (no restrictions)\n\nThe 32K context window is the **decisive factor** - most CTI reports exceed 8K tokens when formatted with system prompts and examples.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "z0zsjbzglia",
   "source": "### 3. Learning Rate Schedules Explained\n\n**Why Cosine Schedule Works Well for Fine-tuning:**\n\n```python\n# Learning rate over time with different schedules\n# Epoch:    1      2      3\n# Cosine:  2e-4 → 1e-4 → 2e-5  (smooth decay)\n# Linear:  2e-4 → 1.3e-4 → 6e-5  (linear decay)\n# Constant: 2e-4 → 2e-4 → 2e-4  (no decay)\n```\n\n**Schedule Comparison:**\n\n| Schedule | Best For | Pros | Cons |\n|----------|----------|------|------|\n| **Cosine** | Fine-tuning, instruction tuning | Smooth convergence, avoids sudden drops | Slightly slower initial learning |\n| **Linear** | Short training runs | Simple, predictable | Can be too aggressive |\n| **Constant + Warmup** | Continued pretraining | Maintains high LR longer | Risk of overshooting |\n| **Inverse Sqrt** | Very long training | Proven for transformers | Not ideal for short fine-tuning |\n| **Polynomial** | Custom decay rate | Flexible | Requires tuning |\n\n**Warmup Phase Importance:**\n\nWarmup gradually increases LR from 0 to target over first N steps:\n- Prevents large gradient updates early\n- Stabilizes training with quantized weights\n- Especially important for QLoRA (4-bit weights)\n\n```python\n# Recommended warmup\nwarmup_ratio=0.1    # 10% of total steps\n# For 1000 total steps: 100 warmup steps\n# LR goes: 0 → 2e-5 → 2e-4 (over first 100 steps)\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "wt7bq3scgr",
   "source": "### 4. Catastrophic Forgetting in Fine-tuning\n\n**The Problem:**\n\nWhen fine-tuning on narrow domains (like MITRE TTPs), models can \"forget\" general capabilities:\n\n```\nBefore fine-tuning: \"What is Python?\" → Detailed, accurate answer\nAfter fine-tuning:  \"What is Python?\" → Tries to map to MITRE technique!\n```\n\n**Why it happens:**\n- Small dataset pushes weights toward specific domain\n- General knowledge connections weakened\n- More severe with higher learning rates and longer training\n\n**Mitigation Strategies:**\n\n#### Strategy 1: Mix in General Examples (Recommended)\n\n```python\n# 80% domain-specific + 20% general instruction data\nmitre_samples = load_mitre_dataset()      # 8,000 samples\ngeneral_samples = load_general_qa()       # 2,000 samples\ncombined = mitre_samples + general_samples\n\n# This maintains general capability while adapting to domain\n```\n\n#### Strategy 2: Lower Learning Rate\n\n```python\n# More conservative adaptation\nlearning_rate=5e-5  # Instead of 2e-4\n# Takes longer but preserves more general knowledge\n```\n\n#### Strategy 3: Shorter Training\n\n```python\n# Don't overtrain\nnum_epochs=1-2  # Instead of 3-5\n# Monitor when validation performance plateaus\n```\n\n#### Strategy 4: Regularization via LoRA Alpha\n\n```python\n# Higher alpha = more conservative\nlora_alpha=32  # With r=16 (ratio=2.0)\n# Weights change more gradually\n```\n\n**Evaluation:**\n\nAlways test on both domain-specific AND general benchmarks:\n\n```python\n# Domain performance\nttp_accuracy = evaluate_mitre_ttps(model)\n\n# General capability (detect forgetting)\ngeneral_qa_accuracy = evaluate_general_qa(model)\ncoding_ability = evaluate_code_generation(model)\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "hbo3uqzzqxb",
   "source": "---\n## Best Practices for Production Fine-tuning\n\n### 1. Proper Train/Validation Split with Early Stopping",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "a0hdpgp92hs",
   "source": "# Create proper train/validation split\nfrom datasets import Dataset\n\n# Combine all data\nall_formatted_data = [\n    {\"text\": format_mitre_example(example)}\n    for example in mitre_samples\n]\n\n# Create dataset and split\ndataset = Dataset.from_list(all_formatted_data)\nsplit_dataset = dataset.train_test_split(test_size=0.15, seed=42)\n\ntrain_dataset_split = split_dataset['train']\nval_dataset_split = split_dataset['test']\n\nprint(f\"Training samples: {len(train_dataset_split)}\")\nprint(f\"Validation samples: {len(val_dataset_split)}\")\nprint(f\"Validation ratio: {len(val_dataset_split) / len(dataset):.1%}\")\n\n# Tokenize both splits\ntrain_tokenized = train_dataset_split.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"text\"],\n)\n\nval_tokenized = val_dataset_split.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"text\"],\n)\n\nprint(\"\\nDatasets ready for training with validation\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "yw2xy898dpe",
   "source": "# Training with validation and early stopping\n\nfrom transformers import EarlyStoppingCallback\n\ntraining_args_with_validation = TrainingArguments(\n    output_dir=\"./qwen-mitre-qlora-validated\",\n    run_name=\"qwen-mitre-with-validation\",\n    \n    # Training\n    num_train_epochs=10,                     # More epochs, but will stop early\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.1,\n    \n    # Mixed precision\n    bf16=True,\n    tf32=True,\n    \n    # Optimization\n    optim=\"paged_adamw_8bit\",\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n    \n    # Logging\n    logging_steps=10,\n    logging_dir=\"./logs\",\n    report_to=\"none\",\n    \n    # Evaluation and early stopping\n    eval_strategy=\"steps\",                   # Evaluate every N steps\n    eval_steps=50,                           # Evaluate every 50 steps\n    save_strategy=\"steps\",\n    save_steps=50,\n    save_total_limit=3,                      # Keep only best 3 checkpoints\n    load_best_model_at_end=True,             # Load best model when training ends\n    metric_for_best_model=\"eval_loss\",       # Use validation loss as metric\n    greater_is_better=False,                 # Lower loss is better\n    \n    # Performance\n    dataloader_num_workers=4,\n    group_by_length=True,\n    seed=42,\n)\n\n# Trainer with early stopping\ntrainer_with_validation = Trainer(\n    model=model,\n    args=training_args_with_validation,\n    train_dataset=train_tokenized,\n    eval_dataset=val_tokenized,              # Add validation dataset\n    data_collator=data_collator,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Stop if no improvement for 3 evals\n)\n\nprint(\"Trainer configured with validation and early stopping\")\nprint(f\"Will stop training if validation loss doesn't improve for {3 * 50} steps\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "77tb2t1w2yx",
   "source": "### 2. Evaluation Metrics for Multi-label Classification (MITRE TTPs)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ujjw9kmphf",
   "source": "# Evaluation metrics for MITRE TTP mapping\n\nfrom sklearn.metrics import (\n    precision_recall_fscore_support,\n    hamming_loss,\n    accuracy_score,\n    jaccard_score\n)\nimport numpy as np\n\ndef evaluate_ttp_predictions(y_true, y_pred, technique_names=None):\n    \"\"\"\n    Comprehensive evaluation for multi-label TTP classification\n    \n    Args:\n        y_true: Ground truth binary matrix (n_samples, n_techniques)\n        y_pred: Predicted binary matrix (n_samples, n_techniques)\n        technique_names: Optional list of technique names for per-class metrics\n    \n    Returns:\n        Dictionary of metrics\n    \"\"\"\n    \n    # Exact match ratio (all techniques must match)\n    exact_match = accuracy_score(y_true, y_pred)\n    \n    # Micro-averaged metrics (treat each technique prediction equally)\n    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n        y_true, y_pred, average='micro', zero_division=0\n    )\n    \n    # Macro-averaged metrics (average per technique, then average)\n    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n        y_true, y_pred, average='macro', zero_division=0\n    )\n    \n    # Hamming loss (fraction of wrong labels)\n    h_loss = hamming_loss(y_true, y_pred)\n    \n    # Jaccard similarity (intersection over union)\n    jaccard = jaccard_score(y_true, y_pred, average='samples', zero_division=0)\n    \n    metrics = {\n        'exact_match_ratio': exact_match,\n        'precision_micro': precision_micro,\n        'recall_micro': recall_micro,\n        'f1_micro': f1_micro,\n        'precision_macro': precision_macro,\n        'recall_macro': recall_macro,\n        'f1_macro': f1_macro,\n        'hamming_loss': h_loss,\n        'jaccard_similarity': jaccard,\n    }\n    \n    # Per-technique metrics (if names provided)\n    if technique_names:\n        precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n            y_true, y_pred, average=None, zero_division=0\n        )\n        \n        print(\"\\nPer-Technique Metrics:\")\n        print(f\"{'Technique':<20} {'Precision':>10} {'Recall':>10} {'F1':>10} {'Support':>10}\")\n        print(\"-\" * 65)\n        for i, name in enumerate(technique_names):\n            print(f\"{name:<20} {precision_per_class[i]:>10.3f} {recall_per_class[i]:>10.3f} \"\n                  f\"{f1_per_class[i]:>10.3f} {int(support[i]):>10}\")\n    \n    return metrics\n\n# Example usage\nprint(\"Evaluation Metrics for MITRE TTP Mapping:\\n\")\nprint(\"1. Exact Match Ratio: % of samples where ALL techniques are correctly predicted\")\nprint(\"2. Precision (Micro): Overall precision across all technique predictions\")\nprint(\"3. Recall (Micro): Overall recall across all technique predictions\")\nprint(\"4. F1 (Micro): Harmonic mean of precision and recall\")\nprint(\"5. Hamming Loss: Fraction of incorrect technique labels\")\nprint(\"6. Jaccard Similarity: Average IoU between predicted and true technique sets\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e1c051okb7t",
   "source": "### 3. Merging LoRA Adapters for Production Deployment",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cpiaqu7lu9",
   "source": "# Merging LoRA adapters into base model for faster inference\n\nfrom peft import PeftModel\n\n# Option 1: Merge adapters (creates single model, faster inference)\n# Load base model\nbase_model_for_merge = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16,\n)\n\n# Load LoRA adapters\npeft_model = PeftModel.from_pretrained(\n    base_model_for_merge,\n    \"./qwen-mitre-qlora-final\"  # Path to saved adapters\n)\n\n# Merge adapters into base weights\nmerged_model = peft_model.merge_and_unload()\n\n# Save merged model\nmerged_model.save_pretrained(\"./qwen-mitre-merged\")\ntokenizer.save_pretrained(\"./qwen-mitre-merged\")\n\nprint(\"Merged model saved to ./qwen-mitre-merged\")\nprint(\"\\nBenefits of merging:\")\nprint(\"- Faster inference (no adapter overhead)\")\nprint(\"- Simpler deployment (single model file)\")\nprint(\"- Compatible with vLLM, TGI, etc.\")\nprint(\"\\nTrade-off:\")\nprint(\"- Larger model size (full precision weights)\")\nprint(\"- Can't switch adapters dynamically\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "tks66f0ps2",
   "source": "### 4. Hyperparameter Search Strategy",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "st6sfjz3zop",
   "source": "# Recommended hyperparameter search grid for LoRA/QLoRA\n\nhyperparameter_search_grid = {\n    # LoRA parameters\n    'rank': [8, 16, 32],                    # Start with 8!\n    'lora_alpha_ratio': [0.5, 1.0, 2.0],    # alpha = r * ratio\n    'lora_dropout': [0.0, 0.05, 0.1],\n    \n    # Training parameters\n    'learning_rate': [5e-5, 1e-4, 2e-4],\n    'num_epochs': [1, 2, 3],\n    'batch_size': [4, 8],                   # Adjust for GPU memory\n}\n\n# RECOMMENDED SEARCH STRATEGY:\n\nprint(\"Phase 1: Quick baseline\")\nprint(\"-\" * 50)\nbaseline = {\n    'rank': 8,\n    'lora_alpha': 8,                        # alpha = r\n    'lora_dropout': 0.05,\n    'learning_rate': 2e-4,\n    'num_epochs': 2,\n    'batch_size': 4,\n}\nprint(f\"Baseline config: {baseline}\")\nprint(\"Train for 2 epochs, evaluate performance\\n\")\n\nprint(\"Phase 2: Optimize rank (if needed)\")\nprint(\"-\" * 50)\nprint(\"Try ranks: [8, 16, 32]\")\nprint(\"Keep other params from baseline\")\nprint(\"Pick rank with best validation performance\\n\")\n\nprint(\"Phase 3: Optimize learning rate\")\nprint(\"-\" * 50)\nprint(\"Try LRs: [5e-5, 1e-4, 2e-4]\")\nprint(\"Use best rank from Phase 2\")\nprint(\"Pick LR with best validation performance\\n\")\n\nprint(\"Phase 4: Fine-tune other params\")\nprint(\"-\" * 50)\nprint(\"Try alpha ratios: [0.5, 1.0] (if overfitting, try 2.0)\")\nprint(\"Try dropout: [0.0, 0.05] (if overfitting, try 0.1)\")\nprint(\"Try epochs: [1, 2, 3] (watch for overfitting)\\n\")\n\nprint(\"IMPORTANT: Don't optimize all at once!\")\nprint(\"- Total search space: 3 × 3 × 3 × 3 × 3 × 2 = 486 combinations\")\nprint(\"- Sequential search: ~10-15 runs to find good config\")\nprint(\"- Use validation set to avoid overfitting to test set\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "---\n",
    "## Production Checklist\n",
    "\n",
    "### Before Fine-tuning\n",
    "- [ ] **Data Quality**: Clean, diverse, representative dataset\n",
    "- [ ] **Data Size**: Minimum 100-1000 examples per task\n",
    "- [ ] **Format**: Consistent instruction format\n",
    "- [ ] **Validation Set**: 10-20% held out for evaluation\n",
    "- [ ] **Baseline**: Test base model performance first\n",
    "\n",
    "### Configuration\n",
    "- [ ] **Quantization**: Choose based on GPU memory\n",
    "- [ ] **Rank**: Start with r=16, adjust based on results\n",
    "- [ ] **Learning Rate**: 2e-4 to 5e-5 (lower for larger models)\n",
    "- [ ] **Batch Size**: As large as memory allows\n",
    "- [ ] **Epochs**: 3-5 (monitor for overfitting)\n",
    "\n",
    "### During Training\n",
    "- [ ] **Monitor Loss**: Should decrease smoothly\n",
    "- [ ] **Check Gradients**: Watch for exploding/vanishing\n",
    "- [ ] **Sample Outputs**: Generate periodically\n",
    "- [ ] **Save Checkpoints**: Every epoch or N steps\n",
    "- [ ] **Log Metrics**: Use W&B or TensorBoard\n",
    "\n",
    "### After Training\n",
    "- [ ] **Evaluate**: Test on held-out validation set\n",
    "- [ ] **Compare**: Benchmark against base model\n",
    "- [ ] **Test Edge Cases**: Adversarial examples\n",
    "- [ ] **Merge Adapters**: Optional, for inference speed\n",
    "- [ ] **Deploy**: Use inference optimization (vLLM, TGI)\n",
    "\n",
    "### MITRE TTP Specific\n",
    "- [ ] **Coverage**: Test all 14 tactics\n",
    "- [ ] **Precision**: Minimize false positives\n",
    "- [ ] **Recall**: Catch all techniques\n",
    "- [ ] **Hierarchy**: Respect sub-technique relationships\n",
    "- [ ] **Updates**: Plan for new MITRE versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources and References\n",
    "\n",
    "### Papers\n",
    "1. **LoRA**: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2106.09685)\n",
    "2. **QLoRA**: [QLORA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
    "3. **DoRA**: [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353)\n",
    "4. **AdaLoRA**: [Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.10512)\n",
    "\n",
    "### MITRE ATT&CK Resources\n",
    "1. **TRAM**: [Threat Report ATT&CK Mapper](https://github.com/center-for-threat-informed-defense/tram)\n",
    "2. **Dataset**: [MITRE TTP Mapping Dataset](https://github.com/tumeteor/mitre-ttp-mapping)\n",
    "3. **Framework**: [MITRE ATT&CK](https://attack.mitre.org/)\n",
    "\n",
    "### Code Repositories\n",
    "1. **Hugging Face PEFT**: [github.com/huggingface/peft](https://github.com/huggingface/peft)\n",
    "2. **QLoRA**: [github.com/artidoro/qlora](https://github.com/artidoro/qlora)\n",
    "3. **Transformers**: [github.com/huggingface/transformers](https://github.com/huggingface/transformers)\n",
    "\n",
    "### Models\n",
    "1. **Qwen 2.5**: [Qwen/Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)\n",
    "2. **Phi-4**: [microsoft/phi-4](https://huggingface.co/microsoft/phi-4)\n",
    "3. **Gemma 3**: [google/gemma-3-1b](https://huggingface.co/google/gemma-3-1b)\n",
    "4. **Llama 3.2**: [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "### LoRA vs QLoRA\n",
    "- **LoRA**: Fast, efficient, 0.1-1% trainable parameters\n",
    "- **QLoRA**: Same as LoRA + 75% memory reduction via 4-bit quantization\n",
    "- **Choice**: Use QLoRA when memory-constrained, LoRA otherwise\n",
    "\n",
    "### Quantization\n",
    "- **4-bit NF4**: Best memory efficiency (75% reduction)\n",
    "- **8-bit INT8**: Good balance (50% reduction, faster convergence)\n",
    "- **Impact**: Minimal accuracy loss (<1%)\n",
    "\n",
    "### PEFT Methods (2025)\n",
    "- **LoRA**: Most versatile, widely supported\n",
    "- **AdaLoRA**: Adaptive rank, better performance with fewer params\n",
    "- **DoRA**: More stable training, separates magnitude/direction\n",
    "- **IA³**: Ultra-efficient (0.01% params), limited capacity\n",
    "\n",
    "### Training Optimizations\n",
    "- **Mixed Precision**: BF16 recommended for LLMs\n",
    "- **Flash Attention 2**: 2-4x speedup, lower memory\n",
    "- **Gradient Checkpointing**: 40% memory reduction\n",
    "- **Paged Optimizer**: Essential for QLoRA\n",
    "\n",
    "### For MITRE TTP Mapping\n",
    "- **Model**: Qwen 2.5 1.5B (best 1B+ model)\n",
    "- **Config**: 4-bit QLoRA, r=16-32\n",
    "- **Data**: TRAM dataset + custom examples\n",
    "- **Evaluation**: Multi-label metrics, tactic coverage\n",
    "\n",
    "### Production Deployment\n",
    "- Fine-tune with QLoRA on consumer GPU\n",
    "- Merge adapters for inference (optional)\n",
    "- Use vLLM or TGI for serving\n",
    "- Monitor drift, update with new MITRE versions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}