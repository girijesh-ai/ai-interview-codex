# ğŸ¨ Attention Mechanism: Complete Visual Deep Dive

**Master self-attention with exact dimensions, step-by-step transformations, and visual diagrams**

---

## ğŸ“ Part 1: Understanding Dimensions Through a Real Example

### Setup: Concrete Example

Let's use a real sentence with actual dimensions:

```
Sentence: "The cat sat"
Tokens: 3 words
d_model: 512 (embedding dimension)
d_k: 64 (key/query dimension)
d_v: 64 (value dimension)
num_heads: 8
```

---

## ğŸ”¢ Step-by-Step: Single-Head Attention with Exact Dimensions

### Step 0: Input Embeddings

```
Input sentence: "The cat sat"

Token Embeddings (from embedding layer):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "The"  â†’ [0.23, -0.15, 0.87, ...]  â”‚  512 numbers
â”‚  "cat"  â†’ [0.45, 0.92, -0.31, ...]  â”‚  512 numbers
â”‚  "sat"  â†’ [-0.67, 0.34, 0.12, ...]  â”‚  512 numbers
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Matrix form X:
        dim 0   dim 1   dim 2  ...  dim 511
The  [  0.23   -0.15    0.87  ...   -0.42  ]
cat  [  0.45    0.92   -0.31  ...    0.78  ]
sat  [ -0.67    0.34    0.12  ...    0.15  ]

Shape: X = (3, 512)
       (seq_len, d_model)
```

**Visual representation:**

```
X matrix visualization:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ The â”‚â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â”‚  â† 512 dimensions
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ cat â”‚â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sat â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†‘
   3 tokens
```

---

### Step 1: Create Q, K, V Matrices

#### Weight Matrices (Learned During Training)

```
W_Q: (512, 64)  - Projects to query space
W_K: (512, 64)  - Projects to key space
W_V: (512, 64)  - Projects to value space

These are LEARNED parameters (trained with backprop)
```

**Visualization of weight matrices:**

```
W_Q matrix (512 Ã— 64):
        d_k=0  d_k=1  ...  d_k=63
d_m=0  [ 0.01   0.23  ...  -0.15 ]
d_m=1  [-0.45   0.67  ...   0.22 ]
  ...
d_m=511[ 0.33  -0.12  ...   0.89 ]

     â†‘                    â†‘
   Input dim         Output dim
    (512)              (64)
```

#### Matrix Multiplication: X Â· W_Q = Q

```
Step-by-step for Query:

X (3, 512) Â· W_Q (512, 64) = Q (3, 64)

Detailed:
         [512 values]              [64 values]
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
The â”‚ 0.23 ... -0.42  â”‚  Â·   â”‚  W_Q weights â”‚ = Q_The [q0, q1, ..., q63]
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

For "The", computing q_0 (first query dimension):
q_0 = (0.23 Ã— W_Q[0,0]) + (-0.15 Ã— W_Q[1,0]) + ... + (-0.42 Ã— W_Q[511,0])
    = dot product of "The" embedding with first column of W_Q
```

**Result:**

```
Q matrix (3, 64): Query vectors
         q_0    q_1   ...   q_63
The  [ 0.56   0.23  ...  -0.12 ]
cat  [-0.34   0.89  ...   0.45 ]
sat  [ 0.12  -0.67  ...   0.78 ]

K matrix (3, 64): Key vectors
         k_0    k_1   ...   k_63
The  [ 0.45  -0.11  ...   0.34 ]
cat  [ 0.78   0.56  ...  -0.23 ]
sat  [-0.23   0.12  ...   0.90 ]

V matrix (3, 64): Value vectors
         v_0    v_1   ...   v_63
The  [ 0.67   0.34  ...  -0.45 ]
cat  [-0.12   0.78  ...   0.56 ]
sat  [ 0.90  -0.34  ...   0.12 ]
```

**Intuitive meaning:**

```
Q (Query):  "What am I looking for?"
            - Each token asks: "What info do I need?"
            
K (Key):    "What do I contain?"
            - Each token advertises: "I have this info!"
            
V (Value):  "Here's my actual information"
            - What gets passed if a token attends here
```

---

### Step 2: Compute Attention Scores (Q Â· K^T)

#### Matrix Multiplication: Q Â· K^T

```
Q (3, 64) Â· K^T (64, 3) = Scores (3, 3)

Step by step:
         [64 values]         [64 values]
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
The â”‚ q0 q1 ... q63  â”‚ Â·â”‚ k0(The)        â”‚ = Score(Theâ†’The)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ k1(The)        â”‚
                        â”‚  ...           â”‚
                        â”‚ k63(The)       â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Score(Theâ†’The) = q0Ã—k0 + q1Ã—k1 + ... + q63Ã—k63
               = 0.56Ã—0.45 + 0.23Ã—(-0.11) + ... + (-0.12)Ã—0.34
               = 12.4  (example value)
```

**Full score matrix:**

```
Scores matrix (3, 3):
             To: The    To: cat    To: sat
From: The  [ 12.4       8.7        5.3   ]
From: cat  [  9.1      15.2        7.8   ]
From: sat  [  6.5       8.9       13.7   ]

Interpretation:
- Score(Theâ†’The) = 12.4  (high: "The" is relevant to itself)
- Score(Theâ†’cat) = 8.7   (medium: "cat" somewhat relevant to "The")
- Score(Theâ†’sat) = 5.3   (low: "sat" less relevant to "The")
```

**Visual representation:**

```
Attention Scores Heatmap:
         The      cat      sat
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
The â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ  â”‚  Higher scores = darker
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
cat â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
sat â”‚ â–ˆâ–ˆâ–ˆâ–ˆ      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Step 3: Scale by âˆšd_k

```
Why scale?
- Dot products can get very large when d_k is large
- Large values â†’ softmax saturates â†’ tiny gradients
- Scaling keeps values in reasonable range

Formula: Scores_scaled = Scores / âˆšd_k

d_k = 64
âˆšd_k = 8

Scaled scores:
             To: The    To: cat    To: sat
From: The  [  1.55       1.09       0.66  ]  = [12.4/8, 8.7/8, 5.3/8]
From: cat  [  1.14       1.90       0.98  ]
From: sat  [  0.81       1.11       1.71  ]
```

**Why this matters:**

```
Before scaling (d_k=64):
Scores: [50, 48, 2]  â†’ after softmax: [0.73, 0.27, 0.00]
                        (2 gets "squashed" to zero!)

After scaling:
Scores: [6.25, 6.0, 0.25] â†’ after softmax: [0.54, 0.43, 0.03]
                             (2 still contributes!)
```

---

### Step 4: Apply Softmax (Row-wise)

#### Softmax Formula

```
softmax(x_i) = exp(x_i) / Î£ exp(x_j)

For each ROW (each query token):
```

**Detailed calculation for "The" row:**

```
Scaled scores for "The": [1.55, 1.09, 0.66]

Step 1: Exponentiate
exp(1.55) = 4.71
exp(1.09) = 2.97
exp(0.66) = 1.93

Step 2: Sum
total = 4.71 + 2.97 + 1.93 = 9.61

Step 3: Normalize
Attention(Theâ†’The) = 4.71 / 9.61 = 0.49  (49%)
Attention(Theâ†’cat) = 2.97 / 9.61 = 0.31  (31%)
Attention(Theâ†’sat) = 1.93 / 9.61 = 0.20  (20%)

Verify: 0.49 + 0.31 + 0.20 = 1.00 âœ“ (sums to 1)
```

**Full attention weight matrix:**

```
Attention Weights (3, 3):
             To: The    To: cat    To: sat    Sum
From: The  [  0.49       0.31       0.20   ] = 1.0 âœ“
From: cat  [  0.24       0.55       0.21   ] = 1.0 âœ“
From: sat  [  0.18       0.25       0.57   ] = 1.0 âœ“

Each row is a probability distribution!
```

**Visual interpretation:**

```
"The" token attention distribution:
    49% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  to itself
    31% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         to "cat"
    20% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             to "sat"

"cat" token attention distribution:
    24% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             to "The"
    55% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ to itself  
    21% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             to "sat"

"sat" token attention distribution:
    18% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              to "The"
    25% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           to "cat"
    57% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  to itself
```

---

### Step 5: Multiply by Values (Weighted Sum)

#### Matrix Multiplication: Attention_weights Â· V

```
Attention (3, 3) Â· V (3, 64) = Output (3, 64)

For "The" token:
Output_The = 0.49 Ã— V_The + 0.31 Ã— V_cat + 0.20 Ã— V_sat

Detailed for dimension 0:
output_The[0] = 0.49 Ã— 0.67 + 0.31 Ã— (-0.12) + 0.20 Ã— 0.90
              = 0.328 + (-0.037) + 0.180
              = 0.471
```

**Full calculation:**

```
V matrix:
         v_0    v_1   ...   v_63
The  [ 0.67   0.34  ...  -0.45 ]
cat  [-0.12   0.78  ...   0.56 ]
sat  [ 0.90  -0.34  ...   0.12 ]

Output for "The":
[0.49Ã—0.67 + 0.31Ã—(-0.12) + 0.20Ã—0.90,  ... (64 dimensions)]
= [0.471, ..., (compute for each dimension)]

Output matrix (3, 64):
         out_0  out_1  ...  out_63
The  [  0.47   0.31  ...   -0.15 ]  â† Weighted combo of all V's
cat  [ -0.03   0.52  ...    0.34 ]
sat  [  0.56  -0.08  ...    0.23 ]
```

**What happened?**

```
Original "The" embedding:  [0.23, -0.15, ..., -0.42]
After attention:           [0.47, 0.31, ..., -0.15]

"The" is now enriched with information from:
- 49% of itself
- 31% of "cat"  
- 20% of "sat"

This is context-aware representation!
```

---

## ğŸ¯ Complete Visual Flow: Single-Head Attention

```
INPUT: X (3, 512)
   "The"  [512 dims]
   "cat"  [512 dims]
   "sat"  [512 dims]
        â†“
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   â†“         â†“         â†“
[Ã—W_Q]    [Ã—W_K]    [Ã—W_V]
(512,64)  (512,64)  (512,64)
   â†“         â†“         â†“
   Q         K         V
 (3,64)    (3,64)    (3,64)
   â†“         â†“
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â†“
    [Q Â· K^T]
        â†“
   Scores (3,3)
   [12.4  8.7  5.3 ]
   [ 9.1 15.2  7.8 ]
   [ 6.5  8.9 13.7 ]
        â†“
   [Ã· âˆš64 = Ã·8]
        â†“
   Scaled (3,3)
   [1.55 1.09 0.66]
   [1.14 1.90 0.98]
   [0.81 1.11 1.71]
        â†“
   [softmax per row]
        â†“
  Attention (3,3)
  [0.49 0.31 0.20]
  [0.24 0.55 0.21]
  [0.18 0.25 0.57]
        â†“         â†“
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
             â†“
        [Attn Â· V]
             â†“
        Output (3,64)
   "The" [enriched 64 dims]
   "cat" [enriched 64 dims]
   "sat" [enriched 64 dims]
```

---

## ğŸ­ Multi-Head Attention: The Complete Picture

### Why Multiple Heads?

```
Single head: Learns ONE type of relationship
Multi-head:  Learns MULTIPLE relationships in parallel

Example with 8 heads:
- Head 1: Subject-verb relationships ("cat" â† "sat")
- Head 2: Article-noun ("The" â† "cat")
- Head 3: Positional proximity (nearby words)
- Head 4: Semantic similarity (related concepts)
- Head 5-8: Other patterns
```

### Multi-Head Architecture (8 heads)

```
INPUT: X (3, 512)
        â†“
    [Split d_model into heads]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Head 1    â”‚ Head 2â”‚ Head 3â”‚ Head 4â”‚ Head 5â”‚ Head 6â”‚ Head 7â”‚ Head 8
â”‚ d_k=64    â”‚ d_k=64â”‚ d_k=64â”‚ d_k=64â”‚ d_k=64â”‚ d_k=64â”‚ d_k=64â”‚ d_k=64
â”‚           â”‚       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚
â”‚ Q (3,64)  â”‚       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚
â”‚ K (3,64)  â”‚       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚
â”‚ V (3,64)  â”‚       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚
â”‚     â†“     â”‚   â†“   â”‚   â†“   â”‚   â†“   â”‚   â†“   â”‚   â†“   â”‚   â†“   â”‚   â†“
â”‚ Attention â”‚  Attn â”‚  Attn â”‚  Attn â”‚  Attn â”‚  Attn â”‚  Attn â”‚  Attn
â”‚ (3, 64)   â”‚(3,64) â”‚(3,64) â”‚(3,64) â”‚(3,64) â”‚(3,64) â”‚(3,64) â”‚(3,64)
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬
        â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
                         [Concatenate]
                                â†“
                       Concat (3, 512)
                     [64+64+64+...+64]
                                â†“
                           [Ã— W_O]
                          (512, 512)
                                â†“
                         Output (3, 512)
```

**Dimension tracking:**

```
Input:        (batch=1, seq_len=3, d_model=512)

Per head:
  Q, K, V:    (batch=1, num_heads=8, seq_len=3, d_k=64)
  Attention:  (batch=1, num_heads=8, seq_len=3, d_k=64)

After concat: (batch=1, seq_len=3, d_model=512)
              [8 Ã— 64 = 512]

After W_O:    (batch=1, seq_len=3, d_model=512)
```

### Example: What Each Head Learns

```
Sentence: "The cat sat on the mat"

Head 1 - Syntactic relationships:
         The    cat    sat    on     the    mat
cat    [0.05   0.10   0.70   0.05   0.05   0.05]  â†’ 70% to "sat" (verb)

Head 2 - Article-noun:
         The    cat    sat    on     the    mat
cat    [0.60   0.20   0.05   0.05   0.05   0.05]  â†’ 60% to "The" (article)

Head 3 - Positional (neighbors):
         The    cat    sat    on     the    mat
cat    [0.30   0.20   0.40   0.05   0.03   0.02]  â†’ Focus on nearby "The", "sat"

Head 4 - Semantic similarity:
         The    cat    sat    on     the    mat
cat    [0.05   0.15   0.10   0.05   0.05   0.60]  â†’ 60% to "mat" (related context)

Different heads capture different aspects!
```

---

## ğŸ“Š Part 2: Complete Transformer Encoder Block

### Full Architecture with Dimensions

```
INPUT: X (batch=32, seq=50, d_model=512)
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ENCODER BLOCK                â”‚
â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚     Layer Norm                 â”‚ â”‚
â”‚  â”‚     (Pre-LN style)             â”‚ â”‚
â”‚  â”‚      â†“                         â”‚ â”‚
â”‚  â”‚  Multi-Head Attention          â”‚ â”‚
â”‚  â”‚  â€¢ 8 heads                     â”‚ â”‚
â”‚  â”‚  â€¢ d_k = 64 per head           â”‚ â”‚
â”‚  â”‚  â€¢ Input: (32, 50, 512)        â”‚ â”‚
â”‚  â”‚  â€¢ Output: (32, 50, 512)       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚      â†“                               â”‚
â”‚  [Residual Add]                      â”‚
â”‚  X_new = X_old + Attention(X)        â”‚
â”‚      â†“                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚     Layer Norm                 â”‚ â”‚
â”‚  â”‚      â†“                         â”‚ â”‚
â”‚  â”‚  Feed-Forward Network          â”‚ â”‚
â”‚  â”‚  â€¢ 512 â†’ 2048 â†’ 512            â”‚ â”‚
â”‚  â”‚  â€¢ FFN(x) = W2Â·ReLU(W1Â·x)      â”‚ â”‚
â”‚  â”‚  â€¢ W1: (512, 2048)             â”‚ â”‚
â”‚  â”‚  â€¢ W2: (2048, 512)             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚      â†“                               â”‚
â”‚  [Residual Add]                      â”‚
â”‚  X_final = X_new + FFN(X_new)        â”‚
â”‚      â†“                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
OUTPUT: (32, 50, 512)
```

### Feed-Forward Network Detailed

```
Input: x (batch=32, seq=50, d_model=512)

Layer 1: Linear expansion
   x (32, 50, 512) Ã— W1 (512, 2048) = h (32, 50, 2048)
   
   For token i:
   h_i = x_i @ W1 + b1
       = [512 dims] @ [512Ã—2048] + [2048]
       = [2048 dims]

Activation: ReLU
   ReLU(h) = max(0, h)
   Shape: (32, 50, 2048)

Layer 2: Linear projection back
   ReLU(h) (32, 50, 2048) Ã— W2 (2048, 512) = out (32, 50, 512)

Total parameters in FFN:
   W1: 512 Ã— 2048 = 1,048,576
   b1: 2048
   W2: 2048 Ã— 512 = 1,048,576
   b2: 512
   Total: ~2.1M parameters per FFN block
```

**Why 4Ã— expansion?**

```
d_model = 512
d_ff = 2048 = 4 Ã— 512

Why so large?
- FFN acts as "memory" for the model
- Larger intermediate dimension = more capacity
- Most parameters are in FFN (not attention!)
- 4Ã— is empirically optimal (not too small, not too large)
```

---

## ğŸ” Causal Mask Visualization

### Causal Mask for "The cat sat"

```
Creating the mask:
import numpy as np
mask = np.tril(np.ones((3, 3)))

Result:
[[1. 0. 0.]     True  False False
 [1. 1. 0.]  =  True  True  False
 [1. 1. 1.]]    True  True  True

Applied to scores BEFORE softmax:
             To: The    To: cat    To: sat
From: The  [ 1.55      -inf       -inf   ]
From: cat  [ 1.14       1.90      -inf   ]
From: sat  [ 0.81       1.11       1.71  ]

After softmax (rows sum to 1):
             To: The    To: cat    To: sat
From: The  [ 1.00       0.00       0.00  ]  â† Can only see "The"
From: cat  [ 0.24       0.76       0.00  ]  â† Can see "The", "cat"
From: sat  [ 0.18       0.25       0.57  ]  â† Can see all
```

**Visual diagram:**

```
Input sequence: "The cat sat on the"

Position 0 (The):
   â”Œâ”€â”€â”€â”
   â”‚Theâ”‚   â† Can only attend to itself
   â””â”€â”€â”€â”˜

Position 1 (cat):
   â”Œâ”€â”€â”€â”¬â”€â”€â”€â”
   â”‚Theâ”‚catâ”‚  â† Can attend to: The, cat
   â””â”€â”€â”€â”´â”€â”€â”€â”˜

Position 2 (sat):
   â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
   â”‚Theâ”‚catâ”‚satâ”‚ â† Can attend to: The, cat, sat
   â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

Position 3 (on):
   â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”
   â”‚Theâ”‚catâ”‚satâ”‚onâ”‚ â† Can attend to: The, cat, sat, on
   â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”˜

This is autoregressive: predict next token without seeing future!
```

---

## ğŸŒŸ Positional Encoding Explained

### Why We Need It

```
Problem:
   "cat sat on mat" 
   "sat cat mat on"   â† Different meaning, same tokens!

Without position encoding, both would be processed identically
because attention is permutation-invariant.
```

### Sinusoidal Encoding Formula

```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

pos: Position in sequence (0, 1, 2, ...)
i: Dimension index (0, 1, 2, ..., d_model/2)
```

### Detailed Calculation

```
For pos=1 (second token), d_model=512:

Dimension 0 (i=0):
PE(1, 0) = sin(1 / 10000^(0/512))
         = sin(1 / 10000^0)
         = sin(1)
         = 0.841

Dimension 1 (i=0):
PE(1, 1) = cos(1 / 10000^(0/512))
         = cos(1)
         = 0.540

Dimension 2 (i=1):
PE(1, 2) = sin(1 / 10000^(2/512))
         = sin(1 / 10000^0.0039)
         = sin(1 / 1.009)
         = sin(0.991)
         = 0.836

Dimension 3 (i=1):
PE(1, 3) = cos(1 / 10000^(2/512))
         = 0.549

...continues for all 512 dimensions
```

**Complete matrix for 3 tokens:**

```
Positional Encoding (3, 512):
         dim0   dim1   dim2   dim3   ...  dim510 dim511
pos=0  [0.000  1.000  0.000  1.000  ...  0.000  1.000 ]
pos=1  [0.841  0.540  0.836  0.549  ...  0.001  1.000 ]
pos=2  [0.909 -0.416  0.895 -0.402  ...  0.002  1.000 ]

Pattern:
- Early dimensions (0-10): High frequency (change rapidly)
- Middle dimensions: Medium frequency
- Late dimensions (500-511): Low frequency (change slowly)
```

**Why this works:**

```
High frequency dims:  Encode absolute position
Low frequency dims:   Encode relative position

Token at pos=5 can determine if another token is at:
- pos=6 (distance=1):  Check high-freq dims
- pos=15 (distance=10): Check mid-freq dims
```

### Visual Heatmap

```
Positional Encoding Heatmap (first 10 positions):
        Dimension â†’
Pos â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
0   â”‚â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â”‚ Fast oscillation
1   â”‚â–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â”‚
2   â”‚â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â”‚
3   â”‚â–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â”‚
4   â”‚â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â”‚
5   â”‚â–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â”‚
...
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     High freq â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Low freq
```

---

## ğŸ“ Key Formulas Summary

### 1. Scaled Dot-Product Attention

```
Attention(Q, K, V) = softmax(QÂ·K^T / âˆšd_k) Â· V

Where:
- Q: (seq_len, d_k)  queries
- K: (seq_len, d_k)  keys  
- V: (seq_len, d_v)  values
- d_k: dimension of keys (e.g., 64)
- Output: (seq_len, d_v)
```

### 2. Multi-Head Attention

```
MultiHead(Q,K,V) = Concat(head_1, ..., head_h) Â· W_O

head_i = Attention(QÂ·W_Q^i, KÂ·W_K^i, VÂ·W_V^i)

Where:
- h: number of heads (e.g., 8)
- W_Q^i: (d_model, d_k)
- W_K^i: (d_model, d_k)
- W_V^i: (d_model, d_v)
- W_O: (hÂ·d_v, d_model)
```

### 3. Feed-Forward Network

```
FFN(x) = max(0, xÂ·W_1 + b_1)Â·W_2 + b_2

Where:
- W_1: (d_model, d_ff)    typically 4Ã— expansion
- W_2: (d_ff, d_model)
- d_ff = 4 Ã— d_model      e.g., 512 â†’ 2048
```

### 4. Layer Normalization

```
LayerNorm(x) = Î³ Â· (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²

Where:
- Î¼ = mean(x)
- ÏƒÂ² = variance(x)
- Î³,Î²: learned parameters
- Îµ: small constant (1e-6) for numerical stability
```

### 5. Encoder Block (Pre-LN)

```
x_1 = x + MultiHeadAttention(LayerNorm(x))
x_2 = x_1 + FFN(LayerNorm(x_1))
```

---

## ğŸ“ˆ Parameter Count Example

### BERT-base Configuration

```
d_model = 768
num_layers = 12
num_heads = 12
d_k = d_v = 64 (768 / 12)
d_ff = 3072 (4 Ã— 768)
vocab_size = 30,000
max_seq_len = 512
```

**Parameters per layer:**

```
Multi-Head Attention:
  W_Q: 768 Ã— 768 = 589,824
  W_K: 768 Ã— 768 = 589,824
  W_V: 768 Ã— 768 = 589,824
  W_O: 768 Ã— 768 = 589,824
  Total MHA: 2,359,296

FFN:
  W_1: 768 Ã— 3072 = 2,359,296
  b_1: 3072
  W_2: 3072 Ã— 768 = 2,359,296
  b_2: 768
  Total FFN: 4,722,432

Layer Norm (Ã—2):
  Î³,Î²: 768 Ã— 2 Ã— 2 = 3,072

Total per layer: ~7.1M parameters
Total for 12 layers: ~85M parameters

Embeddings:
  Token: 30,000 Ã— 768 = 23M
  Position: 512 Ã— 768 = 0.4M
  
BERT-base total: ~110M parameters
```

---

## âœ… Self-Check Questions

After studying this guide, verify you understand:

1. **Dimensions**: What is the shape of Q, K, V for a batch of 32 sentences, max length 50, d_model=512, 8 heads?

2. **Attention scores**: For 3 tokens with d_k=64, if QÂ·K^T gives [32, 28, 12], what are the values after scaling?

3. **Softmax**: If scaled scores are [2.0, 1.5, 1.0], what are the attention weights?

4. **Multi-head**: With d_model=512 and 8 heads, what is d_k per head?

5. **Parameters**: How many parameters in FFN with d_model=512, d_ff=2048?

6. **Masking**: For sequence of length 4, what does position 2 attend to with causal mask?

---

**Next:** Practice implementing these concepts in code! ğŸš€
