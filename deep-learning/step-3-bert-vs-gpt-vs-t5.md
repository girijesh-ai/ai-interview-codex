# ğŸ¨ Visual Comparison: BERT vs GPT vs T5

**A side-by-side visual guide to understand transformer architecture variants**

---

## ğŸ“Š Quick Comparison Table

| Feature | BERT (Encoder-Only) | GPT (Decoder-Only) | T5 (Encoder-Decoder) |
|---------|-------------------|-------------------|---------------------|
| **Architecture** | Stacked Encoders | Stacked Decoders | Encoder + Decoder |
| **Attention** | âœ… Bidirectional | â© Causal (Left-to-right) | Both |
| **Training** | MLM (Masked tokens) | CLM (Next token) | Span corruption |
| **Generation** | âŒ No | âœ… Yes | âœ… Yes |
| **Understanding** | â­â­â­ Excellent | â­â­ Good | â­â­â­ Excellent |
| **Best For** | Classification, embeddings | Text generation, chat | Translation, seq2seq |
| **2025 Popularity** | ğŸ“‰ Declining | ğŸ“ˆ Dominant | ğŸ“Š Moderate |
| **Examples** | BERT, RoBERTa | GPT-4, LLaMA, Claude | T5, BART |

---

## ğŸ” Attention Pattern Visualization

### BERT (Bidirectional Attention)

```
Input: "The cat sat on the mat"

Attention Pattern (each token sees ALL tokens):
        The  cat  sat  on   the  mat
The   [ â–     â–     â–     â–     â–     â–   ]  â†’ Can see everything
cat   [ â–     â–     â–     â–     â–     â–   ]  â†’ Can see everything  
sat   [ â–     â–     â–     â–     â–     â–   ]  â†’ Can see everything
on    [ â–     â–     â–     â–     â–     â–   ]  â†’ Can see everything
the   [ â–     â–     â–     â–     â–     â–   ]  â†’ Can see everything
mat   [ â–     â–     â–     â–     â–     â–   ]  â†’ Can see everything

âœ… Advantage: Rich contextual understanding
âŒ Disadvantage: Cannot generate text naturally
```

### GPT (Causal Attention)

```
Input: "The cat sat on the mat"

Attention Pattern (each token sees ONLY previous tokens):
        The  cat  sat  on   the  mat
The   [ â–     â–¡    â–¡    â–¡    â–¡    â–¡  ]  â†’ Sees only "The"
cat   [ â–     â–     â–¡    â–¡    â–¡    â–¡  ]  â†’ Sees "The cat"
sat   [ â–     â–     â–     â–¡    â–¡    â–¡  ]  â†’ Sees "The cat sat"
on    [ â–     â–     â–     â–     â–¡    â–¡  ]  â†’ Sees "The cat sat on"
the   [ â–     â–     â–     â–     â–     â–¡  ]  â†’ Sees "The cat sat on the"
mat   [ â–     â–     â–     â–     â–     â–   ]  â†’ Sees everything before

â–  = Can attend    â–¡ = Masked (cannot attend)

âœ… Advantage: Perfect for text generation
âœ… Advantage: Simple, scalable architecture
âŒ Disadvantage: Less rich context than bidirectional
```

### T5 (Encoder: Bidirectional, Decoder: Causal + Cross-Attention)

```
Encoder (Bidirectional):
Input: "translate English to German: Hello"
        translate  English  to  German  :  Hello
translate  [ â–          â–       â–      â–      â–     â–   ]
English    [ â–          â–       â–      â–      â–     â–   ]
...

Decoder (Causal + Cross-Attention to Encoder):
Output: "Hallo Welt"
        Hallo  Welt
Hallo [ â–       â–¡  ]  + Cross-attention to ALL encoder outputs
Welt  [ â–       â–   ]  + Cross-attention to ALL encoder outputs

âœ… Advantage: Best of both worlds
âŒ Disadvantage: More complex, harder to scale
```

---

## ğŸ¯ Training Objectives Explained

### BERT: Masked Language Modeling (MLM)

```
Original:  "The cat sat on the mat"
Masked:    "The [MASK] sat on the [MASK]"
Target:    Predict "cat" and "mat"

How it works:
1. Randomly mask 15% of tokens
2. Model predicts masked tokens using bidirectional context
3. Loss = CrossEntropy(predicted, true_masked_tokens)

Example:
Input:     "The [MASK] is sleeping"
Context:   Can see "The", "is", "sleeping" (all directions)
Predict:   "cat" (most likely), "dog", "baby", etc.

Why bidirectional?
"The cat is" â†’ "cat" could be anything
"is sleeping" â†’ likely an animate being
Together â†’  probably "cat", "dog", "baby"
```

**Real training statistics:**
- 15% of tokens are selected for masking:
  - 80% replaced with [MASK]
  - 10% replaced with random token (makes model robust)
  - 10% kept unchanged (reduces train/test mismatch)

### GPT: Causal Language Modeling (CLM)

```
Input sequence:  "The cat sat on the"
Model predicts:  "cat sat on the mat"
                  â†‘   â†‘   â†‘   â†‘   â†‘
Each position predicts the NEXT token

How it works:
1. Feed sequence left-to-right
2. At each position, predict next token
3. Loss = Î£ CrossEntropy(predicted_i, actual_i+1)

Example:
Input:  "The cat sat on"
        â†“   â†“   â†“   â†“
Predict:"cat sat on the"

Position 0: "The"           â†’ Predict "cat"
Position 1: "The cat"       â†’ Predict "sat"
Position 2: "The cat sat"   â†’ Predict "on"
Position 3: "The cat sat on"â†’ Predict "the"

All predictions happen in parallel during training!
```

**Why this works for generation:**
```
At inference:
Start:  "The"
Gen 1:  "The cat"      (model predicted "cat")
Gen 2:  "The cat sat"  (model predicted "sat")
Gen 3:  "The cat sat on" (model predicted "on")
...
```

### T5: Span Corruption

```
Original: "The cat sat on the mat in the sun"
Corrupt:  "The cat <X> on <Y> mat <Z> sun"
Target:   "<X> sat <Y> the <Z> in the <eos>"

How it works:
1. Mask random SPANS (not individual tokens)
2. Replace with sentinel tokens <X>, <Y>, <Z>
3. Model predicts masked spans in order

Why better than MLM?
- More realistic (phrases get masked, not random words)
- Learns to generate multiple tokens
- Works for seq2seq tasks naturally
```

---

## ğŸ—ï¸ Architecture Deep Dive

### BERT Encoder Block

```
Input (e.g., "cat")
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Embedding     â”‚  768-dim vector
â”‚  + Position Embed    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multi-Head Attention â”‚  12 heads Ã— 64 dim = 768
â”‚ (Bidirectional)      â”‚  Attends to ALL tokens
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
    Add & Norm           Residual connection
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Feed-Forward (FFN)  â”‚  768 â†’ 3072 â†’ 768
â”‚  ReLU                â”‚  Position-wise
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
    Add & Norm           Residual connection
       â†“
    Output (768-dim)
```

**Layer organization:**
- BERT-base: 12 encoder blocks
- BERT-large: 24 encoder blocks

**When to use:**
- Sentence classification (spam detection, sentiment)
- Named Entity Recognition (NER)
- Question answering (when answer is in context)
- Sentence similarity/embeddings

### GPT Decoder Block

```
Input (e.g., "cat")
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Embedding     â”‚  
â”‚  + Position Embed    â”‚  (RoPE in modern GPT)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Masked Self-Attentionâ”‚  Causal mask applied
â”‚ (Causal)             â”‚  Only sees â† previous
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
    Add & Norm           Pre-LN in modern GPT
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Feed-Forward (FFN)  â”‚  Often SwiGLU in 2025
â”‚  4x expansion        â”‚  (e.g., 4096â†’16384â†’4096)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
    Add & Norm
       â†“
    Output
```

**Key difference from BERT:**
- âŒ NO cross-attention (not needed for language modeling)
- âœ… Causal masking in self-attention
- âœ… Optimized for generation

**When to use:**
- Text generation (stories, articles)
- Chat/dialogue (ChatGPT, Claude)
- Code generation (Copilot)
- Instruction following

### T5 Full Architecture

```
ENCODER SIDE:
Input: "translate: Hello"
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Encoder Block  1    â”‚  Bidirectional
â”‚  Encoder Block  2    â”‚  attention
â”‚  ...                 â”‚
â”‚  Encoder Block 12    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
  Encoder Output (context)
       â†“
       â†“ (fed to decoder via cross-attention)
       â†“
DECODER SIDE:
Input: "<start> Hallo"
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Masked Self-Attentionâ”‚  Causal
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cross-Attention    â”‚  Attends to encoder output
â”‚   Q: from decoder    â”‚  K,V: from encoder
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Feed-Forward (FFN)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
    Output: "Hallo"
```

**When to use:**
- Machine translation (Enâ†’De, etc.)
- Summarization (long â†’ short)
- Question answering (generate answer)
- Any task that's truly seq2seq

---

## ğŸ’¡ Key Insights for Each Architecture

### BERT Insights

```python
# Why BERT is great for classification:
sentence = "This movie was terrible!"

# BERT sees the ENTIRE sentence bidirectionally
# "terrible" affects understanding of "This movie was"
# "This movie was" affects understanding of "terrible"

# Output: [CLS] token = sentence representation
cls_token = bert(sentence)[0]  # Rich, contextual embedding
classifier = linear(cls_token)  # Simple classifier on top
```

**Limitation:**
```python
# BERT cannot naturally do this:
prompt = "Once upon a time"
next_word = bert.generate()  # âŒ Not designed for this!

# Why? Because BERT sees EVERYTHING at once.
# It's not trained to predict what comes NEXT.
```

### GPT Insights

```python
# GPT predicts one token at a time
prompt = "The capital of France is"

# Step 1: "The"                      â†’ predict "capital"
# Step 2: "The capital"              â†’ predict "of"  
# Step 3: "The capital of"           â†’ predict "France"
# Step 4: "The capital of France"    â†’ predict "is"
# Step 5: "The capital of France is" â†’ predict "Paris"

output = gpt.generate(prompt)
# Output: "The capital of France is Paris"
```

**Why it scales:**
```
Simple architecture â†’ Easy to scale â†’ 175B (GPT-3), 1.7T (GPT-4) parameters
Unified objective â†’ Works for everything with prompting
```

### T5 Insights

```python
# T5 frames everything as text-to-text

# Translation
input = "translate English to German: Hello"
output = t5(input)  # "Hallo"

# Summarization
input = "summarize: [long article]"
output = t5(input)  # "[summary]"

# Classification (weird but works!)
input = "sentiment: This movie is great!"
output = t5(input)  # "positive"

# Why? Encoder understands input deeply (bidirectional)
#       Decoder generates output flexibly
```

**Trade-off:**
- âœ… Task flexibility
- âœ… Excellent understanding + generation
- âŒ More parameters for same capability
- âŒ Harder to scale to 100B+ parameters

---

## ğŸ¯ Decision Tree: Which Architecture?

```
Your Task:
â”‚
â”œâ”€ Need to GENERATE text?
â”‚  â”‚
â”‚  â”œâ”€ Yes â†’ Need to understand INPUT deeply first?
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ Yes (Translation, Summarization)
â”‚  â”‚  â”‚  â””â”€ Use: T5 / Encoder-Decoder
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ No (Chat, Creative writing, Code gen)
â”‚  â”‚     â””â”€ Use: GPT / Decoder-Only â­ (2025 standard)
â”‚  â”‚
â”‚  â””â”€ No â†’ Just classify/embed/extract?
â”‚     â””â”€ Use: BERT / Encoder-Only
â”‚
â””â”€ Special cases:
   - Embeddings for search: BERT-based (e.g., sentence-transformers)
   - Long-form generation: GPT
   - Multi-task learning: T5 or GPT with prompting
```

---

## ğŸ“ˆ Evolution Timeline

```
2017: Original Transformer (Encoder-Decoder)
      â””â”€ "Attention is All You Need"

2018: BERT (Encoder-Only)
      â””â”€ State-of-art on 11 NLP tasks
      â””â”€ Everyone uses BERT for everything

2018-2019: GPT-1, GPT-2 (Decoder-Only)
           â””â”€ "Interesting but niche"

2019: T5 (Encoder-Decoder, unified framework)
      â””â”€ Text-to-text revolution

2020: GPT-3 (175B parameters)
      â””â”€ In-context learning discovered!
      â””â”€ Paradigm shift: Decoder-only dominates

2023-2025: LLaMA, GPT-4, Claude, Qwen
           â””â”€ Only decoder-only at scale
           â””â”€ BERT/T5 for specialized tasks only

Why Decoder-Only Won?
  âœ“ Simpler architecture
  âœ“ Scales better (proven to 1T+ params)
  âœ“ Emergent abilities at scale
  âœ“ Prompting solves most tasks
  âœ“ Chat/instruction following natural fit
```

---

## ğŸ§ª Hands-On: Experience the Differences

### Try with Hugging Face

```python
from transformers import pipeline

# BERT (Encoder-Only): Fill in the blank
fill_mask = pipeline("fill-mask", model="bert-base-uncased")
result = fill_mask("The capital of France is [MASK].")
print(result)
# Output: [{'token_str': 'paris', 'score': 0.9}]

# GPT (Decoder-Only): Text generation  
generator = pipeline("text-generation", model="gpt2")
result = generator("The capital of France is", max_length=10)
print(result)
# Output: [{'generated_text': 'The capital of France is Paris'}]

# T5 (Encoder-Decoder): Translation
translator = pipeline("translation_en_to_de", model="t5-small")
result = translator("The capital of France is Paris")
print(result)
# Output: [{'translation_text': 'Die Hauptstadt Frankreichs ist Paris'}]
```

---

## Summary: The Bottom Line

### BERT (Encoder-Only)
- **Best at:** Understanding text, embeddings, classification
- **Cannot:** Generate text naturally
- **Use when:** You need rich bidirectional context
- **2025 status:** Still used for embeddings/classification, but declining

### GPT (Decoder-Only)
- **Best at:** Generating text, chat, anything with prompting
- **Decent at:** Understanding (especially with large scale)
- **Use when:** General purpose, generation, chat
- **2025 status:** ğŸ† Dominant architecture

### T5 (Encoder-Decoder)
- **Best at:** Seq2seq tasks (translation, summarization)
- **Good at:** Both understanding and generation
- **Use when:** True inputâ†’output transformation needed
- **2025 status:** Niche use cases, not scaling like decoder-only

**The 2025 Reality:**
> "Decoder-only transformers (GPT-style) have become the default choice. They're simpler, scale better, and with sufficient size and prompting, match or exceed specialized architectures on most tasks."

---

## ğŸ“ Next Steps

1. **Implement attention**: Do the hands-on exercises
2. **Run examples**: Try BERT vs GPT on same task
3. **Visualize attention**: Use tools like BertViz
4. **Read papers**: 
   - BERT: "BERT: Pre-training of Deep Bidirectional Transformers"
   - GPT: "Language Models are Few-Shot Learners" (GPT-3)
   - T5: "Exploring the Limits of Transfer Learning"

**Remember:** Architecture matters less than you think. Scale, data, and training matter more! ğŸš€
